{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f6696d1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31c5a298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sadat/miniconda3/envs/scrambmix/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/sadat/miniconda3/envs/scrambmix/lib/python3.8/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from mmaction.datasets import build_dataset, build_dataloader\n",
    "from mmaction.models import build_model\n",
    "from mmcv import Config\n",
    "from mmaction.datasets import MixupBlending\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0db510eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_score(y_true, y_pred):\n",
    "    \"\"\"Calculates precision score.\n",
    "\n",
    "    Args:\n",
    "        y_true: True labels.\n",
    "        y_pred: Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        Precision score.\n",
    "    \"\"\"\n",
    "\n",
    "    tp = sum(y_true[i] == y_pred[i] for i in range(len(y_true)) if y_pred[i] == 1)\n",
    "    fp = sum(y_true[i] != y_pred[i] for i in range(len(y_true)) if y_pred[i] == 1)\n",
    "    precision = tp / (tp + fp) if tp + fp != 0 else 0\n",
    "    return precision\n",
    "\n",
    "def recall_score(y_true, y_pred):\n",
    "    \"\"\"Calculates recall score.\n",
    "\n",
    "    Args:\n",
    "        y_true: True labels.\n",
    "        y_pred: Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        Recall score.\n",
    "    \"\"\"\n",
    "\n",
    "    tp = sum(y_true[i] == y_pred[i] for i in range(len(y_true)) if y_pred[i] == 1)\n",
    "    fn = sum(y_true[i] == 1 and y_pred[i] != 1 for i in range(len(y_true)))\n",
    "    recall = tp / (tp + fn) if tp + fn != 0 else 0\n",
    "    return recall\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    \"\"\"Calculates F1 score.\n",
    "\n",
    "    Args:\n",
    "        y_true: True labels.\n",
    "        y_pred: Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        F1 score.\n",
    "    \"\"\"\n",
    "\n",
    "    p = precision_score(y_true, y_pred)\n",
    "    r = recall_score(y_true, y_pred)\n",
    "    return 2 * p * r / (p + r) if p + r != 0 else 0\n",
    "\n",
    "def weighted_f1_score(y_true, y_pred):\n",
    "    \"\"\"Calculates the weighted F1 score, assuming equal class weights.\n",
    "\n",
    "    Args:\n",
    "        y_true: True labels.\n",
    "        y_pred: Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        Weighted F1 score.\n",
    "    \"\"\"\n",
    "\n",
    "    num_classes = len(set(y_true))\n",
    "    f1_scores = []\n",
    "    for i in range(num_classes):\n",
    "        class_mask = [1 if y == i else 0 for y in y_true]\n",
    "        class_f1 = f1_score(class_mask, [1 if y == i else 0 for y in y_pred])\n",
    "        f1_scores.append(class_f1)\n",
    "    return sum(f1_scores) / num_classes\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    \"\"\"Calculates the accuracy score.\n",
    "    \n",
    "    Args:\n",
    "    y_true: True labels.\n",
    "    y_pred: Predicted labels.\n",
    "    \n",
    "    Returns:\n",
    "    Accuracy score.\n",
    "    \"\"\"\n",
    "    \n",
    "    correct_predictions = sum(np.array(y_true) == np.array(y_pred))\n",
    "    total_predictions = len(y_true)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f3cec5",
   "metadata": {},
   "source": [
    "## Loading batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9df620d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config.fromfile('./mixup.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0258630",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a2c341f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'RawframeDataset',\n",
       " 'ann_file': 'data/hmdb51/annotation_train.txt',\n",
       " 'data_prefix': 'data/hmdb51/rawframes',\n",
       " 'pipeline': [{'type': 'SampleFrames',\n",
       "   'clip_len': 32,\n",
       "   'frame_interval': 2,\n",
       "   'num_clips': 1},\n",
       "  {'type': 'RawFrameDecode'},\n",
       "  {'type': 'Resize', 'scale': (-1, 256)},\n",
       "  {'type': 'RandomResizedCrop'},\n",
       "  {'type': 'Resize', 'scale': (224, 224), 'keep_ratio': False},\n",
       "  {'type': 'Flip', 'flip_ratio': 0.5},\n",
       "  {'type': 'Normalize',\n",
       "   'mean': [123.675, 116.28, 103.53],\n",
       "   'std': [58.395, 57.12, 57.375],\n",
       "   'to_bgr': False},\n",
       "  {'type': 'FormatShape', 'input_format': 'NCTHW'},\n",
       "  {'type': 'Collect', 'keys': ['imgs', 'label'], 'meta_keys': []},\n",
       "  {'type': 'ToTensor', 'keys': ['imgs', 'label']}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.data.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccef55c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = build_dataset(cfg=cfg.data.train)\n",
    "train_loader = build_dataloader(\n",
    "        train_dataset,\n",
    "        videos_per_gpu=8,\n",
    "        workers_per_gpu=4,\n",
    "        persistent_workers=False,\n",
    "        num_gpus=1,\n",
    "        dist=False)\n",
    "\n",
    "val_dataset = build_dataset(cfg=cfg.data.val)\n",
    "val_loader = build_dataloader(\n",
    "        val_dataset,\n",
    "        videos_per_gpu=1,\n",
    "        workers_per_gpu=4,\n",
    "        persistent_workers=False,\n",
    "        num_gpus=1,\n",
    "        dist=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eacfdb",
   "metadata": {},
   "source": [
    "## Learning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46b5656d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 19:37:44,713] Using an existing study with name 'mixup_hmdb' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Optuna study...\n",
      "Starting a new trial...\n",
      "Trial 1: alpha = 3.807947176588889\n",
      "Building the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 19:37:45,099 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r50_ig65m_20210617-ce545a37.pth\n",
      "2024-12-12 19:37:45,100 - mmaction - INFO - load checkpoint from http path: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r50_ig65m_20210617-ce545a37.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model built successfully!\n",
      "Starting training...\n",
      "Epoch [1/60], Batch [0/447], Loss: 3.9613\n",
      "Epoch [1/60], Batch [10/447], Loss: 4.0909\n",
      "Epoch [1/60], Batch [20/447], Loss: 4.0126\n",
      "Epoch [1/60], Batch [30/447], Loss: 3.7987\n",
      "Epoch [1/60], Batch [40/447], Loss: 4.0124\n",
      "Epoch [1/60], Batch [50/447], Loss: 3.9037\n",
      "Epoch [1/60], Batch [60/447], Loss: 3.8960\n",
      "Epoch [1/60], Batch [70/447], Loss: 3.9700\n",
      "Epoch [1/60], Batch [80/447], Loss: 3.9416\n",
      "Epoch [1/60], Batch [90/447], Loss: 3.9618\n",
      "Epoch [1/60], Batch [100/447], Loss: 3.9096\n",
      "Epoch [1/60], Batch [110/447], Loss: 3.9571\n",
      "Epoch [1/60], Batch [120/447], Loss: 3.9281\n",
      "Epoch [1/60], Batch [130/447], Loss: 3.9490\n",
      "Epoch [1/60], Batch [140/447], Loss: 3.9070\n",
      "Epoch [1/60], Batch [150/447], Loss: 3.9105\n",
      "Epoch [1/60], Batch [160/447], Loss: 3.9089\n",
      "Epoch [1/60], Batch [170/447], Loss: 3.9591\n",
      "Epoch [1/60], Batch [180/447], Loss: 3.9466\n",
      "Epoch [1/60], Batch [190/447], Loss: 3.9998\n",
      "Epoch [1/60], Batch [200/447], Loss: 3.9291\n",
      "Epoch [1/60], Batch [210/447], Loss: 3.8567\n",
      "Epoch [1/60], Batch [220/447], Loss: 3.8939\n",
      "Epoch [1/60], Batch [230/447], Loss: 3.9683\n",
      "Epoch [1/60], Batch [240/447], Loss: 3.9286\n",
      "Epoch [1/60], Batch [250/447], Loss: 3.8745\n",
      "Epoch [1/60], Batch [260/447], Loss: 3.8757\n",
      "Epoch [1/60], Batch [270/447], Loss: 3.9261\n",
      "Epoch [1/60], Batch [280/447], Loss: 3.8706\n",
      "Epoch [1/60], Batch [290/447], Loss: 3.9845\n",
      "Epoch [1/60], Batch [300/447], Loss: 3.9620\n",
      "Epoch [1/60], Batch [310/447], Loss: 3.8515\n",
      "Epoch [1/60], Batch [320/447], Loss: 4.0353\n",
      "Epoch [1/60], Batch [330/447], Loss: 3.9759\n",
      "Epoch [1/60], Batch [340/447], Loss: 3.8671\n",
      "Epoch [1/60], Batch [350/447], Loss: 3.9983\n",
      "Epoch [1/60], Batch [360/447], Loss: 3.8119\n",
      "Epoch [1/60], Batch [370/447], Loss: 3.8537\n",
      "Epoch [1/60], Batch [380/447], Loss: 3.8428\n",
      "Epoch [1/60], Batch [390/447], Loss: 3.9201\n",
      "Epoch [1/60], Batch [400/447], Loss: 3.8038\n",
      "Epoch [1/60], Batch [410/447], Loss: 3.8488\n",
      "Epoch [1/60], Batch [420/447], Loss: 3.7072\n",
      "Epoch [1/60], Batch [430/447], Loss: 3.7076\n",
      "Epoch [1/60], Batch [440/447], Loss: 3.8782\n",
      "Epoch [1/60], Average Train Loss: 3.9320\n",
      "Epoch [1/60], Validation Accuracy: 0.2118, Validation Loss: 3.7365\n",
      "New best validation accuracy: 0.2118\n",
      "Epoch [2/60], Batch [0/447], Loss: 3.8035\n",
      "Epoch [2/60], Batch [10/447], Loss: 3.7849\n",
      "Epoch [2/60], Batch [20/447], Loss: 3.9547\n",
      "Epoch [2/60], Batch [30/447], Loss: 3.8602\n",
      "Epoch [2/60], Batch [40/447], Loss: 3.8708\n",
      "Epoch [2/60], Batch [50/447], Loss: 3.8630\n",
      "Epoch [2/60], Batch [60/447], Loss: 3.7887\n",
      "Epoch [2/60], Batch [70/447], Loss: 3.7373\n",
      "Epoch [2/60], Batch [80/447], Loss: 3.9723\n",
      "Epoch [2/60], Batch [90/447], Loss: 3.8106\n",
      "Epoch [2/60], Batch [100/447], Loss: 3.8138\n",
      "Epoch [2/60], Batch [110/447], Loss: 3.8224\n",
      "Epoch [2/60], Batch [120/447], Loss: 3.8112\n",
      "Epoch [2/60], Batch [130/447], Loss: 3.6744\n",
      "Epoch [2/60], Batch [140/447], Loss: 3.9162\n",
      "Epoch [2/60], Batch [150/447], Loss: 3.7598\n",
      "Epoch [2/60], Batch [160/447], Loss: 3.6494\n",
      "Epoch [2/60], Batch [170/447], Loss: 3.7448\n",
      "Epoch [2/60], Batch [180/447], Loss: 3.7295\n",
      "Epoch [2/60], Batch [190/447], Loss: 3.9125\n",
      "Epoch [2/60], Batch [200/447], Loss: 3.7224\n",
      "Epoch [2/60], Batch [210/447], Loss: 3.6166\n",
      "Epoch [2/60], Batch [220/447], Loss: 3.8916\n",
      "Epoch [2/60], Batch [230/447], Loss: 3.7364\n",
      "Epoch [2/60], Batch [240/447], Loss: 3.9559\n",
      "Epoch [2/60], Batch [250/447], Loss: 3.8194\n",
      "Epoch [2/60], Batch [260/447], Loss: 3.8195\n",
      "Epoch [2/60], Batch [270/447], Loss: 3.4338\n",
      "Epoch [2/60], Batch [280/447], Loss: 3.8324\n",
      "Epoch [2/60], Batch [290/447], Loss: 3.9276\n",
      "Epoch [2/60], Batch [300/447], Loss: 3.6894\n",
      "Epoch [2/60], Batch [310/447], Loss: 3.6778\n",
      "Epoch [2/60], Batch [320/447], Loss: 3.5984\n",
      "Epoch [2/60], Batch [330/447], Loss: 3.5919\n",
      "Epoch [2/60], Batch [340/447], Loss: 3.8665\n",
      "Epoch [2/60], Batch [350/447], Loss: 3.8177\n",
      "Epoch [2/60], Batch [360/447], Loss: 3.8863\n",
      "Epoch [2/60], Batch [370/447], Loss: 3.4383\n",
      "Epoch [2/60], Batch [380/447], Loss: 3.8863\n",
      "Epoch [2/60], Batch [390/447], Loss: 3.5074\n",
      "Epoch [2/60], Batch [400/447], Loss: 3.7664\n",
      "Epoch [2/60], Batch [410/447], Loss: 3.8222\n",
      "Epoch [2/60], Batch [420/447], Loss: 3.7480\n",
      "Epoch [2/60], Batch [430/447], Loss: 3.6241\n",
      "Epoch [2/60], Batch [440/447], Loss: 3.6878\n",
      "Epoch [2/60], Average Train Loss: 3.7846\n",
      "Epoch [2/60], Validation Accuracy: 0.3941, Validation Loss: 3.2530\n",
      "New best validation accuracy: 0.3941\n",
      "Epoch [3/60], Batch [0/447], Loss: 3.6532\n",
      "Epoch [3/60], Batch [10/447], Loss: 3.8127\n",
      "Epoch [3/60], Batch [20/447], Loss: 3.6714\n",
      "Epoch [3/60], Batch [30/447], Loss: 3.7142\n",
      "Epoch [3/60], Batch [40/447], Loss: 3.8046\n",
      "Epoch [3/60], Batch [50/447], Loss: 3.6281\n",
      "Epoch [3/60], Batch [60/447], Loss: 3.8492\n",
      "Epoch [3/60], Batch [70/447], Loss: 3.8824\n",
      "Epoch [3/60], Batch [80/447], Loss: 3.8477\n",
      "Epoch [3/60], Batch [90/447], Loss: 3.6611\n",
      "Epoch [3/60], Batch [100/447], Loss: 3.6508\n",
      "Epoch [3/60], Batch [110/447], Loss: 3.3622\n",
      "Epoch [3/60], Batch [120/447], Loss: 3.6845\n",
      "Epoch [3/60], Batch [130/447], Loss: 3.5551\n",
      "Epoch [3/60], Batch [140/447], Loss: 3.7268\n",
      "Epoch [3/60], Batch [150/447], Loss: 3.6825\n",
      "Epoch [3/60], Batch [160/447], Loss: 3.3419\n",
      "Epoch [3/60], Batch [170/447], Loss: 3.7924\n",
      "Epoch [3/60], Batch [180/447], Loss: 3.6270\n",
      "Epoch [3/60], Batch [190/447], Loss: 3.8597\n",
      "Epoch [3/60], Batch [200/447], Loss: 3.6415\n",
      "Epoch [3/60], Batch [210/447], Loss: 3.8346\n",
      "Epoch [3/60], Batch [220/447], Loss: 3.7236\n",
      "Epoch [3/60], Batch [230/447], Loss: 3.4773\n",
      "Epoch [3/60], Batch [240/447], Loss: 3.5636\n",
      "Epoch [3/60], Batch [250/447], Loss: 3.5818\n",
      "Epoch [3/60], Batch [260/447], Loss: 3.4726\n",
      "Epoch [3/60], Batch [270/447], Loss: 3.7052\n",
      "Epoch [3/60], Batch [280/447], Loss: 3.4603\n",
      "Epoch [3/60], Batch [290/447], Loss: 3.6684\n",
      "Epoch [3/60], Batch [300/447], Loss: 3.7457\n",
      "Epoch [3/60], Batch [310/447], Loss: 3.4732\n",
      "Epoch [3/60], Batch [320/447], Loss: 3.6634\n",
      "Epoch [3/60], Batch [330/447], Loss: 3.3442\n",
      "Epoch [3/60], Batch [340/447], Loss: 3.2785\n",
      "Epoch [3/60], Batch [350/447], Loss: 3.7221\n",
      "Epoch [3/60], Batch [360/447], Loss: 3.5710\n",
      "Epoch [3/60], Batch [370/447], Loss: 3.8124\n",
      "Epoch [3/60], Batch [380/447], Loss: 3.7502\n",
      "Epoch [3/60], Batch [390/447], Loss: 3.4256\n",
      "Epoch [3/60], Batch [400/447], Loss: 3.4471\n",
      "Epoch [3/60], Batch [410/447], Loss: 3.4728\n",
      "Epoch [3/60], Batch [420/447], Loss: 3.7210\n",
      "Epoch [3/60], Batch [430/447], Loss: 3.5733\n",
      "Epoch [3/60], Batch [440/447], Loss: 3.7223\n",
      "Epoch [3/60], Average Train Loss: 3.6385\n",
      "Epoch [3/60], Validation Accuracy: 0.4549, Validation Loss: 2.8442\n",
      "New best validation accuracy: 0.4549\n",
      "Epoch [4/60], Batch [0/447], Loss: 3.7117\n",
      "Epoch [4/60], Batch [10/447], Loss: 3.3099\n",
      "Epoch [4/60], Batch [20/447], Loss: 3.2990\n",
      "Epoch [4/60], Batch [30/447], Loss: 3.5293\n",
      "Epoch [4/60], Batch [40/447], Loss: 3.6265\n",
      "Epoch [4/60], Batch [50/447], Loss: 3.8144\n",
      "Epoch [4/60], Batch [60/447], Loss: 3.6165\n",
      "Epoch [4/60], Batch [70/447], Loss: 3.4728\n",
      "Epoch [4/60], Batch [80/447], Loss: 3.4378\n",
      "Epoch [4/60], Batch [90/447], Loss: 3.6682\n",
      "Epoch [4/60], Batch [100/447], Loss: 3.7007\n",
      "Epoch [4/60], Batch [110/447], Loss: 3.3275\n",
      "Epoch [4/60], Batch [120/447], Loss: 3.7240\n",
      "Epoch [4/60], Batch [130/447], Loss: 3.6808\n",
      "Epoch [4/60], Batch [140/447], Loss: 3.6625\n",
      "Epoch [4/60], Batch [150/447], Loss: 3.4963\n",
      "Epoch [4/60], Batch [160/447], Loss: 3.6977\n",
      "Epoch [4/60], Batch [170/447], Loss: 3.6598\n",
      "Epoch [4/60], Batch [180/447], Loss: 3.7986\n",
      "Epoch [4/60], Batch [190/447], Loss: 3.5849\n",
      "Epoch [4/60], Batch [200/447], Loss: 3.6764\n",
      "Epoch [4/60], Batch [210/447], Loss: 3.8030\n",
      "Epoch [4/60], Batch [220/447], Loss: 3.4775\n",
      "Epoch [4/60], Batch [230/447], Loss: 3.4485\n",
      "Epoch [4/60], Batch [240/447], Loss: 3.3758\n",
      "Epoch [4/60], Batch [250/447], Loss: 3.6157\n",
      "Epoch [4/60], Batch [260/447], Loss: 3.4225\n",
      "Epoch [4/60], Batch [270/447], Loss: 3.5072\n",
      "Epoch [4/60], Batch [280/447], Loss: 3.4197\n",
      "Epoch [4/60], Batch [290/447], Loss: 3.3948\n",
      "Epoch [4/60], Batch [300/447], Loss: 3.4647\n",
      "Epoch [4/60], Batch [310/447], Loss: 3.3074\n",
      "Epoch [4/60], Batch [320/447], Loss: 3.4690\n",
      "Epoch [4/60], Batch [330/447], Loss: 3.6389\n",
      "Epoch [4/60], Batch [340/447], Loss: 3.5085\n",
      "Epoch [4/60], Batch [350/447], Loss: 3.1736\n",
      "Epoch [4/60], Batch [360/447], Loss: 3.4549\n",
      "Epoch [4/60], Batch [370/447], Loss: 3.5906\n",
      "Epoch [4/60], Batch [380/447], Loss: 3.7312\n",
      "Epoch [4/60], Batch [390/447], Loss: 3.5221\n",
      "Epoch [4/60], Batch [400/447], Loss: 3.4350\n",
      "Epoch [4/60], Batch [410/447], Loss: 3.4270\n",
      "Epoch [4/60], Batch [420/447], Loss: 3.1723\n",
      "Epoch [4/60], Batch [430/447], Loss: 3.3046\n",
      "Epoch [4/60], Batch [440/447], Loss: 3.4436\n",
      "Epoch [4/60], Average Train Loss: 3.5124\n",
      "Epoch [4/60], Validation Accuracy: 0.5105, Validation Loss: 2.5340\n",
      "New best validation accuracy: 0.5105\n",
      "Epoch [5/60], Batch [0/447], Loss: 3.2736\n",
      "Epoch [5/60], Batch [10/447], Loss: 3.4464\n",
      "Epoch [5/60], Batch [20/447], Loss: 3.5678\n",
      "Epoch [5/60], Batch [30/447], Loss: 3.5957\n",
      "Epoch [5/60], Batch [40/447], Loss: 3.4344\n",
      "Epoch [5/60], Batch [50/447], Loss: 3.4086\n",
      "Epoch [5/60], Batch [60/447], Loss: 3.1034\n",
      "Epoch [5/60], Batch [70/447], Loss: 3.5314\n",
      "Epoch [5/60], Batch [80/447], Loss: 3.3997\n",
      "Epoch [5/60], Batch [90/447], Loss: 3.4695\n",
      "Epoch [5/60], Batch [100/447], Loss: 3.6135\n",
      "Epoch [5/60], Batch [110/447], Loss: 3.3685\n",
      "Epoch [5/60], Batch [120/447], Loss: 3.5251\n",
      "Epoch [5/60], Batch [130/447], Loss: 3.3296\n",
      "Epoch [5/60], Batch [140/447], Loss: 3.2597\n",
      "Epoch [5/60], Batch [150/447], Loss: 3.3693\n",
      "Epoch [5/60], Batch [160/447], Loss: 3.4203\n",
      "Epoch [5/60], Batch [170/447], Loss: 3.2130\n",
      "Epoch [5/60], Batch [180/447], Loss: 3.3814\n",
      "Epoch [5/60], Batch [190/447], Loss: 3.7072\n",
      "Epoch [5/60], Batch [200/447], Loss: 3.2811\n",
      "Epoch [5/60], Batch [210/447], Loss: 3.3133\n",
      "Epoch [5/60], Batch [220/447], Loss: 3.2846\n",
      "Epoch [5/60], Batch [230/447], Loss: 3.0844\n",
      "Epoch [5/60], Batch [240/447], Loss: 3.4623\n",
      "Epoch [5/60], Batch [250/447], Loss: 3.6760\n",
      "Epoch [5/60], Batch [260/447], Loss: 3.4609\n",
      "Epoch [5/60], Batch [270/447], Loss: 3.3364\n",
      "Epoch [5/60], Batch [280/447], Loss: 3.2341\n",
      "Epoch [5/60], Batch [290/447], Loss: 3.4171\n",
      "Epoch [5/60], Batch [300/447], Loss: 3.1872\n",
      "Epoch [5/60], Batch [310/447], Loss: 3.3206\n",
      "Epoch [5/60], Batch [320/447], Loss: 3.2953\n",
      "Epoch [5/60], Batch [330/447], Loss: 3.1058\n",
      "Epoch [5/60], Batch [340/447], Loss: 3.5431\n",
      "Epoch [5/60], Batch [350/447], Loss: 3.3705\n",
      "Epoch [5/60], Batch [360/447], Loss: 3.4615\n",
      "Epoch [5/60], Batch [370/447], Loss: 3.0991\n",
      "Epoch [5/60], Batch [380/447], Loss: 3.3760\n",
      "Epoch [5/60], Batch [390/447], Loss: 3.4607\n",
      "Epoch [5/60], Batch [400/447], Loss: 3.5589\n",
      "Epoch [5/60], Batch [410/447], Loss: 3.1462\n",
      "Epoch [5/60], Batch [420/447], Loss: 3.3314\n",
      "Epoch [5/60], Batch [430/447], Loss: 3.5876\n",
      "Epoch [5/60], Batch [440/447], Loss: 3.6754\n",
      "Epoch [5/60], Average Train Loss: 3.3981\n",
      "Epoch [5/60], Validation Accuracy: 0.5451, Validation Loss: 2.2776\n",
      "New best validation accuracy: 0.5451\n",
      "Epoch [6/60], Batch [0/447], Loss: 3.4562\n",
      "Epoch [6/60], Batch [10/447], Loss: 2.6680\n",
      "Epoch [6/60], Batch [20/447], Loss: 3.5471\n",
      "Epoch [6/60], Batch [30/447], Loss: 3.0321\n",
      "Epoch [6/60], Batch [40/447], Loss: 3.4189\n",
      "Epoch [6/60], Batch [50/447], Loss: 3.1371\n",
      "Epoch [6/60], Batch [60/447], Loss: 3.8978\n",
      "Epoch [6/60], Batch [70/447], Loss: 3.3818\n",
      "Epoch [6/60], Batch [80/447], Loss: 3.2679\n",
      "Epoch [6/60], Batch [90/447], Loss: 3.1820\n",
      "Epoch [6/60], Batch [100/447], Loss: 3.5620\n",
      "Epoch [6/60], Batch [110/447], Loss: 3.3440\n",
      "Epoch [6/60], Batch [120/447], Loss: 2.9656\n",
      "Epoch [6/60], Batch [130/447], Loss: 3.2273\n",
      "Epoch [6/60], Batch [140/447], Loss: 3.4069\n",
      "Epoch [6/60], Batch [150/447], Loss: 3.1013\n",
      "Epoch [6/60], Batch [160/447], Loss: 3.5071\n",
      "Epoch [6/60], Batch [170/447], Loss: 2.9771\n",
      "Epoch [6/60], Batch [180/447], Loss: 3.2363\n",
      "Epoch [6/60], Batch [190/447], Loss: 3.4769\n",
      "Epoch [6/60], Batch [200/447], Loss: 3.2710\n",
      "Epoch [6/60], Batch [210/447], Loss: 3.3114\n",
      "Epoch [6/60], Batch [220/447], Loss: 3.6738\n",
      "Epoch [6/60], Batch [230/447], Loss: 3.0373\n",
      "Epoch [6/60], Batch [240/447], Loss: 3.1772\n",
      "Epoch [6/60], Batch [250/447], Loss: 3.5406\n",
      "Epoch [6/60], Batch [260/447], Loss: 3.4695\n",
      "Epoch [6/60], Batch [270/447], Loss: 2.9470\n",
      "Epoch [6/60], Batch [280/447], Loss: 3.3320\n",
      "Epoch [6/60], Batch [290/447], Loss: 3.3450\n",
      "Epoch [6/60], Batch [300/447], Loss: 3.3146\n",
      "Epoch [6/60], Batch [310/447], Loss: 3.3949\n",
      "Epoch [6/60], Batch [320/447], Loss: 3.3601\n",
      "Epoch [6/60], Batch [330/447], Loss: 3.3657\n",
      "Epoch [6/60], Batch [340/447], Loss: 3.1478\n",
      "Epoch [6/60], Batch [350/447], Loss: 2.9606\n",
      "Epoch [6/60], Batch [360/447], Loss: 3.1721\n",
      "Epoch [6/60], Batch [370/447], Loss: 3.3205\n",
      "Epoch [6/60], Batch [380/447], Loss: 3.4329\n",
      "Epoch [6/60], Batch [390/447], Loss: 3.4615\n",
      "Epoch [6/60], Batch [400/447], Loss: 3.4163\n",
      "Epoch [6/60], Batch [410/447], Loss: 3.5013\n",
      "Epoch [6/60], Batch [420/447], Loss: 3.7951\n",
      "Epoch [6/60], Batch [430/447], Loss: 3.3987\n",
      "Epoch [6/60], Batch [440/447], Loss: 3.6824\n",
      "Epoch [6/60], Average Train Loss: 3.3067\n",
      "Epoch [6/60], Validation Accuracy: 0.5791, Validation Loss: 2.0879\n",
      "New best validation accuracy: 0.5791\n",
      "Epoch [7/60], Batch [0/447], Loss: 3.1736\n",
      "Epoch [7/60], Batch [10/447], Loss: 3.0006\n",
      "Epoch [7/60], Batch [20/447], Loss: 3.1165\n",
      "Epoch [7/60], Batch [30/447], Loss: 3.3949\n",
      "Epoch [7/60], Batch [40/447], Loss: 3.5775\n",
      "Epoch [7/60], Batch [50/447], Loss: 3.2896\n",
      "Epoch [7/60], Batch [60/447], Loss: 3.1491\n",
      "Epoch [7/60], Batch [70/447], Loss: 3.0527\n",
      "Epoch [7/60], Batch [80/447], Loss: 3.3007\n",
      "Epoch [7/60], Batch [90/447], Loss: 2.3782\n",
      "Epoch [7/60], Batch [100/447], Loss: 3.3360\n",
      "Epoch [7/60], Batch [110/447], Loss: 3.4749\n",
      "Epoch [7/60], Batch [120/447], Loss: 3.2652\n",
      "Epoch [7/60], Batch [130/447], Loss: 3.2446\n",
      "Epoch [7/60], Batch [140/447], Loss: 3.0541\n",
      "Epoch [7/60], Batch [150/447], Loss: 3.3410\n",
      "Epoch [7/60], Batch [160/447], Loss: 2.8798\n",
      "Epoch [7/60], Batch [170/447], Loss: 3.4597\n",
      "Epoch [7/60], Batch [180/447], Loss: 3.3291\n",
      "Epoch [7/60], Batch [190/447], Loss: 3.0817\n",
      "Epoch [7/60], Batch [200/447], Loss: 2.5166\n",
      "Epoch [7/60], Batch [210/447], Loss: 3.0332\n",
      "Epoch [7/60], Batch [220/447], Loss: 3.3590\n",
      "Epoch [7/60], Batch [230/447], Loss: 2.7515\n",
      "Epoch [7/60], Batch [240/447], Loss: 3.2763\n",
      "Epoch [7/60], Batch [250/447], Loss: 3.1090\n",
      "Epoch [7/60], Batch [260/447], Loss: 2.4739\n",
      "Epoch [7/60], Batch [270/447], Loss: 3.0502\n",
      "Epoch [7/60], Batch [280/447], Loss: 2.8616\n",
      "Epoch [7/60], Batch [290/447], Loss: 3.0118\n",
      "Epoch [7/60], Batch [300/447], Loss: 3.5411\n",
      "Epoch [7/60], Batch [310/447], Loss: 2.8716\n",
      "Epoch [7/60], Batch [320/447], Loss: 3.4262\n",
      "Epoch [7/60], Batch [330/447], Loss: 3.4298\n",
      "Epoch [7/60], Batch [340/447], Loss: 3.1894\n",
      "Epoch [7/60], Batch [350/447], Loss: 3.6946\n",
      "Epoch [7/60], Batch [360/447], Loss: 2.9510\n",
      "Epoch [7/60], Batch [370/447], Loss: 2.7171\n",
      "Epoch [7/60], Batch [380/447], Loss: 3.6664\n",
      "Epoch [7/60], Batch [390/447], Loss: 2.9128\n",
      "Epoch [7/60], Batch [400/447], Loss: 3.3161\n",
      "Epoch [7/60], Batch [410/447], Loss: 2.8281\n",
      "Epoch [7/60], Batch [420/447], Loss: 3.0284\n",
      "Epoch [7/60], Batch [430/447], Loss: 3.1320\n",
      "Epoch [7/60], Batch [440/447], Loss: 3.3658\n",
      "Epoch [7/60], Average Train Loss: 3.2059\n",
      "Epoch [7/60], Validation Accuracy: 0.6020, Validation Loss: 1.9457\n",
      "New best validation accuracy: 0.6020\n",
      "Epoch [8/60], Batch [0/447], Loss: 2.8767\n",
      "Epoch [8/60], Batch [10/447], Loss: 3.4073\n",
      "Epoch [8/60], Batch [20/447], Loss: 3.1311\n",
      "Epoch [8/60], Batch [30/447], Loss: 2.9390\n",
      "Epoch [8/60], Batch [40/447], Loss: 3.0808\n",
      "Epoch [8/60], Batch [50/447], Loss: 3.0716\n",
      "Epoch [8/60], Batch [60/447], Loss: 3.5394\n",
      "Epoch [8/60], Batch [70/447], Loss: 3.0887\n",
      "Epoch [8/60], Batch [80/447], Loss: 3.2626\n",
      "Epoch [8/60], Batch [90/447], Loss: 2.9429\n",
      "Epoch [8/60], Batch [100/447], Loss: 2.5970\n",
      "Epoch [8/60], Batch [110/447], Loss: 2.6692\n",
      "Epoch [8/60], Batch [120/447], Loss: 3.5941\n",
      "Epoch [8/60], Batch [130/447], Loss: 3.3267\n",
      "Epoch [8/60], Batch [140/447], Loss: 3.5717\n",
      "Epoch [8/60], Batch [150/447], Loss: 3.5803\n",
      "Epoch [8/60], Batch [160/447], Loss: 3.6027\n",
      "Epoch [8/60], Batch [170/447], Loss: 3.2064\n",
      "Epoch [8/60], Batch [180/447], Loss: 3.5117\n",
      "Epoch [8/60], Batch [190/447], Loss: 2.6940\n",
      "Epoch [8/60], Batch [200/447], Loss: 3.5653\n",
      "Epoch [8/60], Batch [210/447], Loss: 2.8408\n",
      "Epoch [8/60], Batch [220/447], Loss: 2.6531\n",
      "Epoch [8/60], Batch [230/447], Loss: 3.0822\n",
      "Epoch [8/60], Batch [240/447], Loss: 3.6433\n",
      "Epoch [8/60], Batch [250/447], Loss: 3.0487\n",
      "Epoch [8/60], Batch [260/447], Loss: 3.2770\n",
      "Epoch [8/60], Batch [270/447], Loss: 3.3692\n",
      "Epoch [8/60], Batch [280/447], Loss: 3.1854\n",
      "Epoch [8/60], Batch [290/447], Loss: 3.3968\n",
      "Epoch [8/60], Batch [300/447], Loss: 2.7687\n",
      "Epoch [8/60], Batch [310/447], Loss: 3.0132\n",
      "Epoch [8/60], Batch [320/447], Loss: 2.4980\n",
      "Epoch [8/60], Batch [330/447], Loss: 2.9998\n",
      "Epoch [8/60], Batch [340/447], Loss: 2.9485\n",
      "Epoch [8/60], Batch [350/447], Loss: 2.8996\n",
      "Epoch [8/60], Batch [360/447], Loss: 3.1076\n",
      "Epoch [8/60], Batch [370/447], Loss: 3.2306\n",
      "Epoch [8/60], Batch [380/447], Loss: 3.5190\n",
      "Epoch [8/60], Batch [390/447], Loss: 3.3152\n",
      "Epoch [8/60], Batch [400/447], Loss: 3.4581\n",
      "Epoch [8/60], Batch [410/447], Loss: 3.0588\n",
      "Epoch [8/60], Batch [420/447], Loss: 3.5024\n",
      "Epoch [8/60], Batch [430/447], Loss: 2.9662\n",
      "Epoch [8/60], Batch [440/447], Loss: 3.2647\n",
      "Epoch [8/60], Average Train Loss: 3.1188\n",
      "Epoch [8/60], Validation Accuracy: 0.6209, Validation Loss: 1.7858\n",
      "New best validation accuracy: 0.6209\n",
      "Epoch [9/60], Batch [0/447], Loss: 3.0738\n",
      "Epoch [9/60], Batch [10/447], Loss: 3.0190\n",
      "Epoch [9/60], Batch [20/447], Loss: 2.9460\n",
      "Epoch [9/60], Batch [30/447], Loss: 3.2296\n",
      "Epoch [9/60], Batch [40/447], Loss: 2.8215\n",
      "Epoch [9/60], Batch [50/447], Loss: 3.1631\n",
      "Epoch [9/60], Batch [60/447], Loss: 3.0248\n",
      "Epoch [9/60], Batch [70/447], Loss: 3.3062\n",
      "Epoch [9/60], Batch [80/447], Loss: 2.8571\n",
      "Epoch [9/60], Batch [90/447], Loss: 3.0079\n",
      "Epoch [9/60], Batch [100/447], Loss: 2.9247\n",
      "Epoch [9/60], Batch [110/447], Loss: 3.1598\n",
      "Epoch [9/60], Batch [120/447], Loss: 3.4336\n",
      "Epoch [9/60], Batch [130/447], Loss: 3.2183\n",
      "Epoch [9/60], Batch [140/447], Loss: 3.0879\n",
      "Epoch [9/60], Batch [150/447], Loss: 2.9553\n",
      "Epoch [9/60], Batch [160/447], Loss: 3.0148\n",
      "Epoch [9/60], Batch [170/447], Loss: 3.0016\n",
      "Epoch [9/60], Batch [180/447], Loss: 3.4437\n",
      "Epoch [9/60], Batch [190/447], Loss: 2.7018\n",
      "Epoch [9/60], Batch [200/447], Loss: 3.0829\n",
      "Epoch [9/60], Batch [210/447], Loss: 2.9114\n",
      "Epoch [9/60], Batch [220/447], Loss: 2.6799\n",
      "Epoch [9/60], Batch [230/447], Loss: 2.9473\n",
      "Epoch [9/60], Batch [240/447], Loss: 3.1493\n",
      "Epoch [9/60], Batch [250/447], Loss: 3.1653\n",
      "Epoch [9/60], Batch [260/447], Loss: 3.3954\n",
      "Epoch [9/60], Batch [270/447], Loss: 2.8358\n",
      "Epoch [9/60], Batch [280/447], Loss: 3.0775\n",
      "Epoch [9/60], Batch [290/447], Loss: 3.1798\n",
      "Epoch [9/60], Batch [300/447], Loss: 2.8599\n",
      "Epoch [9/60], Batch [310/447], Loss: 3.3893\n",
      "Epoch [9/60], Batch [320/447], Loss: 2.9473\n",
      "Epoch [9/60], Batch [330/447], Loss: 3.3811\n",
      "Epoch [9/60], Batch [340/447], Loss: 3.3305\n",
      "Epoch [9/60], Batch [350/447], Loss: 2.4970\n",
      "Epoch [9/60], Batch [360/447], Loss: 2.7216\n",
      "Epoch [9/60], Batch [370/447], Loss: 2.5097\n",
      "Epoch [9/60], Batch [380/447], Loss: 2.6990\n",
      "Epoch [9/60], Batch [390/447], Loss: 3.1981\n",
      "Epoch [9/60], Batch [400/447], Loss: 3.1219\n",
      "Epoch [9/60], Batch [410/447], Loss: 2.8019\n",
      "Epoch [9/60], Batch [420/447], Loss: 3.0361\n",
      "Epoch [9/60], Batch [430/447], Loss: 3.2359\n",
      "Epoch [9/60], Batch [440/447], Loss: 2.9522\n",
      "Epoch [9/60], Average Train Loss: 3.0499\n",
      "Epoch [9/60], Validation Accuracy: 0.6242, Validation Loss: 1.6468\n",
      "New best validation accuracy: 0.6242\n",
      "Epoch [10/60], Batch [0/447], Loss: 3.2838\n",
      "Epoch [10/60], Batch [10/447], Loss: 3.0902\n",
      "Epoch [10/60], Batch [20/447], Loss: 2.7893\n",
      "Epoch [10/60], Batch [30/447], Loss: 2.8102\n",
      "Epoch [10/60], Batch [40/447], Loss: 2.8082\n",
      "Epoch [10/60], Batch [50/447], Loss: 2.8681\n",
      "Epoch [10/60], Batch [60/447], Loss: 3.0558\n",
      "Epoch [10/60], Batch [70/447], Loss: 2.8675\n",
      "Epoch [10/60], Batch [80/447], Loss: 3.1669\n",
      "Epoch [10/60], Batch [90/447], Loss: 3.4269\n",
      "Epoch [10/60], Batch [100/447], Loss: 3.3851\n",
      "Epoch [10/60], Batch [110/447], Loss: 3.0026\n",
      "Epoch [10/60], Batch [120/447], Loss: 2.3141\n",
      "Epoch [10/60], Batch [130/447], Loss: 3.3247\n",
      "Epoch [10/60], Batch [140/447], Loss: 3.2934\n",
      "Epoch [10/60], Batch [150/447], Loss: 2.7458\n",
      "Epoch [10/60], Batch [160/447], Loss: 3.1490\n",
      "Epoch [10/60], Batch [170/447], Loss: 2.4942\n",
      "Epoch [10/60], Batch [180/447], Loss: 2.8190\n",
      "Epoch [10/60], Batch [190/447], Loss: 3.4886\n",
      "Epoch [10/60], Batch [200/447], Loss: 3.1252\n",
      "Epoch [10/60], Batch [210/447], Loss: 2.9638\n",
      "Epoch [10/60], Batch [220/447], Loss: 3.2747\n",
      "Epoch [10/60], Batch [230/447], Loss: 3.1562\n",
      "Epoch [10/60], Batch [240/447], Loss: 2.8453\n",
      "Epoch [10/60], Batch [250/447], Loss: 3.3124\n",
      "Epoch [10/60], Batch [260/447], Loss: 2.8071\n",
      "Epoch [10/60], Batch [270/447], Loss: 2.7923\n",
      "Epoch [10/60], Batch [280/447], Loss: 3.1978\n",
      "Epoch [10/60], Batch [290/447], Loss: 3.6057\n",
      "Epoch [10/60], Batch [300/447], Loss: 2.8441\n",
      "Epoch [10/60], Batch [310/447], Loss: 3.0529\n",
      "Epoch [10/60], Batch [320/447], Loss: 3.1442\n",
      "Epoch [10/60], Batch [330/447], Loss: 2.7621\n",
      "Epoch [10/60], Batch [340/447], Loss: 2.9476\n",
      "Epoch [10/60], Batch [350/447], Loss: 3.4107\n",
      "Epoch [10/60], Batch [360/447], Loss: 2.2456\n",
      "Epoch [10/60], Batch [370/447], Loss: 3.1315\n",
      "Epoch [10/60], Batch [380/447], Loss: 2.9363\n",
      "Epoch [10/60], Batch [390/447], Loss: 2.9629\n",
      "Epoch [10/60], Batch [400/447], Loss: 2.8870\n",
      "Epoch [10/60], Batch [410/447], Loss: 3.3070\n",
      "Epoch [10/60], Batch [420/447], Loss: 2.4878\n",
      "Epoch [10/60], Batch [430/447], Loss: 3.5163\n",
      "Epoch [10/60], Batch [440/447], Loss: 2.3374\n",
      "Epoch [10/60], Average Train Loss: 2.9840\n",
      "Epoch [10/60], Validation Accuracy: 0.6562, Validation Loss: 1.5780\n",
      "New best validation accuracy: 0.6562\n",
      "Epoch [11/60], Batch [0/447], Loss: 3.2791\n",
      "Epoch [11/60], Batch [10/447], Loss: 2.6126\n",
      "Epoch [11/60], Batch [20/447], Loss: 2.8341\n",
      "Epoch [11/60], Batch [30/447], Loss: 2.8741\n",
      "Epoch [11/60], Batch [40/447], Loss: 3.2125\n",
      "Epoch [11/60], Batch [50/447], Loss: 2.8442\n",
      "Epoch [11/60], Batch [60/447], Loss: 2.9645\n",
      "Epoch [11/60], Batch [70/447], Loss: 2.9510\n",
      "Epoch [11/60], Batch [80/447], Loss: 2.7502\n",
      "Epoch [11/60], Batch [90/447], Loss: 3.1389\n",
      "Epoch [11/60], Batch [100/447], Loss: 3.0152\n",
      "Epoch [11/60], Batch [110/447], Loss: 2.5238\n",
      "Epoch [11/60], Batch [120/447], Loss: 3.1037\n",
      "Epoch [11/60], Batch [130/447], Loss: 2.9551\n",
      "Epoch [11/60], Batch [140/447], Loss: 3.0130\n",
      "Epoch [11/60], Batch [150/447], Loss: 2.9691\n",
      "Epoch [11/60], Batch [160/447], Loss: 3.1173\n",
      "Epoch [11/60], Batch [170/447], Loss: 2.4760\n",
      "Epoch [11/60], Batch [180/447], Loss: 3.1414\n",
      "Epoch [11/60], Batch [190/447], Loss: 2.9136\n",
      "Epoch [11/60], Batch [200/447], Loss: 3.0039\n",
      "Epoch [11/60], Batch [210/447], Loss: 3.1215\n",
      "Epoch [11/60], Batch [220/447], Loss: 2.9383\n",
      "Epoch [11/60], Batch [230/447], Loss: 3.1233\n",
      "Epoch [11/60], Batch [240/447], Loss: 2.9106\n",
      "Epoch [11/60], Batch [250/447], Loss: 3.1977\n",
      "Epoch [11/60], Batch [260/447], Loss: 2.4476\n",
      "Epoch [11/60], Batch [270/447], Loss: 2.5157\n",
      "Epoch [11/60], Batch [280/447], Loss: 3.3019\n",
      "Epoch [11/60], Batch [290/447], Loss: 3.3089\n",
      "Epoch [11/60], Batch [300/447], Loss: 2.8857\n",
      "Epoch [11/60], Batch [310/447], Loss: 2.8842\n",
      "Epoch [11/60], Batch [320/447], Loss: 2.7893\n",
      "Epoch [11/60], Batch [330/447], Loss: 2.1730\n",
      "Epoch [11/60], Batch [340/447], Loss: 3.2605\n",
      "Epoch [11/60], Batch [350/447], Loss: 2.7503\n",
      "Epoch [11/60], Batch [360/447], Loss: 3.0388\n",
      "Epoch [11/60], Batch [370/447], Loss: 2.5476\n",
      "Epoch [11/60], Batch [380/447], Loss: 3.3076\n",
      "Epoch [11/60], Batch [390/447], Loss: 3.0155\n",
      "Epoch [11/60], Batch [400/447], Loss: 2.6565\n",
      "Epoch [11/60], Batch [410/447], Loss: 2.8136\n",
      "Epoch [11/60], Batch [420/447], Loss: 3.1569\n",
      "Epoch [11/60], Batch [430/447], Loss: 3.3977\n",
      "Epoch [11/60], Batch [440/447], Loss: 3.0641\n",
      "Epoch [11/60], Average Train Loss: 2.9153\n",
      "Epoch [11/60], Validation Accuracy: 0.6654, Validation Loss: 1.4653\n",
      "New best validation accuracy: 0.6654\n",
      "Epoch [12/60], Batch [0/447], Loss: 3.0815\n",
      "Epoch [12/60], Batch [10/447], Loss: 2.5486\n",
      "Epoch [12/60], Batch [20/447], Loss: 3.2877\n",
      "Epoch [12/60], Batch [30/447], Loss: 2.3155\n",
      "Epoch [12/60], Batch [40/447], Loss: 3.2107\n",
      "Epoch [12/60], Batch [50/447], Loss: 3.1612\n",
      "Epoch [12/60], Batch [60/447], Loss: 3.2488\n",
      "Epoch [12/60], Batch [70/447], Loss: 2.9172\n",
      "Epoch [12/60], Batch [80/447], Loss: 2.7819\n",
      "Epoch [12/60], Batch [90/447], Loss: 2.4527\n",
      "Epoch [12/60], Batch [100/447], Loss: 3.3090\n",
      "Epoch [12/60], Batch [110/447], Loss: 2.2964\n",
      "Epoch [12/60], Batch [120/447], Loss: 2.8985\n",
      "Epoch [12/60], Batch [130/447], Loss: 2.8261\n",
      "Epoch [12/60], Batch [140/447], Loss: 3.1483\n",
      "Epoch [12/60], Batch [150/447], Loss: 3.0198\n",
      "Epoch [12/60], Batch [160/447], Loss: 2.3161\n",
      "Epoch [12/60], Batch [170/447], Loss: 2.6193\n",
      "Epoch [12/60], Batch [180/447], Loss: 2.9708\n",
      "Epoch [12/60], Batch [190/447], Loss: 2.9584\n",
      "Epoch [12/60], Batch [200/447], Loss: 3.1667\n",
      "Epoch [12/60], Batch [210/447], Loss: 2.4136\n",
      "Epoch [12/60], Batch [220/447], Loss: 2.4287\n",
      "Epoch [12/60], Batch [230/447], Loss: 2.5545\n",
      "Epoch [12/60], Batch [240/447], Loss: 2.9954\n",
      "Epoch [12/60], Batch [250/447], Loss: 2.9172\n",
      "Epoch [12/60], Batch [260/447], Loss: 2.7763\n",
      "Epoch [12/60], Batch [270/447], Loss: 3.2765\n",
      "Epoch [12/60], Batch [280/447], Loss: 2.8896\n",
      "Epoch [12/60], Batch [290/447], Loss: 3.0559\n",
      "Epoch [12/60], Batch [300/447], Loss: 2.9516\n",
      "Epoch [12/60], Batch [310/447], Loss: 3.3288\n",
      "Epoch [12/60], Batch [320/447], Loss: 2.6870\n",
      "Epoch [12/60], Batch [330/447], Loss: 3.5398\n",
      "Epoch [12/60], Batch [340/447], Loss: 3.2075\n",
      "Epoch [12/60], Batch [350/447], Loss: 2.9255\n",
      "Epoch [12/60], Batch [360/447], Loss: 2.9000\n",
      "Epoch [12/60], Batch [370/447], Loss: 2.8691\n",
      "Epoch [12/60], Batch [380/447], Loss: 2.5975\n",
      "Epoch [12/60], Batch [390/447], Loss: 2.3942\n",
      "Epoch [12/60], Batch [400/447], Loss: 2.7931\n",
      "Epoch [12/60], Batch [410/447], Loss: 3.3080\n",
      "Epoch [12/60], Batch [420/447], Loss: 3.0669\n",
      "Epoch [12/60], Batch [430/447], Loss: 2.7703\n",
      "Epoch [12/60], Batch [440/447], Loss: 3.2740\n",
      "Epoch [12/60], Average Train Loss: 2.8744\n",
      "Epoch [12/60], Validation Accuracy: 0.6725, Validation Loss: 1.3941\n",
      "New best validation accuracy: 0.6725\n",
      "Epoch [13/60], Batch [0/447], Loss: 2.9097\n",
      "Epoch [13/60], Batch [10/447], Loss: 2.8167\n",
      "Epoch [13/60], Batch [20/447], Loss: 2.7471\n",
      "Epoch [13/60], Batch [30/447], Loss: 2.5820\n",
      "Epoch [13/60], Batch [40/447], Loss: 2.7089\n",
      "Epoch [13/60], Batch [50/447], Loss: 2.4262\n",
      "Epoch [13/60], Batch [60/447], Loss: 2.9664\n",
      "Epoch [13/60], Batch [70/447], Loss: 2.5743\n",
      "Epoch [13/60], Batch [80/447], Loss: 2.5300\n",
      "Epoch [13/60], Batch [90/447], Loss: 2.0725\n",
      "Epoch [13/60], Batch [100/447], Loss: 2.5395\n",
      "Epoch [13/60], Batch [110/447], Loss: 2.1009\n",
      "Epoch [13/60], Batch [120/447], Loss: 3.0829\n",
      "Epoch [13/60], Batch [130/447], Loss: 3.1437\n",
      "Epoch [13/60], Batch [140/447], Loss: 2.9542\n",
      "Epoch [13/60], Batch [150/447], Loss: 2.9645\n",
      "Epoch [13/60], Batch [160/447], Loss: 3.1900\n",
      "Epoch [13/60], Batch [170/447], Loss: 3.0681\n",
      "Epoch [13/60], Batch [180/447], Loss: 2.8754\n",
      "Epoch [13/60], Batch [190/447], Loss: 3.1330\n",
      "Epoch [13/60], Batch [200/447], Loss: 2.9695\n",
      "Epoch [13/60], Batch [210/447], Loss: 2.9198\n",
      "Epoch [13/60], Batch [220/447], Loss: 2.5191\n",
      "Epoch [13/60], Batch [230/447], Loss: 3.2068\n",
      "Epoch [13/60], Batch [240/447], Loss: 3.2042\n",
      "Epoch [13/60], Batch [250/447], Loss: 2.9092\n",
      "Epoch [13/60], Batch [260/447], Loss: 2.7352\n",
      "Epoch [13/60], Batch [270/447], Loss: 2.9385\n",
      "Epoch [13/60], Batch [280/447], Loss: 2.7712\n",
      "Epoch [13/60], Batch [290/447], Loss: 2.9474\n",
      "Epoch [13/60], Batch [300/447], Loss: 3.0402\n",
      "Epoch [13/60], Batch [310/447], Loss: 3.1700\n",
      "Epoch [13/60], Batch [320/447], Loss: 2.9822\n",
      "Epoch [13/60], Batch [330/447], Loss: 3.2461\n",
      "Epoch [13/60], Batch [340/447], Loss: 3.2478\n",
      "Epoch [13/60], Batch [350/447], Loss: 3.3369\n",
      "Epoch [13/60], Batch [360/447], Loss: 2.8126\n",
      "Epoch [13/60], Batch [370/447], Loss: 3.2905\n",
      "Epoch [13/60], Batch [380/447], Loss: 2.5193\n",
      "Epoch [13/60], Batch [390/447], Loss: 3.0239\n",
      "Epoch [13/60], Batch [400/447], Loss: 3.0692\n",
      "Epoch [13/60], Batch [410/447], Loss: 2.9228\n",
      "Epoch [13/60], Batch [420/447], Loss: 3.0808\n",
      "Epoch [13/60], Batch [430/447], Loss: 3.1695\n",
      "Epoch [13/60], Batch [440/447], Loss: 2.7133\n",
      "Epoch [13/60], Average Train Loss: 2.8140\n",
      "Epoch [13/60], Validation Accuracy: 0.6765, Validation Loss: 1.3700\n",
      "New best validation accuracy: 0.6765\n",
      "Epoch [14/60], Batch [0/447], Loss: 2.6963\n",
      "Epoch [14/60], Batch [10/447], Loss: 2.6298\n",
      "Epoch [14/60], Batch [20/447], Loss: 3.2865\n",
      "Epoch [14/60], Batch [30/447], Loss: 2.8040\n",
      "Epoch [14/60], Batch [40/447], Loss: 3.4364\n",
      "Epoch [14/60], Batch [50/447], Loss: 2.5372\n",
      "Epoch [14/60], Batch [60/447], Loss: 2.6485\n",
      "Epoch [14/60], Batch [70/447], Loss: 2.9392\n",
      "Epoch [14/60], Batch [80/447], Loss: 3.0525\n",
      "Epoch [14/60], Batch [90/447], Loss: 2.5344\n",
      "Epoch [14/60], Batch [100/447], Loss: 2.4297\n",
      "Epoch [14/60], Batch [110/447], Loss: 2.5600\n",
      "Epoch [14/60], Batch [120/447], Loss: 2.9704\n",
      "Epoch [14/60], Batch [130/447], Loss: 2.9986\n",
      "Epoch [14/60], Batch [140/447], Loss: 2.5196\n",
      "Epoch [14/60], Batch [150/447], Loss: 2.5678\n",
      "Epoch [14/60], Batch [160/447], Loss: 2.5105\n",
      "Epoch [14/60], Batch [170/447], Loss: 2.4869\n",
      "Epoch [14/60], Batch [180/447], Loss: 2.4572\n",
      "Epoch [14/60], Batch [190/447], Loss: 2.5771\n",
      "Epoch [14/60], Batch [200/447], Loss: 2.7404\n",
      "Epoch [14/60], Batch [210/447], Loss: 2.3678\n",
      "Epoch [14/60], Batch [220/447], Loss: 2.6167\n",
      "Epoch [14/60], Batch [230/447], Loss: 3.0311\n",
      "Epoch [14/60], Batch [240/447], Loss: 2.6308\n",
      "Epoch [14/60], Batch [250/447], Loss: 2.9798\n",
      "Epoch [14/60], Batch [260/447], Loss: 3.2684\n",
      "Epoch [14/60], Batch [270/447], Loss: 2.9018\n",
      "Epoch [14/60], Batch [280/447], Loss: 2.4406\n",
      "Epoch [14/60], Batch [290/447], Loss: 2.4437\n",
      "Epoch [14/60], Batch [300/447], Loss: 2.7786\n",
      "Epoch [14/60], Batch [310/447], Loss: 2.6008\n",
      "Epoch [14/60], Batch [320/447], Loss: 2.8168\n",
      "Epoch [14/60], Batch [330/447], Loss: 3.3588\n",
      "Epoch [14/60], Batch [340/447], Loss: 2.7467\n",
      "Epoch [14/60], Batch [350/447], Loss: 2.3147\n",
      "Epoch [14/60], Batch [360/447], Loss: 2.8736\n",
      "Epoch [14/60], Batch [370/447], Loss: 2.8320\n",
      "Epoch [14/60], Batch [380/447], Loss: 2.9630\n",
      "Epoch [14/60], Batch [390/447], Loss: 3.0687\n",
      "Epoch [14/60], Batch [400/447], Loss: 2.5679\n",
      "Epoch [14/60], Batch [410/447], Loss: 2.9344\n",
      "Epoch [14/60], Batch [420/447], Loss: 2.4272\n",
      "Epoch [14/60], Batch [430/447], Loss: 2.4056\n",
      "Epoch [14/60], Batch [440/447], Loss: 3.0334\n",
      "Epoch [14/60], Average Train Loss: 2.7497\n",
      "Epoch [14/60], Validation Accuracy: 0.6817, Validation Loss: 1.3107\n",
      "New best validation accuracy: 0.6817\n",
      "Epoch [15/60], Batch [0/447], Loss: 2.8034\n",
      "Epoch [15/60], Batch [10/447], Loss: 3.0132\n",
      "Epoch [15/60], Batch [20/447], Loss: 2.9660\n",
      "Epoch [15/60], Batch [30/447], Loss: 2.8893\n",
      "Epoch [15/60], Batch [40/447], Loss: 2.7272\n",
      "Epoch [15/60], Batch [50/447], Loss: 2.9727\n",
      "Epoch [15/60], Batch [60/447], Loss: 2.4160\n",
      "Epoch [15/60], Batch [70/447], Loss: 2.1635\n",
      "Epoch [15/60], Batch [80/447], Loss: 2.8844\n",
      "Epoch [15/60], Batch [90/447], Loss: 2.6625\n",
      "Epoch [15/60], Batch [100/447], Loss: 2.9998\n",
      "Epoch [15/60], Batch [110/447], Loss: 2.9229\n",
      "Epoch [15/60], Batch [120/447], Loss: 2.6436\n",
      "Epoch [15/60], Batch [130/447], Loss: 2.5197\n",
      "Epoch [15/60], Batch [140/447], Loss: 2.8581\n",
      "Epoch [15/60], Batch [150/447], Loss: 2.4959\n",
      "Epoch [15/60], Batch [160/447], Loss: 2.4403\n",
      "Epoch [15/60], Batch [170/447], Loss: 2.9911\n",
      "Epoch [15/60], Batch [180/447], Loss: 2.6070\n",
      "Epoch [15/60], Batch [190/447], Loss: 2.9914\n",
      "Epoch [15/60], Batch [200/447], Loss: 2.5779\n",
      "Epoch [15/60], Batch [210/447], Loss: 2.5535\n",
      "Epoch [15/60], Batch [220/447], Loss: 3.2406\n",
      "Epoch [15/60], Batch [230/447], Loss: 2.8704\n",
      "Epoch [15/60], Batch [240/447], Loss: 2.9292\n",
      "Epoch [15/60], Batch [250/447], Loss: 2.2446\n",
      "Epoch [15/60], Batch [260/447], Loss: 2.1292\n",
      "Epoch [15/60], Batch [270/447], Loss: 2.6168\n",
      "Epoch [15/60], Batch [280/447], Loss: 2.4406\n",
      "Epoch [15/60], Batch [290/447], Loss: 2.5239\n",
      "Epoch [15/60], Batch [300/447], Loss: 2.3826\n",
      "Epoch [15/60], Batch [310/447], Loss: 2.6944\n",
      "Epoch [15/60], Batch [320/447], Loss: 2.7442\n",
      "Epoch [15/60], Batch [330/447], Loss: 2.6204\n",
      "Epoch [15/60], Batch [340/447], Loss: 2.6115\n",
      "Epoch [15/60], Batch [350/447], Loss: 3.3159\n",
      "Epoch [15/60], Batch [360/447], Loss: 2.6771\n",
      "Epoch [15/60], Batch [370/447], Loss: 3.1999\n",
      "Epoch [15/60], Batch [380/447], Loss: 2.8860\n",
      "Epoch [15/60], Batch [390/447], Loss: 2.9281\n",
      "Epoch [15/60], Batch [400/447], Loss: 2.9752\n",
      "Epoch [15/60], Batch [410/447], Loss: 2.6146\n",
      "Epoch [15/60], Batch [420/447], Loss: 2.6498\n",
      "Epoch [15/60], Batch [430/447], Loss: 2.5263\n",
      "Epoch [15/60], Batch [440/447], Loss: 2.9520\n",
      "Epoch [15/60], Average Train Loss: 2.7212\n",
      "Epoch [15/60], Validation Accuracy: 0.7000, Validation Loss: 1.2062\n",
      "New best validation accuracy: 0.7000\n",
      "Epoch [16/60], Batch [0/447], Loss: 2.9408\n",
      "Epoch [16/60], Batch [10/447], Loss: 2.4952\n",
      "Epoch [16/60], Batch [20/447], Loss: 2.3471\n",
      "Epoch [16/60], Batch [30/447], Loss: 2.5887\n",
      "Epoch [16/60], Batch [40/447], Loss: 3.0617\n",
      "Epoch [16/60], Batch [50/447], Loss: 2.6279\n",
      "Epoch [16/60], Batch [60/447], Loss: 1.9404\n",
      "Epoch [16/60], Batch [70/447], Loss: 2.6861\n",
      "Epoch [16/60], Batch [80/447], Loss: 2.3595\n",
      "Epoch [16/60], Batch [90/447], Loss: 2.5961\n",
      "Epoch [16/60], Batch [100/447], Loss: 2.6648\n",
      "Epoch [16/60], Batch [110/447], Loss: 3.3058\n",
      "Epoch [16/60], Batch [120/447], Loss: 2.4458\n",
      "Epoch [16/60], Batch [130/447], Loss: 2.4749\n",
      "Epoch [16/60], Batch [140/447], Loss: 2.9354\n",
      "Epoch [16/60], Batch [150/447], Loss: 1.8081\n",
      "Epoch [16/60], Batch [160/447], Loss: 3.0729\n",
      "Epoch [16/60], Batch [170/447], Loss: 2.8647\n",
      "Epoch [16/60], Batch [180/447], Loss: 3.2759\n",
      "Epoch [16/60], Batch [190/447], Loss: 2.8923\n",
      "Epoch [16/60], Batch [200/447], Loss: 2.8005\n",
      "Epoch [16/60], Batch [210/447], Loss: 2.6138\n",
      "Epoch [16/60], Batch [220/447], Loss: 2.5278\n",
      "Epoch [16/60], Batch [230/447], Loss: 2.4158\n",
      "Epoch [16/60], Batch [240/447], Loss: 2.5162\n",
      "Epoch [16/60], Batch [250/447], Loss: 3.0757\n",
      "Epoch [16/60], Batch [260/447], Loss: 2.9918\n",
      "Epoch [16/60], Batch [270/447], Loss: 2.1069\n",
      "Epoch [16/60], Batch [280/447], Loss: 2.6700\n",
      "Epoch [16/60], Batch [290/447], Loss: 2.4928\n",
      "Epoch [16/60], Batch [300/447], Loss: 2.4514\n",
      "Epoch [16/60], Batch [310/447], Loss: 2.6887\n",
      "Epoch [16/60], Batch [320/447], Loss: 2.5900\n",
      "Epoch [16/60], Batch [330/447], Loss: 2.2404\n",
      "Epoch [16/60], Batch [340/447], Loss: 2.4895\n",
      "Epoch [16/60], Batch [350/447], Loss: 2.8972\n",
      "Epoch [16/60], Batch [360/447], Loss: 2.8002\n",
      "Epoch [16/60], Batch [370/447], Loss: 2.0951\n",
      "Epoch [16/60], Batch [380/447], Loss: 3.2772\n",
      "Epoch [16/60], Batch [390/447], Loss: 2.7868\n",
      "Epoch [16/60], Batch [400/447], Loss: 2.9246\n",
      "Epoch [16/60], Batch [410/447], Loss: 3.0780\n",
      "Epoch [16/60], Batch [420/447], Loss: 2.7839\n",
      "Epoch [16/60], Batch [430/447], Loss: 3.2302\n",
      "Epoch [16/60], Batch [440/447], Loss: 3.0207\n",
      "Epoch [16/60], Average Train Loss: 2.6903\n",
      "Epoch [16/60], Validation Accuracy: 0.7039, Validation Loss: 1.1665\n",
      "New best validation accuracy: 0.7039\n",
      "Epoch [17/60], Batch [0/447], Loss: 2.5324\n",
      "Epoch [17/60], Batch [10/447], Loss: 3.2333\n",
      "Epoch [17/60], Batch [20/447], Loss: 2.5037\n",
      "Epoch [17/60], Batch [30/447], Loss: 2.5187\n",
      "Epoch [17/60], Batch [40/447], Loss: 2.5111\n",
      "Epoch [17/60], Batch [50/447], Loss: 2.8883\n",
      "Epoch [17/60], Batch [60/447], Loss: 3.2395\n",
      "Epoch [17/60], Batch [70/447], Loss: 2.3839\n",
      "Epoch [17/60], Batch [80/447], Loss: 3.0401\n",
      "Epoch [17/60], Batch [90/447], Loss: 2.8874\n",
      "Epoch [17/60], Batch [100/447], Loss: 2.8019\n",
      "Epoch [17/60], Batch [110/447], Loss: 3.0556\n",
      "Epoch [17/60], Batch [120/447], Loss: 2.5191\n",
      "Epoch [17/60], Batch [130/447], Loss: 2.5955\n",
      "Epoch [17/60], Batch [140/447], Loss: 2.9936\n",
      "Epoch [17/60], Batch [150/447], Loss: 1.8944\n",
      "Epoch [17/60], Batch [160/447], Loss: 3.0184\n",
      "Epoch [17/60], Batch [170/447], Loss: 2.4806\n",
      "Epoch [17/60], Batch [180/447], Loss: 2.7476\n",
      "Epoch [17/60], Batch [190/447], Loss: 2.7564\n",
      "Epoch [17/60], Batch [200/447], Loss: 1.9009\n",
      "Epoch [17/60], Batch [210/447], Loss: 3.1321\n",
      "Epoch [17/60], Batch [220/447], Loss: 2.5065\n",
      "Epoch [17/60], Batch [230/447], Loss: 2.7017\n",
      "Epoch [17/60], Batch [240/447], Loss: 2.6252\n",
      "Epoch [17/60], Batch [250/447], Loss: 2.6924\n",
      "Epoch [17/60], Batch [260/447], Loss: 2.2414\n",
      "Epoch [17/60], Batch [270/447], Loss: 2.9450\n",
      "Epoch [17/60], Batch [280/447], Loss: 2.4210\n",
      "Epoch [17/60], Batch [290/447], Loss: 2.9390\n",
      "Epoch [17/60], Batch [300/447], Loss: 2.3483\n",
      "Epoch [17/60], Batch [310/447], Loss: 3.1050\n",
      "Epoch [17/60], Batch [320/447], Loss: 3.0344\n",
      "Epoch [17/60], Batch [330/447], Loss: 2.9449\n",
      "Epoch [17/60], Batch [340/447], Loss: 2.7663\n",
      "Epoch [17/60], Batch [350/447], Loss: 3.1477\n",
      "Epoch [17/60], Batch [360/447], Loss: 2.7522\n",
      "Epoch [17/60], Batch [370/447], Loss: 2.5072\n",
      "Epoch [17/60], Batch [380/447], Loss: 2.7544\n",
      "Epoch [17/60], Batch [390/447], Loss: 2.5238\n",
      "Epoch [17/60], Batch [400/447], Loss: 2.8489\n",
      "Epoch [17/60], Batch [410/447], Loss: 2.5149\n",
      "Epoch [17/60], Batch [420/447], Loss: 2.2641\n",
      "Epoch [17/60], Batch [430/447], Loss: 2.8391\n",
      "Epoch [17/60], Batch [440/447], Loss: 2.3783\n",
      "Epoch [17/60], Average Train Loss: 2.6260\n",
      "Epoch [17/60], Validation Accuracy: 0.7098, Validation Loss: 1.1439\n",
      "New best validation accuracy: 0.7098\n",
      "Epoch [18/60], Batch [0/447], Loss: 2.4651\n",
      "Epoch [18/60], Batch [10/447], Loss: 2.7093\n",
      "Epoch [18/60], Batch [20/447], Loss: 2.5354\n",
      "Epoch [18/60], Batch [30/447], Loss: 2.3962\n",
      "Epoch [18/60], Batch [40/447], Loss: 2.5042\n",
      "Epoch [18/60], Batch [50/447], Loss: 2.8943\n",
      "Epoch [18/60], Batch [60/447], Loss: 2.6389\n",
      "Epoch [18/60], Batch [70/447], Loss: 2.5685\n",
      "Epoch [18/60], Batch [80/447], Loss: 2.8622\n",
      "Epoch [18/60], Batch [90/447], Loss: 2.1961\n",
      "Epoch [18/60], Batch [100/447], Loss: 2.6209\n",
      "Epoch [18/60], Batch [110/447], Loss: 2.5430\n",
      "Epoch [18/60], Batch [120/447], Loss: 2.5148\n",
      "Epoch [18/60], Batch [130/447], Loss: 1.9446\n",
      "Epoch [18/60], Batch [140/447], Loss: 2.4450\n",
      "Epoch [18/60], Batch [150/447], Loss: 2.1974\n",
      "Epoch [18/60], Batch [160/447], Loss: 1.8046\n",
      "Epoch [18/60], Batch [170/447], Loss: 2.7163\n",
      "Epoch [18/60], Batch [180/447], Loss: 2.4783\n",
      "Epoch [18/60], Batch [190/447], Loss: 2.8592\n",
      "Epoch [18/60], Batch [200/447], Loss: 2.0586\n",
      "Epoch [18/60], Batch [210/447], Loss: 1.8877\n",
      "Epoch [18/60], Batch [220/447], Loss: 2.1456\n",
      "Epoch [18/60], Batch [230/447], Loss: 3.1122\n",
      "Epoch [18/60], Batch [240/447], Loss: 2.8738\n",
      "Epoch [18/60], Batch [250/447], Loss: 1.5634\n",
      "Epoch [18/60], Batch [260/447], Loss: 2.9954\n",
      "Epoch [18/60], Batch [270/447], Loss: 2.0066\n",
      "Epoch [18/60], Batch [280/447], Loss: 2.5878\n",
      "Epoch [18/60], Batch [290/447], Loss: 2.8344\n",
      "Epoch [18/60], Batch [300/447], Loss: 3.1114\n",
      "Epoch [18/60], Batch [310/447], Loss: 2.7777\n",
      "Epoch [18/60], Batch [320/447], Loss: 2.5309\n",
      "Epoch [18/60], Batch [330/447], Loss: 2.6672\n",
      "Epoch [18/60], Batch [340/447], Loss: 3.0107\n",
      "Epoch [18/60], Batch [350/447], Loss: 2.2925\n",
      "Epoch [18/60], Batch [360/447], Loss: 2.7686\n",
      "Epoch [18/60], Batch [370/447], Loss: 2.8600\n",
      "Epoch [18/60], Batch [380/447], Loss: 3.2421\n",
      "Epoch [18/60], Batch [390/447], Loss: 2.9002\n",
      "Epoch [18/60], Batch [400/447], Loss: 2.3621\n",
      "Epoch [18/60], Batch [410/447], Loss: 1.7792\n",
      "Epoch [18/60], Batch [420/447], Loss: 2.8228\n",
      "Epoch [18/60], Batch [430/447], Loss: 2.5988\n",
      "Epoch [18/60], Batch [440/447], Loss: 2.5239\n",
      "Epoch [18/60], Average Train Loss: 2.6164\n",
      "Epoch [18/60], Validation Accuracy: 0.7196, Validation Loss: 1.1237\n",
      "New best validation accuracy: 0.7196\n",
      "Epoch [19/60], Batch [0/447], Loss: 2.5956\n",
      "Epoch [19/60], Batch [10/447], Loss: 2.3331\n",
      "Epoch [19/60], Batch [20/447], Loss: 2.6651\n",
      "Epoch [19/60], Batch [30/447], Loss: 2.5012\n",
      "Epoch [19/60], Batch [40/447], Loss: 2.4161\n",
      "Epoch [19/60], Batch [50/447], Loss: 3.1087\n",
      "Epoch [19/60], Batch [60/447], Loss: 2.8918\n",
      "Epoch [19/60], Batch [70/447], Loss: 2.0257\n",
      "Epoch [19/60], Batch [80/447], Loss: 2.1542\n",
      "Epoch [19/60], Batch [90/447], Loss: 2.7765\n",
      "Epoch [19/60], Batch [100/447], Loss: 3.3355\n",
      "Epoch [19/60], Batch [110/447], Loss: 2.9714\n",
      "Epoch [19/60], Batch [120/447], Loss: 1.4926\n",
      "Epoch [19/60], Batch [130/447], Loss: 2.9928\n",
      "Epoch [19/60], Batch [140/447], Loss: 2.3645\n",
      "Epoch [19/60], Batch [150/447], Loss: 2.6969\n",
      "Epoch [19/60], Batch [160/447], Loss: 2.6585\n",
      "Epoch [19/60], Batch [170/447], Loss: 2.9881\n",
      "Epoch [19/60], Batch [180/447], Loss: 2.0598\n",
      "Epoch [19/60], Batch [190/447], Loss: 2.9526\n",
      "Epoch [19/60], Batch [200/447], Loss: 2.5996\n",
      "Epoch [19/60], Batch [210/447], Loss: 2.1066\n",
      "Epoch [19/60], Batch [220/447], Loss: 2.1066\n",
      "Epoch [19/60], Batch [230/447], Loss: 2.8714\n",
      "Epoch [19/60], Batch [240/447], Loss: 2.4634\n",
      "Epoch [19/60], Batch [250/447], Loss: 2.7048\n",
      "Epoch [19/60], Batch [260/447], Loss: 2.7745\n",
      "Epoch [19/60], Batch [270/447], Loss: 2.8803\n",
      "Epoch [19/60], Batch [280/447], Loss: 2.6248\n",
      "Epoch [19/60], Batch [290/447], Loss: 2.2316\n",
      "Epoch [19/60], Batch [300/447], Loss: 2.8969\n",
      "Epoch [19/60], Batch [310/447], Loss: 2.6375\n",
      "Epoch [19/60], Batch [320/447], Loss: 2.8187\n",
      "Epoch [19/60], Batch [330/447], Loss: 2.6989\n",
      "Epoch [19/60], Batch [340/447], Loss: 2.6819\n",
      "Epoch [19/60], Batch [350/447], Loss: 2.4964\n",
      "Epoch [19/60], Batch [360/447], Loss: 2.8972\n",
      "Epoch [19/60], Batch [370/447], Loss: 1.9512\n",
      "Epoch [19/60], Batch [380/447], Loss: 2.8275\n",
      "Epoch [19/60], Batch [390/447], Loss: 2.1529\n",
      "Epoch [19/60], Batch [400/447], Loss: 1.7198\n",
      "Epoch [19/60], Batch [410/447], Loss: 2.3361\n",
      "Epoch [19/60], Batch [420/447], Loss: 2.4725\n",
      "Epoch [19/60], Batch [430/447], Loss: 2.9347\n",
      "Epoch [19/60], Batch [440/447], Loss: 1.8634\n",
      "Epoch [19/60], Average Train Loss: 2.5915\n",
      "Epoch [19/60], Validation Accuracy: 0.7255, Validation Loss: 1.0756\n",
      "New best validation accuracy: 0.7255\n",
      "Epoch [20/60], Batch [0/447], Loss: 2.5608\n",
      "Epoch [20/60], Batch [10/447], Loss: 2.5566\n",
      "Epoch [20/60], Batch [20/447], Loss: 1.7904\n",
      "Epoch [20/60], Batch [30/447], Loss: 2.6517\n",
      "Epoch [20/60], Batch [40/447], Loss: 2.7038\n",
      "Epoch [20/60], Batch [50/447], Loss: 2.6330\n",
      "Epoch [20/60], Batch [60/447], Loss: 2.9125\n",
      "Epoch [20/60], Batch [70/447], Loss: 2.5059\n",
      "Epoch [20/60], Batch [80/447], Loss: 2.2356\n",
      "Epoch [20/60], Batch [90/447], Loss: 2.4802\n",
      "Epoch [20/60], Batch [100/447], Loss: 2.5447\n",
      "Epoch [20/60], Batch [110/447], Loss: 2.5447\n",
      "Epoch [20/60], Batch [120/447], Loss: 2.8896\n",
      "Epoch [20/60], Batch [130/447], Loss: 2.6806\n",
      "Epoch [20/60], Batch [140/447], Loss: 2.2611\n",
      "Epoch [20/60], Batch [150/447], Loss: 2.8037\n",
      "Epoch [20/60], Batch [160/447], Loss: 1.9225\n",
      "Epoch [20/60], Batch [170/447], Loss: 2.9367\n",
      "Epoch [20/60], Batch [180/447], Loss: 2.5672\n",
      "Epoch [20/60], Batch [190/447], Loss: 2.3258\n",
      "Epoch [20/60], Batch [200/447], Loss: 2.5127\n",
      "Epoch [20/60], Batch [210/447], Loss: 2.3293\n",
      "Epoch [20/60], Batch [220/447], Loss: 2.7924\n",
      "Epoch [20/60], Batch [230/447], Loss: 2.8640\n",
      "Epoch [20/60], Batch [240/447], Loss: 2.3333\n",
      "Epoch [20/60], Batch [250/447], Loss: 2.5918\n",
      "Epoch [20/60], Batch [260/447], Loss: 2.7327\n",
      "Epoch [20/60], Batch [270/447], Loss: 2.6520\n",
      "Epoch [20/60], Batch [280/447], Loss: 2.0090\n",
      "Epoch [20/60], Batch [290/447], Loss: 2.0489\n",
      "Epoch [20/60], Batch [300/447], Loss: 2.8775\n",
      "Epoch [20/60], Batch [310/447], Loss: 2.8123\n",
      "Epoch [20/60], Batch [320/447], Loss: 2.3409\n",
      "Epoch [20/60], Batch [330/447], Loss: 2.8352\n",
      "Epoch [20/60], Batch [340/447], Loss: 2.1529\n",
      "Epoch [20/60], Batch [350/447], Loss: 2.0442\n",
      "Epoch [20/60], Batch [360/447], Loss: 3.0315\n",
      "Epoch [20/60], Batch [370/447], Loss: 2.7385\n",
      "Epoch [20/60], Batch [380/447], Loss: 2.5036\n",
      "Epoch [20/60], Batch [390/447], Loss: 2.7739\n",
      "Epoch [20/60], Batch [400/447], Loss: 1.3902\n",
      "Epoch [20/60], Batch [410/447], Loss: 2.5953\n",
      "Epoch [20/60], Batch [420/447], Loss: 2.9510\n",
      "Epoch [20/60], Batch [430/447], Loss: 2.5176\n",
      "Epoch [20/60], Batch [440/447], Loss: 2.2578\n",
      "Epoch [20/60], Average Train Loss: 2.5399\n",
      "Epoch [20/60], Validation Accuracy: 0.7464, Validation Loss: 1.0672\n",
      "New best validation accuracy: 0.7464\n",
      "Epoch [21/60], Batch [0/447], Loss: 1.9598\n",
      "Epoch [21/60], Batch [10/447], Loss: 2.6148\n",
      "Epoch [21/60], Batch [20/447], Loss: 2.8320\n",
      "Epoch [21/60], Batch [30/447], Loss: 2.9887\n",
      "Epoch [21/60], Batch [40/447], Loss: 2.2724\n",
      "Epoch [21/60], Batch [50/447], Loss: 2.1517\n",
      "Epoch [21/60], Batch [60/447], Loss: 2.7486\n",
      "Epoch [21/60], Batch [70/447], Loss: 2.4333\n",
      "Epoch [21/60], Batch [80/447], Loss: 1.9621\n",
      "Epoch [21/60], Batch [90/447], Loss: 2.5807\n",
      "Epoch [21/60], Batch [100/447], Loss: 2.1368\n",
      "Epoch [21/60], Batch [110/447], Loss: 2.5485\n",
      "Epoch [21/60], Batch [120/447], Loss: 2.3953\n",
      "Epoch [21/60], Batch [130/447], Loss: 2.4576\n",
      "Epoch [21/60], Batch [140/447], Loss: 2.7058\n",
      "Epoch [21/60], Batch [150/447], Loss: 2.2576\n",
      "Epoch [21/60], Batch [160/447], Loss: 2.1486\n",
      "Epoch [21/60], Batch [170/447], Loss: 2.4332\n",
      "Epoch [21/60], Batch [180/447], Loss: 2.1769\n",
      "Epoch [21/60], Batch [190/447], Loss: 2.1228\n",
      "Epoch [21/60], Batch [200/447], Loss: 2.3206\n",
      "Epoch [21/60], Batch [210/447], Loss: 1.8911\n",
      "Epoch [21/60], Batch [220/447], Loss: 2.9767\n",
      "Epoch [21/60], Batch [230/447], Loss: 2.7267\n",
      "Epoch [21/60], Batch [240/447], Loss: 2.8042\n",
      "Epoch [21/60], Batch [250/447], Loss: 2.2198\n",
      "Epoch [21/60], Batch [260/447], Loss: 2.8617\n",
      "Epoch [21/60], Batch [270/447], Loss: 2.4506\n",
      "Epoch [21/60], Batch [280/447], Loss: 2.7760\n",
      "Epoch [21/60], Batch [290/447], Loss: 2.1213\n",
      "Epoch [21/60], Batch [300/447], Loss: 2.6929\n",
      "Epoch [21/60], Batch [310/447], Loss: 2.3526\n",
      "Epoch [21/60], Batch [320/447], Loss: 2.6668\n",
      "Epoch [21/60], Batch [330/447], Loss: 2.4111\n",
      "Epoch [21/60], Batch [340/447], Loss: 2.1573\n",
      "Epoch [21/60], Batch [350/447], Loss: 2.8645\n",
      "Epoch [21/60], Batch [360/447], Loss: 2.4486\n",
      "Epoch [21/60], Batch [370/447], Loss: 2.4339\n",
      "Epoch [21/60], Batch [380/447], Loss: 2.9579\n",
      "Epoch [21/60], Batch [390/447], Loss: 2.0228\n",
      "Epoch [21/60], Batch [400/447], Loss: 2.3013\n",
      "Epoch [21/60], Batch [410/447], Loss: 2.9081\n",
      "Epoch [21/60], Batch [420/447], Loss: 2.0759\n",
      "Epoch [21/60], Batch [430/447], Loss: 3.1222\n",
      "Epoch [21/60], Batch [440/447], Loss: 2.3563\n",
      "Epoch [21/60], Average Train Loss: 2.5171\n",
      "Epoch [21/60], Validation Accuracy: 0.7490, Validation Loss: 1.0231\n",
      "New best validation accuracy: 0.7490\n",
      "Epoch [22/60], Batch [0/447], Loss: 2.4359\n",
      "Epoch [22/60], Batch [10/447], Loss: 2.5468\n",
      "Epoch [22/60], Batch [20/447], Loss: 2.2082\n",
      "Epoch [22/60], Batch [30/447], Loss: 2.5636\n",
      "Epoch [22/60], Batch [40/447], Loss: 2.2215\n",
      "Epoch [22/60], Batch [50/447], Loss: 2.3520\n",
      "Epoch [22/60], Batch [60/447], Loss: 2.5716\n",
      "Epoch [22/60], Batch [70/447], Loss: 1.6004\n",
      "Epoch [22/60], Batch [80/447], Loss: 2.5695\n",
      "Epoch [22/60], Batch [90/447], Loss: 2.6898\n",
      "Epoch [22/60], Batch [100/447], Loss: 2.2260\n",
      "Epoch [22/60], Batch [110/447], Loss: 2.1604\n",
      "Epoch [22/60], Batch [120/447], Loss: 2.0997\n",
      "Epoch [22/60], Batch [130/447], Loss: 2.5095\n",
      "Epoch [22/60], Batch [140/447], Loss: 2.7312\n",
      "Epoch [22/60], Batch [150/447], Loss: 2.5877\n",
      "Epoch [22/60], Batch [160/447], Loss: 3.0186\n",
      "Epoch [22/60], Batch [170/447], Loss: 1.9360\n",
      "Epoch [22/60], Batch [180/447], Loss: 2.6335\n",
      "Epoch [22/60], Batch [190/447], Loss: 2.4660\n",
      "Epoch [22/60], Batch [200/447], Loss: 2.1593\n",
      "Epoch [22/60], Batch [210/447], Loss: 2.1427\n",
      "Epoch [22/60], Batch [220/447], Loss: 1.8989\n",
      "Epoch [22/60], Batch [230/447], Loss: 2.1081\n",
      "Epoch [22/60], Batch [240/447], Loss: 3.0994\n",
      "Epoch [22/60], Batch [250/447], Loss: 2.2868\n",
      "Epoch [22/60], Batch [260/447], Loss: 2.8255\n",
      "Epoch [22/60], Batch [270/447], Loss: 2.1886\n",
      "Epoch [22/60], Batch [280/447], Loss: 2.8672\n",
      "Epoch [22/60], Batch [290/447], Loss: 2.2425\n",
      "Epoch [22/60], Batch [300/447], Loss: 2.3038\n",
      "Epoch [22/60], Batch [310/447], Loss: 2.6430\n",
      "Epoch [22/60], Batch [320/447], Loss: 2.7102\n",
      "Epoch [22/60], Batch [330/447], Loss: 2.3948\n",
      "Epoch [22/60], Batch [340/447], Loss: 2.3405\n",
      "Epoch [22/60], Batch [350/447], Loss: 2.3801\n",
      "Epoch [22/60], Batch [360/447], Loss: 2.2329\n",
      "Epoch [22/60], Batch [370/447], Loss: 2.8812\n",
      "Epoch [22/60], Batch [380/447], Loss: 2.5074\n",
      "Epoch [22/60], Batch [390/447], Loss: 1.8174\n",
      "Epoch [22/60], Batch [400/447], Loss: 2.3874\n",
      "Epoch [22/60], Batch [410/447], Loss: 2.6743\n",
      "Epoch [22/60], Batch [420/447], Loss: 2.3537\n",
      "Epoch [22/60], Batch [430/447], Loss: 2.3768\n",
      "Epoch [22/60], Batch [440/447], Loss: 2.1744\n",
      "Epoch [22/60], Average Train Loss: 2.5242\n",
      "Epoch [22/60], Validation Accuracy: 0.7392, Validation Loss: 1.0027\n",
      "No improvement in validation accuracy for 1 epoch(s).\n",
      "Epoch [23/60], Batch [0/447], Loss: 2.2910\n",
      "Epoch [23/60], Batch [10/447], Loss: 2.2844\n",
      "Epoch [23/60], Batch [20/447], Loss: 2.5033\n",
      "Epoch [23/60], Batch [30/447], Loss: 2.0032\n",
      "Epoch [23/60], Batch [40/447], Loss: 2.4353\n",
      "Epoch [23/60], Batch [50/447], Loss: 1.9050\n",
      "Epoch [23/60], Batch [60/447], Loss: 2.8231\n",
      "Epoch [23/60], Batch [70/447], Loss: 2.3855\n",
      "Epoch [23/60], Batch [80/447], Loss: 2.5669\n",
      "Epoch [23/60], Batch [90/447], Loss: 3.1932\n",
      "Epoch [23/60], Batch [100/447], Loss: 2.9386\n",
      "Epoch [23/60], Batch [110/447], Loss: 2.5285\n",
      "Epoch [23/60], Batch [120/447], Loss: 1.3256\n",
      "Epoch [23/60], Batch [130/447], Loss: 1.4314\n",
      "Epoch [23/60], Batch [140/447], Loss: 2.7567\n",
      "Epoch [23/60], Batch [150/447], Loss: 2.1247\n",
      "Epoch [23/60], Batch [160/447], Loss: 2.1991\n",
      "Epoch [23/60], Batch [170/447], Loss: 2.5456\n",
      "Epoch [23/60], Batch [180/447], Loss: 2.3200\n",
      "Epoch [23/60], Batch [190/447], Loss: 2.0491\n",
      "Epoch [23/60], Batch [200/447], Loss: 2.3402\n",
      "Epoch [23/60], Batch [210/447], Loss: 2.7534\n",
      "Epoch [23/60], Batch [220/447], Loss: 2.9721\n",
      "Epoch [23/60], Batch [230/447], Loss: 2.8962\n",
      "Epoch [23/60], Batch [240/447], Loss: 2.5819\n",
      "Epoch [23/60], Batch [250/447], Loss: 2.3983\n",
      "Epoch [23/60], Batch [260/447], Loss: 2.4561\n",
      "Epoch [23/60], Batch [270/447], Loss: 2.1633\n",
      "Epoch [23/60], Batch [280/447], Loss: 2.3550\n",
      "Epoch [23/60], Batch [290/447], Loss: 2.3681\n",
      "Epoch [23/60], Batch [300/447], Loss: 2.3154\n",
      "Epoch [23/60], Batch [310/447], Loss: 2.8532\n",
      "Epoch [23/60], Batch [320/447], Loss: 2.4525\n",
      "Epoch [23/60], Batch [330/447], Loss: 2.9797\n",
      "Epoch [23/60], Batch [340/447], Loss: 2.5310\n",
      "Epoch [23/60], Batch [350/447], Loss: 2.7421\n",
      "Epoch [23/60], Batch [360/447], Loss: 1.5771\n",
      "Epoch [23/60], Batch [370/447], Loss: 2.3616\n",
      "Epoch [23/60], Batch [380/447], Loss: 3.2550\n",
      "Epoch [23/60], Batch [390/447], Loss: 2.7580\n",
      "Epoch [23/60], Batch [400/447], Loss: 2.2368\n",
      "Epoch [23/60], Batch [410/447], Loss: 1.8131\n",
      "Epoch [23/60], Batch [420/447], Loss: 2.4945\n",
      "Epoch [23/60], Batch [430/447], Loss: 2.1167\n",
      "Epoch [23/60], Batch [440/447], Loss: 2.7302\n",
      "Epoch [23/60], Average Train Loss: 2.5042\n",
      "Epoch [23/60], Validation Accuracy: 0.7484, Validation Loss: 1.0061\n",
      "No improvement in validation accuracy for 2 epoch(s).\n",
      "Epoch [24/60], Batch [0/447], Loss: 2.7631\n",
      "Epoch [24/60], Batch [10/447], Loss: 2.8852\n",
      "Epoch [24/60], Batch [20/447], Loss: 2.4674\n",
      "Epoch [24/60], Batch [30/447], Loss: 2.2258\n",
      "Epoch [24/60], Batch [40/447], Loss: 2.4017\n",
      "Epoch [24/60], Batch [50/447], Loss: 2.9153\n",
      "Epoch [24/60], Batch [60/447], Loss: 2.6063\n",
      "Epoch [24/60], Batch [70/447], Loss: 2.0925\n",
      "Epoch [24/60], Batch [80/447], Loss: 2.0725\n",
      "Epoch [24/60], Batch [90/447], Loss: 3.0860\n",
      "Epoch [24/60], Batch [100/447], Loss: 2.2361\n",
      "Epoch [24/60], Batch [110/447], Loss: 1.6804\n",
      "Epoch [24/60], Batch [120/447], Loss: 3.0984\n",
      "Epoch [24/60], Batch [130/447], Loss: 2.4244\n",
      "Epoch [24/60], Batch [140/447], Loss: 2.6453\n",
      "Epoch [24/60], Batch [150/447], Loss: 2.8747\n",
      "Epoch [24/60], Batch [160/447], Loss: 2.4280\n",
      "Epoch [24/60], Batch [170/447], Loss: 2.4772\n",
      "Epoch [24/60], Batch [180/447], Loss: 2.5198\n",
      "Epoch [24/60], Batch [190/447], Loss: 1.9358\n",
      "Epoch [24/60], Batch [200/447], Loss: 3.3363\n",
      "Epoch [24/60], Batch [210/447], Loss: 2.2804\n",
      "Epoch [24/60], Batch [220/447], Loss: 2.4971\n",
      "Epoch [24/60], Batch [230/447], Loss: 2.9922\n",
      "Epoch [24/60], Batch [240/447], Loss: 2.5117\n",
      "Epoch [24/60], Batch [250/447], Loss: 1.9043\n",
      "Epoch [24/60], Batch [260/447], Loss: 2.6109\n",
      "Epoch [24/60], Batch [270/447], Loss: 2.3416\n",
      "Epoch [24/60], Batch [280/447], Loss: 1.9035\n",
      "Epoch [24/60], Batch [290/447], Loss: 3.2094\n",
      "Epoch [24/60], Batch [300/447], Loss: 1.9125\n",
      "Epoch [24/60], Batch [310/447], Loss: 2.1641\n",
      "Epoch [24/60], Batch [320/447], Loss: 2.6574\n",
      "Epoch [24/60], Batch [330/447], Loss: 2.5438\n",
      "Epoch [24/60], Batch [340/447], Loss: 2.3681\n",
      "Epoch [24/60], Batch [350/447], Loss: 2.6039\n",
      "Epoch [24/60], Batch [360/447], Loss: 2.3004\n",
      "Epoch [24/60], Batch [370/447], Loss: 2.1534\n",
      "Epoch [24/60], Batch [380/447], Loss: 1.8495\n",
      "Epoch [24/60], Batch [390/447], Loss: 2.2753\n",
      "Epoch [24/60], Batch [400/447], Loss: 1.6725\n",
      "Epoch [24/60], Batch [410/447], Loss: 2.5466\n",
      "Epoch [24/60], Batch [420/447], Loss: 2.1212\n",
      "Epoch [24/60], Batch [430/447], Loss: 2.5077\n",
      "Epoch [24/60], Batch [440/447], Loss: 1.9213\n",
      "Epoch [24/60], Average Train Loss: 2.4652\n",
      "Epoch [24/60], Validation Accuracy: 0.7458, Validation Loss: 0.9875\n",
      "No improvement in validation accuracy for 3 epoch(s).\n",
      "Epoch [25/60], Batch [0/447], Loss: 2.7604\n",
      "Epoch [25/60], Batch [10/447], Loss: 2.6411\n",
      "Epoch [25/60], Batch [20/447], Loss: 1.7850\n",
      "Epoch [25/60], Batch [30/447], Loss: 2.8095\n",
      "Epoch [25/60], Batch [40/447], Loss: 2.1695\n",
      "Epoch [25/60], Batch [50/447], Loss: 2.6675\n",
      "Epoch [25/60], Batch [60/447], Loss: 2.8033\n",
      "Epoch [25/60], Batch [70/447], Loss: 2.7191\n",
      "Epoch [25/60], Batch [80/447], Loss: 2.2063\n",
      "Epoch [25/60], Batch [90/447], Loss: 3.2162\n",
      "Epoch [25/60], Batch [100/447], Loss: 1.7567\n",
      "Epoch [25/60], Batch [110/447], Loss: 2.4524\n",
      "Epoch [25/60], Batch [120/447], Loss: 2.5060\n",
      "Epoch [25/60], Batch [130/447], Loss: 2.5970\n",
      "Epoch [25/60], Batch [140/447], Loss: 1.7608\n",
      "Epoch [25/60], Batch [150/447], Loss: 2.4291\n",
      "Epoch [25/60], Batch [160/447], Loss: 2.8825\n",
      "Epoch [25/60], Batch [170/447], Loss: 2.2261\n",
      "Epoch [25/60], Batch [180/447], Loss: 2.0741\n",
      "Epoch [25/60], Batch [190/447], Loss: 3.0158\n",
      "Epoch [25/60], Batch [200/447], Loss: 2.7771\n",
      "Epoch [25/60], Batch [210/447], Loss: 2.4272\n",
      "Epoch [25/60], Batch [220/447], Loss: 2.0153\n",
      "Epoch [25/60], Batch [230/447], Loss: 2.7304\n",
      "Epoch [25/60], Batch [240/447], Loss: 2.2621\n",
      "Epoch [25/60], Batch [250/447], Loss: 1.9191\n",
      "Epoch [25/60], Batch [260/447], Loss: 2.2154\n",
      "Epoch [25/60], Batch [270/447], Loss: 2.7737\n",
      "Epoch [25/60], Batch [280/447], Loss: 2.5970\n",
      "Epoch [25/60], Batch [290/447], Loss: 2.5499\n",
      "Epoch [25/60], Batch [300/447], Loss: 2.6047\n",
      "Epoch [25/60], Batch [310/447], Loss: 1.4648\n",
      "Epoch [25/60], Batch [320/447], Loss: 2.7792\n",
      "Epoch [25/60], Batch [330/447], Loss: 2.8692\n",
      "Epoch [25/60], Batch [340/447], Loss: 2.7437\n",
      "Epoch [25/60], Batch [350/447], Loss: 1.9858\n",
      "Epoch [25/60], Batch [360/447], Loss: 2.3965\n",
      "Epoch [25/60], Batch [370/447], Loss: 2.1224\n",
      "Epoch [25/60], Batch [380/447], Loss: 2.4913\n",
      "Epoch [25/60], Batch [390/447], Loss: 2.5761\n",
      "Epoch [25/60], Batch [400/447], Loss: 2.0106\n",
      "Epoch [25/60], Batch [410/447], Loss: 2.6211\n",
      "Epoch [25/60], Batch [420/447], Loss: 2.6431\n",
      "Epoch [25/60], Batch [430/447], Loss: 2.6521\n",
      "Epoch [25/60], Batch [440/447], Loss: 3.0896\n",
      "Epoch [25/60], Average Train Loss: 2.4628\n",
      "Epoch [25/60], Validation Accuracy: 0.7484, Validation Loss: 0.9582\n",
      "No improvement in validation accuracy for 4 epoch(s).\n",
      "Epoch [26/60], Batch [0/447], Loss: 2.0344\n",
      "Epoch [26/60], Batch [10/447], Loss: 2.5155\n",
      "Epoch [26/60], Batch [20/447], Loss: 2.3092\n",
      "Epoch [26/60], Batch [30/447], Loss: 2.8134\n",
      "Epoch [26/60], Batch [40/447], Loss: 2.3692\n",
      "Epoch [26/60], Batch [50/447], Loss: 1.8090\n",
      "Epoch [26/60], Batch [60/447], Loss: 2.6500\n",
      "Epoch [26/60], Batch [70/447], Loss: 2.4338\n",
      "Epoch [26/60], Batch [80/447], Loss: 2.9239\n",
      "Epoch [26/60], Batch [90/447], Loss: 2.7921\n",
      "Epoch [26/60], Batch [100/447], Loss: 2.7126\n",
      "Epoch [26/60], Batch [110/447], Loss: 2.0886\n",
      "Epoch [26/60], Batch [120/447], Loss: 2.3123\n",
      "Epoch [26/60], Batch [130/447], Loss: 2.3017\n",
      "Epoch [26/60], Batch [140/447], Loss: 2.6593\n",
      "Epoch [26/60], Batch [150/447], Loss: 2.4261\n",
      "Epoch [26/60], Batch [160/447], Loss: 2.0339\n",
      "Epoch [26/60], Batch [170/447], Loss: 2.4198\n",
      "Epoch [26/60], Batch [180/447], Loss: 2.8041\n",
      "Epoch [26/60], Batch [190/447], Loss: 3.0424\n",
      "Epoch [26/60], Batch [200/447], Loss: 2.6349\n",
      "Epoch [26/60], Batch [210/447], Loss: 2.4946\n",
      "Epoch [26/60], Batch [220/447], Loss: 2.3243\n",
      "Epoch [26/60], Batch [230/447], Loss: 2.8050\n",
      "Epoch [26/60], Batch [240/447], Loss: 1.8778\n",
      "Epoch [26/60], Batch [250/447], Loss: 2.3469\n",
      "Epoch [26/60], Batch [260/447], Loss: 2.3025\n",
      "Epoch [26/60], Batch [270/447], Loss: 3.0224\n",
      "Epoch [26/60], Batch [280/447], Loss: 2.8848\n",
      "Epoch [26/60], Batch [290/447], Loss: 1.8328\n",
      "Epoch [26/60], Batch [300/447], Loss: 2.5297\n",
      "Epoch [26/60], Batch [310/447], Loss: 2.4402\n",
      "Epoch [26/60], Batch [320/447], Loss: 2.2639\n",
      "Epoch [26/60], Batch [330/447], Loss: 2.7734\n",
      "Epoch [26/60], Batch [340/447], Loss: 2.3022\n",
      "Epoch [26/60], Batch [350/447], Loss: 2.2291\n",
      "Epoch [26/60], Batch [360/447], Loss: 2.3114\n",
      "Epoch [26/60], Batch [370/447], Loss: 2.3463\n",
      "Epoch [26/60], Batch [380/447], Loss: 2.6644\n",
      "Epoch [26/60], Batch [390/447], Loss: 3.2435\n",
      "Epoch [26/60], Batch [400/447], Loss: 2.3417\n",
      "Epoch [26/60], Batch [410/447], Loss: 2.5314\n",
      "Epoch [26/60], Batch [420/447], Loss: 2.4394\n",
      "Epoch [26/60], Batch [430/447], Loss: 2.6519\n",
      "Epoch [26/60], Batch [440/447], Loss: 2.7146\n",
      "Epoch [26/60], Average Train Loss: 2.4147\n",
      "Epoch [26/60], Validation Accuracy: 0.7497, Validation Loss: 0.9576\n",
      "New best validation accuracy: 0.7497\n",
      "Epoch [27/60], Batch [0/447], Loss: 2.8459\n",
      "Epoch [27/60], Batch [10/447], Loss: 2.6286\n",
      "Epoch [27/60], Batch [20/447], Loss: 2.9233\n",
      "Epoch [27/60], Batch [30/447], Loss: 2.3517\n",
      "Epoch [27/60], Batch [40/447], Loss: 2.5887\n",
      "Epoch [27/60], Batch [50/447], Loss: 2.6760\n",
      "Epoch [27/60], Batch [60/447], Loss: 2.5653\n",
      "Epoch [27/60], Batch [70/447], Loss: 2.0727\n",
      "Epoch [27/60], Batch [80/447], Loss: 2.3600\n",
      "Epoch [27/60], Batch [90/447], Loss: 2.7936\n",
      "Epoch [27/60], Batch [100/447], Loss: 2.9224\n",
      "Epoch [27/60], Batch [110/447], Loss: 2.3054\n",
      "Epoch [27/60], Batch [120/447], Loss: 2.2568\n",
      "Epoch [27/60], Batch [130/447], Loss: 2.1328\n",
      "Epoch [27/60], Batch [140/447], Loss: 2.4751\n",
      "Epoch [27/60], Batch [150/447], Loss: 2.1121\n",
      "Epoch [27/60], Batch [160/447], Loss: 2.9632\n",
      "Epoch [27/60], Batch [170/447], Loss: 3.0853\n",
      "Epoch [27/60], Batch [180/447], Loss: 2.5504\n",
      "Epoch [27/60], Batch [190/447], Loss: 2.8886\n",
      "Epoch [27/60], Batch [200/447], Loss: 2.8333\n",
      "Epoch [27/60], Batch [210/447], Loss: 2.5224\n",
      "Epoch [27/60], Batch [220/447], Loss: 2.9077\n",
      "Epoch [27/60], Batch [230/447], Loss: 2.5854\n",
      "Epoch [27/60], Batch [240/447], Loss: 2.4001\n",
      "Epoch [27/60], Batch [250/447], Loss: 2.3349\n",
      "Epoch [27/60], Batch [260/447], Loss: 2.5430\n",
      "Epoch [27/60], Batch [270/447], Loss: 2.9786\n",
      "Epoch [27/60], Batch [280/447], Loss: 2.9525\n",
      "Epoch [27/60], Batch [290/447], Loss: 2.6223\n",
      "Epoch [27/60], Batch [300/447], Loss: 2.7891\n",
      "Epoch [27/60], Batch [310/447], Loss: 1.5835\n",
      "Epoch [27/60], Batch [320/447], Loss: 2.1245\n",
      "Epoch [27/60], Batch [330/447], Loss: 2.4369\n",
      "Epoch [27/60], Batch [340/447], Loss: 2.6612\n",
      "Epoch [27/60], Batch [350/447], Loss: 2.7182\n",
      "Epoch [27/60], Batch [360/447], Loss: 2.0584\n",
      "Epoch [27/60], Batch [370/447], Loss: 2.4468\n",
      "Epoch [27/60], Batch [380/447], Loss: 3.0402\n",
      "Epoch [27/60], Batch [390/447], Loss: 2.7385\n",
      "Epoch [27/60], Batch [400/447], Loss: 3.3559\n",
      "Epoch [27/60], Batch [410/447], Loss: 1.7631\n",
      "Epoch [27/60], Batch [420/447], Loss: 2.2886\n",
      "Epoch [27/60], Batch [430/447], Loss: 2.1203\n",
      "Epoch [27/60], Batch [440/447], Loss: 2.0659\n",
      "Epoch [27/60], Average Train Loss: 2.4174\n",
      "Epoch [27/60], Validation Accuracy: 0.7549, Validation Loss: 0.9544\n",
      "New best validation accuracy: 0.7549\n",
      "Epoch [28/60], Batch [0/447], Loss: 2.4284\n",
      "Epoch [28/60], Batch [10/447], Loss: 2.4997\n",
      "Epoch [28/60], Batch [20/447], Loss: 2.6006\n",
      "Epoch [28/60], Batch [30/447], Loss: 2.6182\n",
      "Epoch [28/60], Batch [40/447], Loss: 2.9125\n",
      "Epoch [28/60], Batch [50/447], Loss: 2.5963\n",
      "Epoch [28/60], Batch [60/447], Loss: 2.4495\n",
      "Epoch [28/60], Batch [70/447], Loss: 1.7942\n",
      "Epoch [28/60], Batch [80/447], Loss: 2.2809\n",
      "Epoch [28/60], Batch [90/447], Loss: 2.3951\n",
      "Epoch [28/60], Batch [100/447], Loss: 2.2760\n",
      "Epoch [28/60], Batch [110/447], Loss: 2.9126\n",
      "Epoch [28/60], Batch [120/447], Loss: 2.3500\n",
      "Epoch [28/60], Batch [130/447], Loss: 2.4593\n",
      "Epoch [28/60], Batch [140/447], Loss: 2.0792\n",
      "Epoch [28/60], Batch [150/447], Loss: 2.7435\n",
      "Epoch [28/60], Batch [160/447], Loss: 2.1064\n",
      "Epoch [28/60], Batch [170/447], Loss: 2.4351\n",
      "Epoch [28/60], Batch [180/447], Loss: 2.7692\n",
      "Epoch [28/60], Batch [190/447], Loss: 2.1587\n",
      "Epoch [28/60], Batch [200/447], Loss: 2.3382\n",
      "Epoch [28/60], Batch [210/447], Loss: 2.4927\n",
      "Epoch [28/60], Batch [220/447], Loss: 2.1733\n",
      "Epoch [28/60], Batch [230/447], Loss: 2.5621\n",
      "Epoch [28/60], Batch [240/447], Loss: 1.9169\n",
      "Epoch [28/60], Batch [250/447], Loss: 2.0832\n",
      "Epoch [28/60], Batch [260/447], Loss: 2.0770\n",
      "Epoch [28/60], Batch [270/447], Loss: 2.9930\n",
      "Epoch [28/60], Batch [280/447], Loss: 2.7530\n",
      "Epoch [28/60], Batch [290/447], Loss: 2.6458\n",
      "Epoch [28/60], Batch [300/447], Loss: 2.7564\n",
      "Epoch [28/60], Batch [310/447], Loss: 1.9002\n",
      "Epoch [28/60], Batch [320/447], Loss: 1.9437\n",
      "Epoch [28/60], Batch [330/447], Loss: 2.0689\n",
      "Epoch [28/60], Batch [340/447], Loss: 2.4726\n",
      "Epoch [28/60], Batch [350/447], Loss: 1.6051\n",
      "Epoch [28/60], Batch [360/447], Loss: 2.0609\n",
      "Epoch [28/60], Batch [370/447], Loss: 2.6274\n",
      "Epoch [28/60], Batch [380/447], Loss: 1.6410\n",
      "Epoch [28/60], Batch [390/447], Loss: 2.6114\n",
      "Epoch [28/60], Batch [400/447], Loss: 2.1995\n",
      "Epoch [28/60], Batch [410/447], Loss: 3.0437\n",
      "Epoch [28/60], Batch [420/447], Loss: 2.7394\n",
      "Epoch [28/60], Batch [430/447], Loss: 2.4072\n",
      "Epoch [28/60], Batch [440/447], Loss: 2.3697\n",
      "Epoch [28/60], Average Train Loss: 2.3829\n",
      "Epoch [28/60], Validation Accuracy: 0.7523, Validation Loss: 0.9328\n",
      "No improvement in validation accuracy for 1 epoch(s).\n",
      "Epoch [29/60], Batch [0/447], Loss: 2.6037\n",
      "Epoch [29/60], Batch [10/447], Loss: 3.0535\n",
      "Epoch [29/60], Batch [20/447], Loss: 2.1072\n",
      "Epoch [29/60], Batch [30/447], Loss: 3.1481\n",
      "Epoch [29/60], Batch [40/447], Loss: 2.9314\n",
      "Epoch [29/60], Batch [50/447], Loss: 2.0624\n",
      "Epoch [29/60], Batch [60/447], Loss: 2.7581\n",
      "Epoch [29/60], Batch [70/447], Loss: 2.7581\n",
      "Epoch [29/60], Batch [80/447], Loss: 2.8309\n",
      "Epoch [29/60], Batch [90/447], Loss: 1.9350\n",
      "Epoch [29/60], Batch [100/447], Loss: 2.1129\n",
      "Epoch [29/60], Batch [110/447], Loss: 2.6219\n",
      "Epoch [29/60], Batch [120/447], Loss: 1.6495\n",
      "Epoch [29/60], Batch [130/447], Loss: 2.2207\n",
      "Epoch [29/60], Batch [140/447], Loss: 2.4649\n",
      "Epoch [29/60], Batch [150/447], Loss: 3.4621\n",
      "Epoch [29/60], Batch [160/447], Loss: 2.2958\n",
      "Epoch [29/60], Batch [170/447], Loss: 2.6788\n",
      "Epoch [29/60], Batch [180/447], Loss: 2.8013\n",
      "Epoch [29/60], Batch [190/447], Loss: 2.5377\n",
      "Epoch [29/60], Batch [200/447], Loss: 2.0727\n",
      "Epoch [29/60], Batch [210/447], Loss: 2.7457\n",
      "Epoch [29/60], Batch [220/447], Loss: 2.5329\n",
      "Epoch [29/60], Batch [230/447], Loss: 1.8427\n",
      "Epoch [29/60], Batch [240/447], Loss: 2.8898\n",
      "Epoch [29/60], Batch [250/447], Loss: 2.0876\n",
      "Epoch [29/60], Batch [260/447], Loss: 2.2595\n",
      "Epoch [29/60], Batch [270/447], Loss: 2.0710\n",
      "Epoch [29/60], Batch [280/447], Loss: 2.5225\n",
      "Epoch [29/60], Batch [290/447], Loss: 1.5534\n",
      "Epoch [29/60], Batch [300/447], Loss: 2.3932\n",
      "Epoch [29/60], Batch [310/447], Loss: 2.8117\n",
      "Epoch [29/60], Batch [320/447], Loss: 2.1899\n",
      "Epoch [29/60], Batch [330/447], Loss: 2.6022\n",
      "Epoch [29/60], Batch [340/447], Loss: 2.5652\n",
      "Epoch [29/60], Batch [350/447], Loss: 2.9209\n",
      "Epoch [29/60], Batch [360/447], Loss: 1.6715\n",
      "Epoch [29/60], Batch [370/447], Loss: 2.1174\n",
      "Epoch [29/60], Batch [380/447], Loss: 2.0978\n",
      "Epoch [29/60], Batch [390/447], Loss: 2.4871\n",
      "Epoch [29/60], Batch [400/447], Loss: 2.4344\n",
      "Epoch [29/60], Batch [410/447], Loss: 2.2939\n",
      "Epoch [29/60], Batch [420/447], Loss: 2.3342\n",
      "Epoch [29/60], Batch [430/447], Loss: 2.5771\n",
      "Epoch [29/60], Batch [440/447], Loss: 2.6349\n",
      "Epoch [29/60], Average Train Loss: 2.3525\n",
      "Epoch [29/60], Validation Accuracy: 0.7621, Validation Loss: 0.9095\n",
      "New best validation accuracy: 0.7621\n",
      "Epoch [30/60], Batch [0/447], Loss: 1.6968\n",
      "Epoch [30/60], Batch [10/447], Loss: 2.5015\n",
      "Epoch [30/60], Batch [20/447], Loss: 2.2108\n",
      "Epoch [30/60], Batch [30/447], Loss: 2.2563\n",
      "Epoch [30/60], Batch [40/447], Loss: 2.5712\n",
      "Epoch [30/60], Batch [50/447], Loss: 2.7686\n",
      "Epoch [30/60], Batch [60/447], Loss: 2.8331\n",
      "Epoch [30/60], Batch [70/447], Loss: 2.9409\n",
      "Epoch [30/60], Batch [80/447], Loss: 2.1365\n",
      "Epoch [30/60], Batch [90/447], Loss: 1.8789\n",
      "Epoch [30/60], Batch [100/447], Loss: 2.7107\n",
      "Epoch [30/60], Batch [110/447], Loss: 2.0598\n",
      "Epoch [30/60], Batch [120/447], Loss: 2.7034\n",
      "Epoch [30/60], Batch [130/447], Loss: 2.3197\n",
      "Epoch [30/60], Batch [140/447], Loss: 2.6255\n",
      "Epoch [30/60], Batch [150/447], Loss: 2.0794\n",
      "Epoch [30/60], Batch [160/447], Loss: 2.5136\n",
      "Epoch [30/60], Batch [170/447], Loss: 2.3101\n",
      "Epoch [30/60], Batch [180/447], Loss: 2.3899\n",
      "Epoch [30/60], Batch [190/447], Loss: 2.3755\n",
      "Epoch [30/60], Batch [200/447], Loss: 1.6896\n",
      "Epoch [30/60], Batch [210/447], Loss: 1.5918\n",
      "Epoch [30/60], Batch [220/447], Loss: 2.5544\n",
      "Epoch [30/60], Batch [230/447], Loss: 2.1707\n",
      "Epoch [30/60], Batch [240/447], Loss: 2.5419\n",
      "Epoch [30/60], Batch [250/447], Loss: 2.0495\n",
      "Epoch [30/60], Batch [260/447], Loss: 2.5997\n",
      "Epoch [30/60], Batch [270/447], Loss: 2.6097\n",
      "Epoch [30/60], Batch [280/447], Loss: 2.6459\n",
      "Epoch [30/60], Batch [290/447], Loss: 2.5219\n",
      "Epoch [30/60], Batch [300/447], Loss: 2.6072\n",
      "Epoch [30/60], Batch [310/447], Loss: 3.0038\n",
      "Epoch [30/60], Batch [320/447], Loss: 2.9455\n",
      "Epoch [30/60], Batch [330/447], Loss: 2.4764\n",
      "Epoch [30/60], Batch [340/447], Loss: 1.3725\n",
      "Epoch [30/60], Batch [350/447], Loss: 2.8777\n",
      "Epoch [30/60], Batch [360/447], Loss: 2.3337\n",
      "Epoch [30/60], Batch [370/447], Loss: 2.2717\n",
      "Epoch [30/60], Batch [380/447], Loss: 2.3444\n",
      "Epoch [30/60], Batch [390/447], Loss: 1.9318\n",
      "Epoch [30/60], Batch [400/447], Loss: 1.7039\n",
      "Epoch [30/60], Batch [410/447], Loss: 2.1879\n",
      "Epoch [30/60], Batch [420/447], Loss: 2.1623\n",
      "Epoch [30/60], Batch [430/447], Loss: 2.9302\n",
      "Epoch [30/60], Batch [440/447], Loss: 2.3865\n",
      "Epoch [30/60], Average Train Loss: 2.3247\n",
      "Epoch [30/60], Validation Accuracy: 0.7595, Validation Loss: 0.9295\n",
      "No improvement in validation accuracy for 1 epoch(s).\n",
      "Epoch [31/60], Batch [0/447], Loss: 2.1914\n",
      "Epoch [31/60], Batch [10/447], Loss: 2.4440\n",
      "Epoch [31/60], Batch [20/447], Loss: 2.2543\n",
      "Epoch [31/60], Batch [30/447], Loss: 1.9616\n",
      "Epoch [31/60], Batch [40/447], Loss: 2.6230\n",
      "Epoch [31/60], Batch [50/447], Loss: 2.4767\n",
      "Epoch [31/60], Batch [60/447], Loss: 2.6811\n",
      "Epoch [31/60], Batch [70/447], Loss: 1.8599\n",
      "Epoch [31/60], Batch [80/447], Loss: 2.6753\n",
      "Epoch [31/60], Batch [90/447], Loss: 2.1218\n",
      "Epoch [31/60], Batch [100/447], Loss: 2.4979\n",
      "Epoch [31/60], Batch [110/447], Loss: 2.5611\n",
      "Epoch [31/60], Batch [120/447], Loss: 2.6122\n",
      "Epoch [31/60], Batch [130/447], Loss: 2.9430\n",
      "Epoch [31/60], Batch [140/447], Loss: 2.4213\n",
      "Epoch [31/60], Batch [150/447], Loss: 2.3014\n",
      "Epoch [31/60], Batch [160/447], Loss: 2.3782\n",
      "Epoch [31/60], Batch [170/447], Loss: 2.0118\n",
      "Epoch [31/60], Batch [180/447], Loss: 2.4338\n",
      "Epoch [31/60], Batch [190/447], Loss: 2.1482\n",
      "Epoch [31/60], Batch [200/447], Loss: 2.0152\n",
      "Epoch [31/60], Batch [210/447], Loss: 2.1008\n",
      "Epoch [31/60], Batch [220/447], Loss: 1.6725\n",
      "Epoch [31/60], Batch [230/447], Loss: 2.5130\n",
      "Epoch [31/60], Batch [240/447], Loss: 2.2919\n",
      "Epoch [31/60], Batch [250/447], Loss: 2.1091\n",
      "Epoch [31/60], Batch [260/447], Loss: 2.6168\n",
      "Epoch [31/60], Batch [270/447], Loss: 2.7006\n",
      "Epoch [31/60], Batch [280/447], Loss: 2.4748\n",
      "Epoch [31/60], Batch [290/447], Loss: 1.2314\n",
      "Epoch [31/60], Batch [300/447], Loss: 1.8794\n",
      "Epoch [31/60], Batch [310/447], Loss: 1.5126\n",
      "Epoch [31/60], Batch [320/447], Loss: 2.0316\n",
      "Epoch [31/60], Batch [330/447], Loss: 2.4319\n",
      "Epoch [31/60], Batch [340/447], Loss: 2.4311\n",
      "Epoch [31/60], Batch [350/447], Loss: 2.0838\n",
      "Epoch [31/60], Batch [360/447], Loss: 2.4646\n",
      "Epoch [31/60], Batch [370/447], Loss: 2.6991\n",
      "Epoch [31/60], Batch [380/447], Loss: 1.3732\n",
      "Epoch [31/60], Batch [390/447], Loss: 2.3134\n",
      "Epoch [31/60], Batch [400/447], Loss: 2.7503\n",
      "Epoch [31/60], Batch [410/447], Loss: 2.8737\n",
      "Epoch [31/60], Batch [420/447], Loss: 1.6850\n",
      "Epoch [31/60], Batch [430/447], Loss: 2.1590\n",
      "Epoch [31/60], Batch [440/447], Loss: 1.3597\n",
      "Epoch [31/60], Average Train Loss: 2.3318\n",
      "Epoch [31/60], Validation Accuracy: 0.7601, Validation Loss: 0.9357\n",
      "No improvement in validation accuracy for 2 epoch(s).\n",
      "Epoch [32/60], Batch [0/447], Loss: 2.0805\n",
      "Epoch [32/60], Batch [10/447], Loss: 1.9866\n",
      "Epoch [32/60], Batch [20/447], Loss: 2.3680\n",
      "Epoch [32/60], Batch [30/447], Loss: 2.3359\n",
      "Epoch [32/60], Batch [40/447], Loss: 1.5712\n",
      "Epoch [32/60], Batch [50/447], Loss: 2.4339\n",
      "Epoch [32/60], Batch [60/447], Loss: 2.0303\n",
      "Epoch [32/60], Batch [70/447], Loss: 1.4422\n",
      "Epoch [32/60], Batch [80/447], Loss: 2.6024\n",
      "Epoch [32/60], Batch [90/447], Loss: 2.4250\n",
      "Epoch [32/60], Batch [100/447], Loss: 2.2038\n",
      "Epoch [32/60], Batch [110/447], Loss: 2.8019\n",
      "Epoch [32/60], Batch [120/447], Loss: 1.9169\n",
      "Epoch [32/60], Batch [130/447], Loss: 2.6178\n",
      "Epoch [32/60], Batch [140/447], Loss: 2.7455\n",
      "Epoch [32/60], Batch [150/447], Loss: 1.7242\n",
      "Epoch [32/60], Batch [160/447], Loss: 2.0015\n",
      "Epoch [32/60], Batch [170/447], Loss: 2.0893\n",
      "Epoch [32/60], Batch [180/447], Loss: 1.5475\n",
      "Epoch [32/60], Batch [190/447], Loss: 2.3111\n",
      "Epoch [32/60], Batch [200/447], Loss: 2.6243\n",
      "Epoch [32/60], Batch [210/447], Loss: 2.4977\n",
      "Epoch [32/60], Batch [220/447], Loss: 2.0577\n",
      "Epoch [32/60], Batch [230/447], Loss: 2.0980\n",
      "Epoch [32/60], Batch [240/447], Loss: 1.6752\n",
      "Epoch [32/60], Batch [250/447], Loss: 1.8437\n",
      "Epoch [32/60], Batch [260/447], Loss: 2.0809\n",
      "Epoch [32/60], Batch [270/447], Loss: 1.8857\n",
      "Epoch [32/60], Batch [280/447], Loss: 1.9599\n",
      "Epoch [32/60], Batch [290/447], Loss: 2.5358\n",
      "Epoch [32/60], Batch [300/447], Loss: 2.0718\n",
      "Epoch [32/60], Batch [310/447], Loss: 1.5485\n",
      "Epoch [32/60], Batch [320/447], Loss: 2.4554\n",
      "Epoch [32/60], Batch [330/447], Loss: 1.9768\n",
      "Epoch [32/60], Batch [340/447], Loss: 1.8575\n",
      "Epoch [32/60], Batch [350/447], Loss: 1.8806\n",
      "Epoch [32/60], Batch [360/447], Loss: 1.9961\n",
      "Epoch [32/60], Batch [370/447], Loss: 2.2724\n",
      "Epoch [32/60], Batch [380/447], Loss: 2.0660\n",
      "Epoch [32/60], Batch [390/447], Loss: 2.5662\n",
      "Epoch [32/60], Batch [400/447], Loss: 1.9779\n",
      "Epoch [32/60], Batch [410/447], Loss: 2.7069\n",
      "Epoch [32/60], Batch [420/447], Loss: 2.6835\n",
      "Epoch [32/60], Batch [430/447], Loss: 1.7472\n",
      "Epoch [32/60], Batch [440/447], Loss: 2.6626\n",
      "Epoch [32/60], Average Train Loss: 2.2848\n",
      "Epoch [32/60], Validation Accuracy: 0.7608, Validation Loss: 0.9271\n",
      "No improvement in validation accuracy for 3 epoch(s).\n",
      "Epoch [33/60], Batch [0/447], Loss: 2.9838\n",
      "Epoch [33/60], Batch [10/447], Loss: 2.5520\n",
      "Epoch [33/60], Batch [20/447], Loss: 2.4393\n",
      "Epoch [33/60], Batch [30/447], Loss: 2.9441\n",
      "Epoch [33/60], Batch [40/447], Loss: 1.6442\n",
      "Epoch [33/60], Batch [50/447], Loss: 2.5304\n",
      "Epoch [33/60], Batch [60/447], Loss: 1.9618\n",
      "Epoch [33/60], Batch [70/447], Loss: 2.3850\n",
      "Epoch [33/60], Batch [80/447], Loss: 2.1926\n",
      "Epoch [33/60], Batch [90/447], Loss: 2.3442\n",
      "Epoch [33/60], Batch [100/447], Loss: 2.6947\n",
      "Epoch [33/60], Batch [110/447], Loss: 2.5465\n",
      "Epoch [33/60], Batch [120/447], Loss: 2.1632\n",
      "Epoch [33/60], Batch [130/447], Loss: 2.4516\n",
      "Epoch [33/60], Batch [140/447], Loss: 1.8613\n",
      "Epoch [33/60], Batch [150/447], Loss: 2.7589\n",
      "Epoch [33/60], Batch [160/447], Loss: 2.0224\n",
      "Epoch [33/60], Batch [170/447], Loss: 2.5701\n",
      "Epoch [33/60], Batch [180/447], Loss: 2.3946\n",
      "Epoch [33/60], Batch [190/447], Loss: 2.1525\n",
      "Epoch [33/60], Batch [200/447], Loss: 2.4241\n",
      "Epoch [33/60], Batch [210/447], Loss: 1.8455\n",
      "Epoch [33/60], Batch [220/447], Loss: 2.3247\n",
      "Epoch [33/60], Batch [230/447], Loss: 2.0960\n",
      "Epoch [33/60], Batch [240/447], Loss: 2.1215\n",
      "Epoch [33/60], Batch [250/447], Loss: 2.4452\n",
      "Epoch [33/60], Batch [260/447], Loss: 1.9747\n",
      "Epoch [33/60], Batch [270/447], Loss: 3.0632\n",
      "Epoch [33/60], Batch [280/447], Loss: 2.7791\n",
      "Epoch [33/60], Batch [290/447], Loss: 1.9151\n",
      "Epoch [33/60], Batch [300/447], Loss: 2.6214\n",
      "Epoch [33/60], Batch [310/447], Loss: 3.0141\n",
      "Epoch [33/60], Batch [320/447], Loss: 2.1707\n",
      "Epoch [33/60], Batch [330/447], Loss: 2.4824\n",
      "Epoch [33/60], Batch [340/447], Loss: 2.3836\n",
      "Epoch [33/60], Batch [350/447], Loss: 1.9775\n",
      "Epoch [33/60], Batch [360/447], Loss: 3.3876\n",
      "Epoch [33/60], Batch [370/447], Loss: 2.0816\n",
      "Epoch [33/60], Batch [380/447], Loss: 2.8379\n",
      "Epoch [33/60], Batch [390/447], Loss: 2.4030\n",
      "Epoch [33/60], Batch [400/447], Loss: 2.7018\n",
      "Epoch [33/60], Batch [410/447], Loss: 2.2288\n",
      "Epoch [33/60], Batch [420/447], Loss: 3.1103\n",
      "Epoch [33/60], Batch [430/447], Loss: 2.0684\n",
      "Epoch [33/60], Batch [440/447], Loss: 2.8975\n",
      "Epoch [33/60], Average Train Loss: 2.3019\n",
      "Epoch [33/60], Validation Accuracy: 0.7673, Validation Loss: 0.9012\n",
      "New best validation accuracy: 0.7673\n",
      "Epoch [34/60], Batch [0/447], Loss: 2.2044\n",
      "Epoch [34/60], Batch [10/447], Loss: 2.0382\n",
      "Epoch [34/60], Batch [20/447], Loss: 1.9875\n",
      "Epoch [34/60], Batch [30/447], Loss: 2.3580\n",
      "Epoch [34/60], Batch [40/447], Loss: 1.3836\n",
      "Epoch [34/60], Batch [50/447], Loss: 1.7836\n",
      "Epoch [34/60], Batch [60/447], Loss: 1.3501\n",
      "Epoch [34/60], Batch [70/447], Loss: 2.7723\n",
      "Epoch [34/60], Batch [80/447], Loss: 2.5899\n",
      "Epoch [34/60], Batch [90/447], Loss: 1.8679\n",
      "Epoch [34/60], Batch [100/447], Loss: 2.2475\n",
      "Epoch [34/60], Batch [110/447], Loss: 1.5889\n",
      "Epoch [34/60], Batch [120/447], Loss: 1.5727\n",
      "Epoch [34/60], Batch [130/447], Loss: 1.4811\n",
      "Epoch [34/60], Batch [140/447], Loss: 2.0572\n",
      "Epoch [34/60], Batch [150/447], Loss: 2.4965\n",
      "Epoch [34/60], Batch [160/447], Loss: 1.7094\n",
      "Epoch [34/60], Batch [170/447], Loss: 1.9340\n",
      "Epoch [34/60], Batch [180/447], Loss: 2.0447\n",
      "Epoch [34/60], Batch [190/447], Loss: 2.5955\n",
      "Epoch [34/60], Batch [200/447], Loss: 2.4562\n",
      "Epoch [34/60], Batch [210/447], Loss: 2.6369\n",
      "Epoch [34/60], Batch [220/447], Loss: 2.8828\n",
      "Epoch [34/60], Batch [230/447], Loss: 2.5179\n",
      "Epoch [34/60], Batch [240/447], Loss: 2.4329\n",
      "Epoch [34/60], Batch [250/447], Loss: 2.6600\n",
      "Epoch [34/60], Batch [260/447], Loss: 2.1856\n",
      "Epoch [34/60], Batch [270/447], Loss: 2.6846\n",
      "Epoch [34/60], Batch [280/447], Loss: 2.4554\n",
      "Epoch [34/60], Batch [290/447], Loss: 2.4498\n",
      "Epoch [34/60], Batch [300/447], Loss: 1.9853\n",
      "Epoch [34/60], Batch [310/447], Loss: 1.8168\n",
      "Epoch [34/60], Batch [320/447], Loss: 1.7116\n",
      "Epoch [34/60], Batch [330/447], Loss: 2.2114\n",
      "Epoch [34/60], Batch [340/447], Loss: 2.0155\n",
      "Epoch [34/60], Batch [350/447], Loss: 2.4778\n",
      "Epoch [34/60], Batch [360/447], Loss: 2.2279\n",
      "Epoch [34/60], Batch [370/447], Loss: 2.1014\n",
      "Epoch [34/60], Batch [380/447], Loss: 2.9647\n",
      "Epoch [34/60], Batch [390/447], Loss: 2.6317\n",
      "Epoch [34/60], Batch [400/447], Loss: 2.7493\n",
      "Epoch [34/60], Batch [410/447], Loss: 1.6958\n",
      "Epoch [34/60], Batch [420/447], Loss: 2.5153\n",
      "Epoch [34/60], Batch [430/447], Loss: 2.4497\n",
      "Epoch [34/60], Batch [440/447], Loss: 2.5633\n",
      "Epoch [34/60], Average Train Loss: 2.2917\n",
      "Epoch [34/60], Validation Accuracy: 0.7582, Validation Loss: 0.9101\n",
      "No improvement in validation accuracy for 1 epoch(s).\n",
      "Epoch [35/60], Batch [0/447], Loss: 1.7661\n",
      "Epoch [35/60], Batch [10/447], Loss: 1.7570\n",
      "Epoch [35/60], Batch [20/447], Loss: 2.3529\n",
      "Epoch [35/60], Batch [30/447], Loss: 2.0401\n",
      "Epoch [35/60], Batch [40/447], Loss: 2.3302\n",
      "Epoch [35/60], Batch [50/447], Loss: 2.4546\n",
      "Epoch [35/60], Batch [60/447], Loss: 2.7568\n",
      "Epoch [35/60], Batch [70/447], Loss: 2.4205\n",
      "Epoch [35/60], Batch [80/447], Loss: 2.2621\n",
      "Epoch [35/60], Batch [90/447], Loss: 1.4823\n",
      "Epoch [35/60], Batch [100/447], Loss: 2.6040\n",
      "Epoch [35/60], Batch [110/447], Loss: 1.6987\n",
      "Epoch [35/60], Batch [120/447], Loss: 1.3024\n",
      "Epoch [35/60], Batch [130/447], Loss: 2.4310\n",
      "Epoch [35/60], Batch [140/447], Loss: 2.2060\n",
      "Epoch [35/60], Batch [150/447], Loss: 2.3750\n",
      "Epoch [35/60], Batch [160/447], Loss: 2.5179\n",
      "Epoch [35/60], Batch [170/447], Loss: 1.8595\n",
      "Epoch [35/60], Batch [180/447], Loss: 1.6728\n",
      "Epoch [35/60], Batch [190/447], Loss: 2.3006\n",
      "Epoch [35/60], Batch [200/447], Loss: 2.5487\n",
      "Epoch [35/60], Batch [210/447], Loss: 2.3557\n",
      "Epoch [35/60], Batch [220/447], Loss: 1.7786\n",
      "Epoch [35/60], Batch [230/447], Loss: 2.3353\n",
      "Epoch [35/60], Batch [240/447], Loss: 2.6503\n",
      "Epoch [35/60], Batch [250/447], Loss: 2.2688\n",
      "Epoch [35/60], Batch [260/447], Loss: 2.0099\n",
      "Epoch [35/60], Batch [270/447], Loss: 2.1441\n",
      "Epoch [35/60], Batch [280/447], Loss: 2.6369\n",
      "Epoch [35/60], Batch [290/447], Loss: 2.7001\n",
      "Epoch [35/60], Batch [300/447], Loss: 2.6844\n",
      "Epoch [35/60], Batch [310/447], Loss: 2.4982\n",
      "Epoch [35/60], Batch [320/447], Loss: 2.3285\n",
      "Epoch [35/60], Batch [330/447], Loss: 1.9222\n",
      "Epoch [35/60], Batch [340/447], Loss: 2.0200\n",
      "Epoch [35/60], Batch [350/447], Loss: 2.4974\n",
      "Epoch [35/60], Batch [360/447], Loss: 2.2353\n",
      "Epoch [35/60], Batch [370/447], Loss: 1.9125\n",
      "Epoch [35/60], Batch [380/447], Loss: 2.4825\n",
      "Epoch [35/60], Batch [390/447], Loss: 2.0642\n",
      "Epoch [35/60], Batch [400/447], Loss: 2.0308\n",
      "Epoch [35/60], Batch [410/447], Loss: 2.7444\n",
      "Epoch [35/60], Batch [420/447], Loss: 2.7635\n",
      "Epoch [35/60], Batch [430/447], Loss: 2.0761\n",
      "Epoch [35/60], Batch [440/447], Loss: 2.1873\n",
      "Epoch [35/60], Average Train Loss: 2.2721\n",
      "Epoch [35/60], Validation Accuracy: 0.7582, Validation Loss: 0.8939\n",
      "No improvement in validation accuracy for 2 epoch(s).\n",
      "Epoch [36/60], Batch [0/447], Loss: 1.7901\n",
      "Epoch [36/60], Batch [10/447], Loss: 2.2586\n",
      "Epoch [36/60], Batch [20/447], Loss: 1.9835\n",
      "Epoch [36/60], Batch [30/447], Loss: 2.3228\n",
      "Epoch [36/60], Batch [40/447], Loss: 2.0746\n",
      "Epoch [36/60], Batch [50/447], Loss: 2.2533\n",
      "Epoch [36/60], Batch [60/447], Loss: 2.3308\n",
      "Epoch [36/60], Batch [70/447], Loss: 2.4816\n",
      "Epoch [36/60], Batch [80/447], Loss: 2.2342\n",
      "Epoch [36/60], Batch [90/447], Loss: 2.1047\n",
      "Epoch [36/60], Batch [100/447], Loss: 2.6819\n",
      "Epoch [36/60], Batch [110/447], Loss: 2.1815\n",
      "Epoch [36/60], Batch [120/447], Loss: 1.9766\n",
      "Epoch [36/60], Batch [130/447], Loss: 2.1215\n",
      "Epoch [36/60], Batch [140/447], Loss: 2.4892\n",
      "Epoch [36/60], Batch [150/447], Loss: 2.7897\n",
      "Epoch [36/60], Batch [160/447], Loss: 1.3383\n",
      "Epoch [36/60], Batch [170/447], Loss: 2.0299\n",
      "Epoch [36/60], Batch [180/447], Loss: 1.8493\n",
      "Epoch [36/60], Batch [190/447], Loss: 2.3664\n",
      "Epoch [36/60], Batch [200/447], Loss: 2.4301\n",
      "Epoch [36/60], Batch [210/447], Loss: 2.2971\n",
      "Epoch [36/60], Batch [220/447], Loss: 1.8560\n",
      "Epoch [36/60], Batch [230/447], Loss: 1.9338\n",
      "Epoch [36/60], Batch [240/447], Loss: 2.4966\n",
      "Epoch [36/60], Batch [250/447], Loss: 2.0021\n",
      "Epoch [36/60], Batch [260/447], Loss: 2.7495\n",
      "Epoch [36/60], Batch [270/447], Loss: 2.1682\n",
      "Epoch [36/60], Batch [280/447], Loss: 2.0564\n",
      "Epoch [36/60], Batch [290/447], Loss: 2.7480\n",
      "Epoch [36/60], Batch [300/447], Loss: 1.6065\n",
      "Epoch [36/60], Batch [310/447], Loss: 2.0875\n",
      "Epoch [36/60], Batch [320/447], Loss: 2.0065\n",
      "Epoch [36/60], Batch [330/447], Loss: 2.6298\n",
      "Epoch [36/60], Batch [340/447], Loss: 2.1327\n",
      "Epoch [36/60], Batch [350/447], Loss: 2.0833\n",
      "Epoch [36/60], Batch [360/447], Loss: 2.6267\n",
      "Epoch [36/60], Batch [370/447], Loss: 2.4247\n",
      "Epoch [36/60], Batch [380/447], Loss: 2.6548\n",
      "Epoch [36/60], Batch [390/447], Loss: 2.1669\n",
      "Epoch [36/60], Batch [400/447], Loss: 2.1431\n",
      "Epoch [36/60], Batch [410/447], Loss: 2.3268\n",
      "Epoch [36/60], Batch [420/447], Loss: 2.5417\n",
      "Epoch [36/60], Batch [430/447], Loss: 1.6995\n",
      "Epoch [36/60], Batch [440/447], Loss: 2.5747\n",
      "Epoch [36/60], Average Train Loss: 2.2817\n",
      "Epoch [36/60], Validation Accuracy: 0.7569, Validation Loss: 0.9157\n",
      "No improvement in validation accuracy for 3 epoch(s).\n",
      "Epoch [37/60], Batch [0/447], Loss: 2.7369\n",
      "Epoch [37/60], Batch [10/447], Loss: 2.5557\n",
      "Epoch [37/60], Batch [20/447], Loss: 2.5939\n",
      "Epoch [37/60], Batch [30/447], Loss: 2.0265\n",
      "Epoch [37/60], Batch [40/447], Loss: 2.2831\n",
      "Epoch [37/60], Batch [50/447], Loss: 2.7245\n",
      "Epoch [37/60], Batch [60/447], Loss: 2.1791\n",
      "Epoch [37/60], Batch [70/447], Loss: 2.3532\n",
      "Epoch [37/60], Batch [80/447], Loss: 1.7364\n",
      "Epoch [37/60], Batch [90/447], Loss: 2.1413\n",
      "Epoch [37/60], Batch [100/447], Loss: 2.7622\n",
      "Epoch [37/60], Batch [110/447], Loss: 2.0106\n",
      "Epoch [37/60], Batch [120/447], Loss: 3.1360\n",
      "Epoch [37/60], Batch [130/447], Loss: 2.6123\n",
      "Epoch [37/60], Batch [140/447], Loss: 2.0936\n",
      "Epoch [37/60], Batch [150/447], Loss: 2.8709\n",
      "Epoch [37/60], Batch [160/447], Loss: 2.0364\n",
      "Epoch [37/60], Batch [170/447], Loss: 1.7585\n",
      "Epoch [37/60], Batch [180/447], Loss: 2.7177\n",
      "Epoch [37/60], Batch [190/447], Loss: 3.2201\n",
      "Epoch [37/60], Batch [200/447], Loss: 2.0283\n",
      "Epoch [37/60], Batch [210/447], Loss: 1.4972\n",
      "Epoch [37/60], Batch [220/447], Loss: 2.0879\n",
      "Epoch [37/60], Batch [230/447], Loss: 1.6666\n",
      "Epoch [37/60], Batch [240/447], Loss: 2.8781\n",
      "Epoch [37/60], Batch [250/447], Loss: 1.9951\n",
      "Epoch [37/60], Batch [260/447], Loss: 1.7645\n",
      "Epoch [37/60], Batch [270/447], Loss: 2.5894\n",
      "Epoch [37/60], Batch [280/447], Loss: 3.1607\n",
      "Epoch [37/60], Batch [290/447], Loss: 2.1597\n",
      "Epoch [37/60], Batch [300/447], Loss: 1.6214\n",
      "Epoch [37/60], Batch [310/447], Loss: 2.6272\n",
      "Epoch [37/60], Batch [320/447], Loss: 2.5784\n",
      "Epoch [37/60], Batch [330/447], Loss: 2.1950\n",
      "Epoch [37/60], Batch [340/447], Loss: 2.2201\n",
      "Epoch [37/60], Batch [350/447], Loss: 2.3376\n",
      "Epoch [37/60], Batch [360/447], Loss: 2.0121\n",
      "Epoch [37/60], Batch [370/447], Loss: 2.3157\n",
      "Epoch [37/60], Batch [380/447], Loss: 2.0864\n",
      "Epoch [37/60], Batch [390/447], Loss: 2.3775\n",
      "Epoch [37/60], Batch [400/447], Loss: 1.8484\n",
      "Epoch [37/60], Batch [410/447], Loss: 2.3778\n",
      "Epoch [37/60], Batch [420/447], Loss: 2.2846\n",
      "Epoch [37/60], Batch [430/447], Loss: 3.0339\n",
      "Epoch [37/60], Batch [440/447], Loss: 2.5254\n",
      "Epoch [37/60], Average Train Loss: 2.2789\n",
      "Epoch [37/60], Validation Accuracy: 0.7601, Validation Loss: 0.8759\n",
      "No improvement in validation accuracy for 4 epoch(s).\n",
      "Epoch [38/60], Batch [0/447], Loss: 2.0703\n",
      "Epoch [38/60], Batch [10/447], Loss: 1.5087\n",
      "Epoch [38/60], Batch [20/447], Loss: 2.4045\n",
      "Epoch [38/60], Batch [30/447], Loss: 2.6096\n",
      "Epoch [38/60], Batch [40/447], Loss: 2.2724\n",
      "Epoch [38/60], Batch [50/447], Loss: 1.8297\n",
      "Epoch [38/60], Batch [60/447], Loss: 3.1466\n",
      "Epoch [38/60], Batch [70/447], Loss: 2.2776\n",
      "Epoch [38/60], Batch [80/447], Loss: 2.1095\n",
      "Epoch [38/60], Batch [90/447], Loss: 2.4073\n",
      "Epoch [38/60], Batch [100/447], Loss: 2.7708\n",
      "Epoch [38/60], Batch [110/447], Loss: 2.0209\n",
      "Epoch [38/60], Batch [120/447], Loss: 2.1132\n",
      "Epoch [38/60], Batch [130/447], Loss: 2.1626\n",
      "Epoch [38/60], Batch [140/447], Loss: 2.3833\n",
      "Epoch [38/60], Batch [150/447], Loss: 2.5057\n",
      "Epoch [38/60], Batch [160/447], Loss: 2.5840\n",
      "Epoch [38/60], Batch [170/447], Loss: 1.8442\n",
      "Epoch [38/60], Batch [180/447], Loss: 2.2188\n",
      "Epoch [38/60], Batch [190/447], Loss: 2.3038\n",
      "Epoch [38/60], Batch [200/447], Loss: 2.0879\n",
      "Epoch [38/60], Batch [210/447], Loss: 2.5823\n",
      "Epoch [38/60], Batch [220/447], Loss: 2.3222\n",
      "Epoch [38/60], Batch [230/447], Loss: 1.8354\n",
      "Epoch [38/60], Batch [240/447], Loss: 1.9699\n",
      "Epoch [38/60], Batch [250/447], Loss: 2.2675\n",
      "Epoch [38/60], Batch [260/447], Loss: 2.3459\n",
      "Epoch [38/60], Batch [270/447], Loss: 2.5785\n",
      "Epoch [38/60], Batch [280/447], Loss: 2.0999\n",
      "Epoch [38/60], Batch [290/447], Loss: 2.5696\n",
      "Epoch [38/60], Batch [300/447], Loss: 1.7846\n",
      "Epoch [38/60], Batch [310/447], Loss: 2.3058\n",
      "Epoch [38/60], Batch [320/447], Loss: 2.2564\n",
      "Epoch [38/60], Batch [330/447], Loss: 2.8758\n",
      "Epoch [38/60], Batch [340/447], Loss: 2.8396\n",
      "Epoch [38/60], Batch [350/447], Loss: 1.8015\n",
      "Epoch [38/60], Batch [360/447], Loss: 2.2873\n",
      "Epoch [38/60], Batch [370/447], Loss: 2.4148\n",
      "Epoch [38/60], Batch [380/447], Loss: 1.6541\n",
      "Epoch [38/60], Batch [390/447], Loss: 2.1218\n",
      "Epoch [38/60], Batch [400/447], Loss: 2.2065\n",
      "Epoch [38/60], Batch [410/447], Loss: 2.1533\n",
      "Epoch [38/60], Batch [420/447], Loss: 2.4805\n",
      "Epoch [38/60], Batch [430/447], Loss: 2.9231\n",
      "Epoch [38/60], Batch [440/447], Loss: 1.5101\n",
      "Epoch [38/60], Average Train Loss: 2.2196\n",
      "Epoch [38/60], Validation Accuracy: 0.7699, Validation Loss: 0.8814\n",
      "New best validation accuracy: 0.7699\n",
      "Epoch [39/60], Batch [0/447], Loss: 2.4418\n",
      "Epoch [39/60], Batch [10/447], Loss: 2.2006\n",
      "Epoch [39/60], Batch [20/447], Loss: 2.3284\n",
      "Epoch [39/60], Batch [30/447], Loss: 2.5658\n",
      "Epoch [39/60], Batch [40/447], Loss: 1.9695\n",
      "Epoch [39/60], Batch [50/447], Loss: 2.4159\n",
      "Epoch [39/60], Batch [60/447], Loss: 1.5538\n",
      "Epoch [39/60], Batch [70/447], Loss: 2.0254\n",
      "Epoch [39/60], Batch [80/447], Loss: 2.7667\n",
      "Epoch [39/60], Batch [90/447], Loss: 2.5265\n",
      "Epoch [39/60], Batch [100/447], Loss: 1.9393\n",
      "Epoch [39/60], Batch [110/447], Loss: 2.5121\n",
      "Epoch [39/60], Batch [120/447], Loss: 1.9126\n",
      "Epoch [39/60], Batch [130/447], Loss: 2.1800\n",
      "Epoch [39/60], Batch [140/447], Loss: 2.9285\n",
      "Epoch [39/60], Batch [150/447], Loss: 2.1476\n",
      "Epoch [39/60], Batch [160/447], Loss: 1.7128\n",
      "Epoch [39/60], Batch [170/447], Loss: 1.6893\n",
      "Epoch [39/60], Batch [180/447], Loss: 2.5798\n",
      "Epoch [39/60], Batch [190/447], Loss: 2.2377\n",
      "Epoch [39/60], Batch [200/447], Loss: 1.8367\n",
      "Epoch [39/60], Batch [210/447], Loss: 2.3260\n",
      "Epoch [39/60], Batch [220/447], Loss: 2.0363\n",
      "Epoch [39/60], Batch [230/447], Loss: 2.3876\n",
      "Epoch [39/60], Batch [240/447], Loss: 2.4663\n",
      "Epoch [39/60], Batch [250/447], Loss: 2.2280\n",
      "Epoch [39/60], Batch [260/447], Loss: 1.7198\n",
      "Epoch [39/60], Batch [270/447], Loss: 2.4226\n",
      "Epoch [39/60], Batch [280/447], Loss: 1.7489\n",
      "Epoch [39/60], Batch [290/447], Loss: 1.8597\n",
      "Epoch [39/60], Batch [300/447], Loss: 2.3195\n",
      "Epoch [39/60], Batch [310/447], Loss: 0.9469\n",
      "Epoch [39/60], Batch [320/447], Loss: 2.9639\n",
      "Epoch [39/60], Batch [330/447], Loss: 2.6884\n",
      "Epoch [39/60], Batch [340/447], Loss: 1.8138\n",
      "Epoch [39/60], Batch [350/447], Loss: 2.9459\n",
      "Epoch [39/60], Batch [360/447], Loss: 2.1849\n",
      "Epoch [39/60], Batch [370/447], Loss: 2.0694\n",
      "Epoch [39/60], Batch [380/447], Loss: 2.4034\n",
      "Epoch [39/60], Batch [390/447], Loss: 1.8323\n",
      "Epoch [39/60], Batch [400/447], Loss: 2.9523\n",
      "Epoch [39/60], Batch [410/447], Loss: 2.6066\n",
      "Epoch [39/60], Batch [420/447], Loss: 2.2415\n",
      "Epoch [39/60], Batch [430/447], Loss: 1.8807\n",
      "Epoch [39/60], Batch [440/447], Loss: 2.1521\n",
      "Epoch [39/60], Average Train Loss: 2.1858\n",
      "Epoch [39/60], Validation Accuracy: 0.7614, Validation Loss: 0.8828\n",
      "No improvement in validation accuracy for 1 epoch(s).\n",
      "Epoch [40/60], Batch [0/447], Loss: 2.0813\n",
      "Epoch [40/60], Batch [10/447], Loss: 2.5227\n",
      "Epoch [40/60], Batch [20/447], Loss: 2.1096\n",
      "Epoch [40/60], Batch [30/447], Loss: 2.9607\n",
      "Epoch [40/60], Batch [40/447], Loss: 2.3509\n",
      "Epoch [40/60], Batch [50/447], Loss: 2.3190\n",
      "Epoch [40/60], Batch [60/447], Loss: 2.1754\n",
      "Epoch [40/60], Batch [70/447], Loss: 2.7561\n",
      "Epoch [40/60], Batch [80/447], Loss: 2.6296\n",
      "Epoch [40/60], Batch [90/447], Loss: 1.7716\n",
      "Epoch [40/60], Batch [100/447], Loss: 2.0489\n",
      "Epoch [40/60], Batch [110/447], Loss: 1.5485\n",
      "Epoch [40/60], Batch [120/447], Loss: 1.9957\n",
      "Epoch [40/60], Batch [130/447], Loss: 2.3753\n",
      "Epoch [40/60], Batch [140/447], Loss: 2.6455\n",
      "Epoch [40/60], Batch [150/447], Loss: 1.8661\n",
      "Epoch [40/60], Batch [160/447], Loss: 2.2664\n",
      "Epoch [40/60], Batch [170/447], Loss: 1.8178\n",
      "Epoch [40/60], Batch [180/447], Loss: 2.0397\n",
      "Epoch [40/60], Batch [190/447], Loss: 1.0679\n",
      "Epoch [40/60], Batch [200/447], Loss: 1.6537\n",
      "Epoch [40/60], Batch [210/447], Loss: 2.0980\n",
      "Epoch [40/60], Batch [220/447], Loss: 2.3331\n",
      "Epoch [40/60], Batch [230/447], Loss: 2.4495\n",
      "Epoch [40/60], Batch [240/447], Loss: 2.0888\n",
      "Epoch [40/60], Batch [250/447], Loss: 1.9791\n",
      "Epoch [40/60], Batch [260/447], Loss: 2.0324\n",
      "Epoch [40/60], Batch [270/447], Loss: 1.9366\n",
      "Epoch [40/60], Batch [280/447], Loss: 2.6078\n",
      "Epoch [40/60], Batch [290/447], Loss: 1.5541\n",
      "Epoch [40/60], Batch [300/447], Loss: 3.0168\n",
      "Epoch [40/60], Batch [310/447], Loss: 2.9678\n",
      "Epoch [40/60], Batch [320/447], Loss: 2.7576\n",
      "Epoch [40/60], Batch [330/447], Loss: 2.0598\n",
      "Epoch [40/60], Batch [340/447], Loss: 2.1413\n",
      "Epoch [40/60], Batch [350/447], Loss: 1.3039\n",
      "Epoch [40/60], Batch [360/447], Loss: 2.6730\n",
      "Epoch [40/60], Batch [370/447], Loss: 2.1192\n",
      "Epoch [40/60], Batch [380/447], Loss: 1.8248\n",
      "Epoch [40/60], Batch [390/447], Loss: 2.2111\n",
      "Epoch [40/60], Batch [400/447], Loss: 2.3551\n",
      "Epoch [40/60], Batch [410/447], Loss: 2.5312\n",
      "Epoch [40/60], Batch [420/447], Loss: 2.0522\n",
      "Epoch [40/60], Batch [430/447], Loss: 1.1935\n",
      "Epoch [40/60], Batch [440/447], Loss: 2.2307\n",
      "Epoch [40/60], Average Train Loss: 2.1814\n",
      "Epoch [40/60], Validation Accuracy: 0.7601, Validation Loss: 0.9073\n",
      "No improvement in validation accuracy for 2 epoch(s).\n",
      "Epoch [41/60], Batch [0/447], Loss: 2.6198\n",
      "Epoch [41/60], Batch [10/447], Loss: 2.0289\n",
      "Epoch [41/60], Batch [20/447], Loss: 2.0324\n",
      "Epoch [41/60], Batch [30/447], Loss: 2.2524\n",
      "Epoch [41/60], Batch [40/447], Loss: 2.3443\n",
      "Epoch [41/60], Batch [50/447], Loss: 2.9453\n",
      "Epoch [41/60], Batch [60/447], Loss: 1.1757\n",
      "Epoch [41/60], Batch [70/447], Loss: 2.3075\n",
      "Epoch [41/60], Batch [80/447], Loss: 1.8818\n",
      "Epoch [41/60], Batch [90/447], Loss: 2.0161\n",
      "Epoch [41/60], Batch [100/447], Loss: 2.5152\n",
      "Epoch [41/60], Batch [110/447], Loss: 2.3142\n",
      "Epoch [41/60], Batch [120/447], Loss: 2.3673\n",
      "Epoch [41/60], Batch [130/447], Loss: 2.8324\n",
      "Epoch [41/60], Batch [140/447], Loss: 2.5843\n",
      "Epoch [41/60], Batch [150/447], Loss: 2.1461\n",
      "Epoch [41/60], Batch [160/447], Loss: 1.8317\n",
      "Epoch [41/60], Batch [170/447], Loss: 2.1884\n",
      "Epoch [41/60], Batch [180/447], Loss: 2.1097\n",
      "Epoch [41/60], Batch [190/447], Loss: 2.1865\n",
      "Epoch [41/60], Batch [200/447], Loss: 2.7408\n",
      "Epoch [41/60], Batch [210/447], Loss: 1.4093\n",
      "Epoch [41/60], Batch [220/447], Loss: 1.7676\n",
      "Epoch [41/60], Batch [230/447], Loss: 2.2591\n",
      "Epoch [41/60], Batch [240/447], Loss: 2.3030\n",
      "Epoch [41/60], Batch [250/447], Loss: 1.7181\n",
      "Epoch [41/60], Batch [260/447], Loss: 2.6809\n",
      "Epoch [41/60], Batch [270/447], Loss: 1.8094\n",
      "Epoch [41/60], Batch [280/447], Loss: 2.6166\n",
      "Epoch [41/60], Batch [290/447], Loss: 1.6072\n",
      "Epoch [41/60], Batch [300/447], Loss: 2.4809\n",
      "Epoch [41/60], Batch [310/447], Loss: 2.0175\n",
      "Epoch [41/60], Batch [320/447], Loss: 1.7537\n",
      "Epoch [41/60], Batch [330/447], Loss: 2.7166\n",
      "Epoch [41/60], Batch [340/447], Loss: 2.9426\n",
      "Epoch [41/60], Batch [350/447], Loss: 2.7118\n",
      "Epoch [41/60], Batch [360/447], Loss: 2.8909\n",
      "Epoch [41/60], Batch [370/447], Loss: 2.7178\n",
      "Epoch [41/60], Batch [380/447], Loss: 2.5363\n",
      "Epoch [41/60], Batch [390/447], Loss: 2.7232\n",
      "Epoch [41/60], Batch [400/447], Loss: 1.8425\n",
      "Epoch [41/60], Batch [410/447], Loss: 2.3114\n",
      "Epoch [41/60], Batch [420/447], Loss: 2.0338\n",
      "Epoch [41/60], Batch [430/447], Loss: 2.1843\n",
      "Epoch [41/60], Batch [440/447], Loss: 2.5907\n",
      "Epoch [41/60], Average Train Loss: 2.1715\n",
      "Epoch [41/60], Validation Accuracy: 0.7765, Validation Loss: 0.8688\n",
      "New best validation accuracy: 0.7765\n",
      "Epoch [42/60], Batch [0/447], Loss: 2.0329\n",
      "Epoch [42/60], Batch [10/447], Loss: 3.2536\n",
      "Epoch [42/60], Batch [20/447], Loss: 1.9802\n",
      "Epoch [42/60], Batch [30/447], Loss: 2.4021\n",
      "Epoch [42/60], Batch [40/447], Loss: 2.4340\n",
      "Epoch [42/60], Batch [50/447], Loss: 1.5564\n",
      "Epoch [42/60], Batch [60/447], Loss: 2.0134\n",
      "Epoch [42/60], Batch [70/447], Loss: 2.2239\n",
      "Epoch [42/60], Batch [80/447], Loss: 1.9770\n",
      "Epoch [42/60], Batch [90/447], Loss: 2.7682\n",
      "Epoch [42/60], Batch [100/447], Loss: 2.6312\n",
      "Epoch [42/60], Batch [110/447], Loss: 1.1381\n",
      "Epoch [42/60], Batch [120/447], Loss: 1.8351\n",
      "Epoch [42/60], Batch [130/447], Loss: 2.1431\n",
      "Epoch [42/60], Batch [140/447], Loss: 2.0818\n",
      "Epoch [42/60], Batch [150/447], Loss: 2.7722\n",
      "Epoch [42/60], Batch [160/447], Loss: 1.8458\n",
      "Epoch [42/60], Batch [170/447], Loss: 2.8818\n",
      "Epoch [42/60], Batch [180/447], Loss: 1.6380\n",
      "Epoch [42/60], Batch [190/447], Loss: 2.7488\n",
      "Epoch [42/60], Batch [200/447], Loss: 2.6508\n",
      "Epoch [42/60], Batch [210/447], Loss: 1.6228\n",
      "Epoch [42/60], Batch [220/447], Loss: 1.3562\n",
      "Epoch [42/60], Batch [230/447], Loss: 2.3133\n",
      "Epoch [42/60], Batch [240/447], Loss: 2.0214\n",
      "Epoch [42/60], Batch [250/447], Loss: 2.1625\n",
      "Epoch [42/60], Batch [260/447], Loss: 2.0282\n",
      "Epoch [42/60], Batch [270/447], Loss: 2.0706\n",
      "Epoch [42/60], Batch [280/447], Loss: 1.8859\n",
      "Epoch [42/60], Batch [290/447], Loss: 1.6963\n",
      "Epoch [42/60], Batch [300/447], Loss: 2.4363\n",
      "Epoch [42/60], Batch [310/447], Loss: 2.1491\n",
      "Epoch [42/60], Batch [320/447], Loss: 2.1100\n",
      "Epoch [42/60], Batch [330/447], Loss: 1.5548\n",
      "Epoch [42/60], Batch [340/447], Loss: 2.2563\n",
      "Epoch [42/60], Batch [350/447], Loss: 2.6010\n",
      "Epoch [42/60], Batch [360/447], Loss: 2.2953\n",
      "Epoch [42/60], Batch [370/447], Loss: 2.4256\n",
      "Epoch [42/60], Batch [380/447], Loss: 1.8176\n",
      "Epoch [42/60], Batch [390/447], Loss: 2.2667\n",
      "Epoch [42/60], Batch [400/447], Loss: 2.6376\n",
      "Epoch [42/60], Batch [410/447], Loss: 1.8967\n",
      "Epoch [42/60], Batch [420/447], Loss: 2.3058\n",
      "Epoch [42/60], Batch [430/447], Loss: 2.8202\n",
      "Epoch [42/60], Batch [440/447], Loss: 1.2732\n",
      "Epoch [42/60], Average Train Loss: 2.1594\n",
      "Epoch [42/60], Validation Accuracy: 0.7641, Validation Loss: 0.9066\n",
      "No improvement in validation accuracy for 1 epoch(s).\n",
      "Epoch [43/60], Batch [0/447], Loss: 2.2922\n",
      "Epoch [43/60], Batch [10/447], Loss: 2.5038\n",
      "Epoch [43/60], Batch [20/447], Loss: 1.9399\n",
      "Epoch [43/60], Batch [30/447], Loss: 2.0724\n",
      "Epoch [43/60], Batch [40/447], Loss: 2.1243\n",
      "Epoch [43/60], Batch [50/447], Loss: 1.6014\n",
      "Epoch [43/60], Batch [60/447], Loss: 2.0549\n",
      "Epoch [43/60], Batch [70/447], Loss: 2.1495\n",
      "Epoch [43/60], Batch [80/447], Loss: 1.7877\n",
      "Epoch [43/60], Batch [90/447], Loss: 2.4410\n",
      "Epoch [43/60], Batch [100/447], Loss: 1.7538\n",
      "Epoch [43/60], Batch [110/447], Loss: 2.2632\n",
      "Epoch [43/60], Batch [120/447], Loss: 2.2001\n",
      "Epoch [43/60], Batch [130/447], Loss: 2.6539\n",
      "Epoch [43/60], Batch [140/447], Loss: 1.9394\n",
      "Epoch [43/60], Batch [150/447], Loss: 2.3103\n",
      "Epoch [43/60], Batch [160/447], Loss: 2.1007\n",
      "Epoch [43/60], Batch [170/447], Loss: 2.0295\n",
      "Epoch [43/60], Batch [180/447], Loss: 2.9539\n",
      "Epoch [43/60], Batch [190/447], Loss: 2.2831\n",
      "Epoch [43/60], Batch [200/447], Loss: 1.9713\n",
      "Epoch [43/60], Batch [210/447], Loss: 1.8085\n",
      "Epoch [43/60], Batch [220/447], Loss: 2.5215\n",
      "Epoch [43/60], Batch [230/447], Loss: 1.9930\n",
      "Epoch [43/60], Batch [240/447], Loss: 2.2496\n",
      "Epoch [43/60], Batch [250/447], Loss: 1.4950\n",
      "Epoch [43/60], Batch [260/447], Loss: 2.5618\n",
      "Epoch [43/60], Batch [270/447], Loss: 2.4066\n",
      "Epoch [43/60], Batch [280/447], Loss: 2.6406\n",
      "Epoch [43/60], Batch [290/447], Loss: 1.5235\n",
      "Epoch [43/60], Batch [300/447], Loss: 2.1150\n",
      "Epoch [43/60], Batch [310/447], Loss: 2.3436\n",
      "Epoch [43/60], Batch [320/447], Loss: 1.5726\n",
      "Epoch [43/60], Batch [330/447], Loss: 1.5647\n",
      "Epoch [43/60], Batch [340/447], Loss: 2.1265\n",
      "Epoch [43/60], Batch [350/447], Loss: 2.0730\n",
      "Epoch [43/60], Batch [360/447], Loss: 3.3723\n",
      "Epoch [43/60], Batch [370/447], Loss: 1.8346\n",
      "Epoch [43/60], Batch [380/447], Loss: 2.0135\n",
      "Epoch [43/60], Batch [390/447], Loss: 2.8290\n",
      "Epoch [43/60], Batch [400/447], Loss: 1.9357\n",
      "Epoch [43/60], Batch [410/447], Loss: 1.9529\n",
      "Epoch [43/60], Batch [420/447], Loss: 1.7652\n",
      "Epoch [43/60], Batch [430/447], Loss: 2.5446\n",
      "Epoch [43/60], Batch [440/447], Loss: 2.4635\n",
      "Epoch [43/60], Average Train Loss: 2.1660\n",
      "Epoch [43/60], Validation Accuracy: 0.7660, Validation Loss: 0.8978\n",
      "No improvement in validation accuracy for 2 epoch(s).\n",
      "Epoch [44/60], Batch [0/447], Loss: 2.1296\n",
      "Epoch [44/60], Batch [10/447], Loss: 2.8156\n",
      "Epoch [44/60], Batch [20/447], Loss: 2.4041\n",
      "Epoch [44/60], Batch [30/447], Loss: 1.7898\n",
      "Epoch [44/60], Batch [40/447], Loss: 2.0933\n",
      "Epoch [44/60], Batch [50/447], Loss: 2.5077\n",
      "Epoch [44/60], Batch [60/447], Loss: 2.1495\n",
      "Epoch [44/60], Batch [70/447], Loss: 2.3392\n",
      "Epoch [44/60], Batch [80/447], Loss: 1.3826\n",
      "Epoch [44/60], Batch [90/447], Loss: 2.3048\n",
      "Epoch [44/60], Batch [100/447], Loss: 2.0830\n",
      "Epoch [44/60], Batch [110/447], Loss: 2.1506\n",
      "Epoch [44/60], Batch [120/447], Loss: 1.8278\n",
      "Epoch [44/60], Batch [130/447], Loss: 2.3329\n",
      "Epoch [44/60], Batch [140/447], Loss: 1.7495\n",
      "Epoch [44/60], Batch [150/447], Loss: 2.1415\n",
      "Epoch [44/60], Batch [160/447], Loss: 2.3383\n",
      "Epoch [44/60], Batch [170/447], Loss: 2.5907\n",
      "Epoch [44/60], Batch [180/447], Loss: 1.6028\n",
      "Epoch [44/60], Batch [190/447], Loss: 1.4168\n",
      "Epoch [44/60], Batch [200/447], Loss: 1.2905\n",
      "Epoch [44/60], Batch [210/447], Loss: 1.9735\n",
      "Epoch [44/60], Batch [220/447], Loss: 2.3783\n",
      "Epoch [44/60], Batch [230/447], Loss: 1.8775\n",
      "Epoch [44/60], Batch [240/447], Loss: 2.1459\n",
      "Epoch [44/60], Batch [250/447], Loss: 1.9754\n",
      "Epoch [44/60], Batch [260/447], Loss: 1.8627\n",
      "Epoch [44/60], Batch [270/447], Loss: 1.5846\n",
      "Epoch [44/60], Batch [280/447], Loss: 1.9293\n",
      "Epoch [44/60], Batch [290/447], Loss: 2.4280\n",
      "Epoch [44/60], Batch [300/447], Loss: 2.7546\n",
      "Epoch [44/60], Batch [310/447], Loss: 3.2099\n",
      "Epoch [44/60], Batch [320/447], Loss: 1.6956\n",
      "Epoch [44/60], Batch [330/447], Loss: 2.6488\n",
      "Epoch [44/60], Batch [340/447], Loss: 1.6878\n",
      "Epoch [44/60], Batch [350/447], Loss: 2.6758\n",
      "Epoch [44/60], Batch [360/447], Loss: 2.2999\n",
      "Epoch [44/60], Batch [370/447], Loss: 1.2605\n",
      "Epoch [44/60], Batch [380/447], Loss: 1.2698\n",
      "Epoch [44/60], Batch [390/447], Loss: 2.3546\n",
      "Epoch [44/60], Batch [400/447], Loss: 1.8029\n",
      "Epoch [44/60], Batch [410/447], Loss: 2.7300\n",
      "Epoch [44/60], Batch [420/447], Loss: 2.1003\n",
      "Epoch [44/60], Batch [430/447], Loss: 1.4543\n",
      "Epoch [44/60], Batch [440/447], Loss: 1.7182\n",
      "Epoch [44/60], Average Train Loss: 2.1493\n",
      "Epoch [44/60], Validation Accuracy: 0.7588, Validation Loss: 0.8895\n",
      "No improvement in validation accuracy for 3 epoch(s).\n",
      "Epoch [45/60], Batch [0/447], Loss: 2.6104\n",
      "Epoch [45/60], Batch [10/447], Loss: 1.1876\n",
      "Epoch [45/60], Batch [20/447], Loss: 2.0094\n",
      "Epoch [45/60], Batch [30/447], Loss: 1.6727\n",
      "Epoch [45/60], Batch [40/447], Loss: 2.1154\n",
      "Epoch [45/60], Batch [50/447], Loss: 2.0040\n",
      "Epoch [45/60], Batch [60/447], Loss: 2.4399\n",
      "Epoch [45/60], Batch [70/447], Loss: 1.8320\n",
      "Epoch [45/60], Batch [80/447], Loss: 1.8658\n",
      "Epoch [45/60], Batch [90/447], Loss: 1.6147\n",
      "Epoch [45/60], Batch [100/447], Loss: 2.9590\n",
      "Epoch [45/60], Batch [110/447], Loss: 1.7341\n",
      "Epoch [45/60], Batch [120/447], Loss: 2.6276\n",
      "Epoch [45/60], Batch [130/447], Loss: 1.6914\n",
      "Epoch [45/60], Batch [140/447], Loss: 1.9932\n",
      "Epoch [45/60], Batch [150/447], Loss: 2.1210\n",
      "Epoch [45/60], Batch [160/447], Loss: 1.6124\n",
      "Epoch [45/60], Batch [170/447], Loss: 2.1758\n",
      "Epoch [45/60], Batch [180/447], Loss: 1.3257\n",
      "Epoch [45/60], Batch [190/447], Loss: 1.6492\n",
      "Epoch [45/60], Batch [200/447], Loss: 2.3462\n",
      "Epoch [45/60], Batch [210/447], Loss: 1.5782\n",
      "Epoch [45/60], Batch [220/447], Loss: 1.9854\n",
      "Epoch [45/60], Batch [230/447], Loss: 2.7365\n",
      "Epoch [45/60], Batch [240/447], Loss: 1.9302\n",
      "Epoch [45/60], Batch [250/447], Loss: 2.4598\n",
      "Epoch [45/60], Batch [260/447], Loss: 2.6330\n",
      "Epoch [45/60], Batch [270/447], Loss: 2.4548\n",
      "Epoch [45/60], Batch [280/447], Loss: 2.3217\n",
      "Epoch [45/60], Batch [290/447], Loss: 1.6738\n",
      "Epoch [45/60], Batch [300/447], Loss: 2.7328\n",
      "Epoch [45/60], Batch [310/447], Loss: 1.7752\n",
      "Epoch [45/60], Batch [320/447], Loss: 1.9190\n",
      "Epoch [45/60], Batch [330/447], Loss: 2.5646\n",
      "Epoch [45/60], Batch [340/447], Loss: 2.0402\n",
      "Epoch [45/60], Batch [350/447], Loss: 2.0538\n",
      "Epoch [45/60], Batch [360/447], Loss: 2.6295\n",
      "Epoch [45/60], Batch [370/447], Loss: 1.6381\n",
      "Epoch [45/60], Batch [380/447], Loss: 2.5532\n",
      "Epoch [45/60], Batch [390/447], Loss: 2.2114\n",
      "Epoch [45/60], Batch [400/447], Loss: 2.7503\n",
      "Epoch [45/60], Batch [410/447], Loss: 1.6813\n",
      "Epoch [45/60], Batch [420/447], Loss: 2.5858\n",
      "Epoch [45/60], Batch [430/447], Loss: 2.0162\n",
      "Epoch [45/60], Batch [440/447], Loss: 2.9527\n",
      "Epoch [45/60], Average Train Loss: 2.1490\n",
      "Epoch [45/60], Validation Accuracy: 0.7699, Validation Loss: 0.8921\n",
      "No improvement in validation accuracy for 4 epoch(s).\n",
      "Epoch [46/60], Batch [0/447], Loss: 1.9570\n",
      "Epoch [46/60], Batch [10/447], Loss: 1.8674\n",
      "Epoch [46/60], Batch [20/447], Loss: 2.1251\n",
      "Epoch [46/60], Batch [30/447], Loss: 2.3522\n",
      "Epoch [46/60], Batch [40/447], Loss: 2.1972\n",
      "Epoch [46/60], Batch [50/447], Loss: 2.4803\n",
      "Epoch [46/60], Batch [60/447], Loss: 2.2885\n",
      "Epoch [46/60], Batch [70/447], Loss: 1.8965\n",
      "Epoch [46/60], Batch [80/447], Loss: 2.4179\n",
      "Epoch [46/60], Batch [90/447], Loss: 2.2398\n",
      "Epoch [46/60], Batch [100/447], Loss: 2.0882\n",
      "Epoch [46/60], Batch [110/447], Loss: 2.2013\n",
      "Epoch [46/60], Batch [120/447], Loss: 2.3746\n",
      "Epoch [46/60], Batch [130/447], Loss: 2.0826\n",
      "Epoch [46/60], Batch [140/447], Loss: 1.5543\n",
      "Epoch [46/60], Batch [150/447], Loss: 2.1715\n",
      "Epoch [46/60], Batch [160/447], Loss: 1.7307\n",
      "Epoch [46/60], Batch [170/447], Loss: 2.8786\n",
      "Epoch [46/60], Batch [180/447], Loss: 2.2390\n",
      "Epoch [46/60], Batch [190/447], Loss: 2.4308\n",
      "Epoch [46/60], Batch [200/447], Loss: 2.3612\n",
      "Epoch [46/60], Batch [210/447], Loss: 1.3769\n",
      "Epoch [46/60], Batch [220/447], Loss: 2.4366\n",
      "Epoch [46/60], Batch [230/447], Loss: 2.0294\n",
      "Epoch [46/60], Batch [240/447], Loss: 2.0077\n",
      "Epoch [46/60], Batch [250/447], Loss: 1.8640\n",
      "Epoch [46/60], Batch [260/447], Loss: 1.6073\n",
      "Epoch [46/60], Batch [270/447], Loss: 1.8602\n",
      "Epoch [46/60], Batch [280/447], Loss: 1.7202\n",
      "Epoch [46/60], Batch [290/447], Loss: 2.4652\n",
      "Epoch [46/60], Batch [300/447], Loss: 1.9816\n",
      "Epoch [46/60], Batch [310/447], Loss: 2.0210\n",
      "Epoch [46/60], Batch [320/447], Loss: 1.7445\n",
      "Epoch [46/60], Batch [330/447], Loss: 1.2897\n",
      "Epoch [46/60], Batch [340/447], Loss: 2.7199\n",
      "Epoch [46/60], Batch [350/447], Loss: 1.8544\n",
      "Epoch [46/60], Batch [360/447], Loss: 1.4776\n",
      "Epoch [46/60], Batch [370/447], Loss: 1.6779\n",
      "Epoch [46/60], Batch [380/447], Loss: 2.7252\n",
      "Epoch [46/60], Batch [390/447], Loss: 1.1669\n",
      "Epoch [46/60], Batch [400/447], Loss: 1.1415\n",
      "Epoch [46/60], Batch [410/447], Loss: 2.4693\n",
      "Epoch [46/60], Batch [420/447], Loss: 2.3010\n",
      "Epoch [46/60], Batch [430/447], Loss: 2.1521\n",
      "Epoch [46/60], Batch [440/447], Loss: 1.3249\n",
      "Epoch [46/60], Average Train Loss: 2.1163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:32:19,501] Trial 1 finished with value: 0.7764705882352941 and parameters: {'alpha': 3.807947176588889}. Best is trial 1 with value: 0.7764705882352941.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/60], Validation Accuracy: 0.7556, Validation Loss: 0.8869\n",
      "No improvement in validation accuracy for 5 epoch(s).\n",
      "Early stopping at epoch 46.\n",
      "Best hyperparameters: {'alpha': 3.807947176588889}\n",
      "Best validation accuracy: 0.7765\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import logging\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Best parameters from previous study\n",
    "best_params = {\n",
    "    'dropout_ratio': 0.6795542149013333,\n",
    "    'lr': 7.886714129990479e-06,\n",
    "    'max_norm': 41,\n",
    "    'with_pool2': True,\n",
    "    'bottleneck_mode': 'ir',\n",
    "    'norm_eval': False,\n",
    "    'bn_frozen': False\n",
    "}\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='optuna_training_mixup_hmdb.log', \n",
    "                    filemode='w', \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "                    level=logging.INFO)\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Database file path for saving study\n",
    "db_file = \"sqlite:///optuna_study_mixup_hmdb.db\"\n",
    "\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "\n",
    "# Set up study with the option to minimize validation loss\n",
    "study = optuna.create_study(\n",
    "    sampler=sampler,\n",
    "    direction=\"maximize\",\n",
    "    study_name=\"mixup_hmdb\", \n",
    "    storage=db_file,\n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "def objective(trial):\n",
    "    print(\"Starting a new trial...\")\n",
    "\n",
    "    # Hyperparameters to tune\n",
    "    alpha = trial.suggest_float(\"alpha\", 0.1, 10)  \n",
    "    print(f\"Trial {trial.number}: alpha = {alpha}\")\n",
    "\n",
    "    # Set hyperparameters\n",
    "    dropout_ratio = best_params['dropout_ratio']\n",
    "    lr = best_params['lr']\n",
    "    max_norm = best_params['max_norm']\n",
    "\n",
    "    # Backbone parameters\n",
    "    cfg.model.backbone.with_pool2 = best_params['with_pool2']\n",
    "    cfg.model.backbone.bottleneck_mode = best_params['bottleneck_mode']\n",
    "    cfg.model.backbone.norm_eval = best_params['norm_eval']\n",
    "    cfg.model.backbone.bn_frozen = best_params['bn_frozen']\n",
    "\n",
    "    # Fixed pretrained URL\n",
    "    cfg.model.backbone.pretrained = 'https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r50_ig65m_20210617-ce545a37.pth'\n",
    "\n",
    "    # Adjust config parameters\n",
    "    cfg.model.cls_head.dropout_ratio = dropout_ratio\n",
    "\n",
    "    # Initialize model, criterion, optimizer, scheduler\n",
    "    print(\"Building the model...\")\n",
    "    model = build_model(cfg.model, train_cfg=None, test_cfg=cfg.get('test_cfg')).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        weight_decay=0.00001\n",
    "    )\n",
    "\n",
    "    print(\"Model built successfully!\")\n",
    "    \n",
    "    # Early stopping parameters\n",
    "    total_epochs = 60\n",
    "    eval_interval = 1\n",
    "    patience = 5\n",
    "    best_val_accuracy = 0\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    # Mixup Blending instance\n",
    "    mixup = MixupBlending(num_classes=cfg.model.cls_head.num_classes, alpha=alpha)\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(total_epochs):\n",
    "\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            inputs, labels = data['imgs'].to(device), data['label'].to(device)\n",
    "\n",
    "            # Convert labels to one-hot encoding\n",
    "            labels_one_hot = F.one_hot(labels, num_classes=cfg.model.cls_head.num_classes).float()\n",
    "\n",
    "            # Apply Mixup\n",
    "            mixed_inputs, mixed_labels = mixup.do_blending(inputs, labels_one_hot)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(mixed_inputs, mixed_labels, return_loss=True)\n",
    "            loss = outputs['loss_cls']\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{total_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        logging.info(f\"Epoch [{epoch + 1}/{total_epochs}], Train Loss: {epoch_loss / len(train_loader):.4f}\")\n",
    "        print(f\"Epoch [{epoch + 1}/{total_epochs}], Average Train Loss: {epoch_loss / len(train_loader):.4f}\")\n",
    "\n",
    "        # Validation loop (every `eval_interval` epochs)\n",
    "        if (epoch + 1) % eval_interval == 0:\n",
    "            model.eval()\n",
    "            total_val_loss = 0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            with torch.no_grad():\n",
    "                for val_data in val_loader:\n",
    "                    val_inputs, val_labels = val_data['imgs'].to(device), val_data['label'].to(device)\n",
    "\n",
    "                    val_results = model(val_inputs, return_loss=False)\n",
    "                    val_loss = model(val_inputs, val_labels, return_loss=True)['loss_cls']\n",
    "\n",
    "                    total_val_loss += val_loss.item()\n",
    "\n",
    "                    # Collect predictions and true labels\n",
    "                    predictions = np.argmax(val_results, axis=1)\n",
    "                    true_labels = val_labels.cpu().numpy()\n",
    "\n",
    "                    all_preds.extend(predictions)\n",
    "                    all_labels.extend(true_labels)\n",
    "\n",
    "            val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "            print(f\"Epoch [{epoch + 1}/{total_epochs}], Validation Accuracy: {val_accuracy:.4f}, Validation Loss: {total_val_loss / len(val_loader):.4f}\")\n",
    "            logging.info(f\"Epoch [{epoch + 1}/{total_epochs}], Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "            # Report validation loss to Optuna\n",
    "            trial.report(val_accuracy, epoch)\n",
    "\n",
    "            # Check if validation accuracy improved\n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                epochs_without_improvement = 0  # Reset counter\n",
    "                print(f\"New best validation accuracy: {val_accuracy:.4f}\")\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                print(f\"No improvement in validation accuracy for {epochs_without_improvement} epoch(s).\")\n",
    "\n",
    "            # Early stopping check\n",
    "            if epochs_without_improvement >= patience:\n",
    "                logging.info(f\"Early stopping at epoch {epoch + 1} due to no improvement in validation accuracy.\")\n",
    "                print(f\"Early stopping at epoch {epoch + 1}.\")\n",
    "                print(\"Trial early stopped due to lack of improvement.\")\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "            # Prune unpromising trials\n",
    "            if trial.should_prune():\n",
    "                print(\"Trial pruned due to lack of improvement.\")\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return best_val_accuracy\n",
    "\n",
    "# Run Optuna Study\n",
    "print(\"Starting Optuna study...\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "print(f\"Best hyperparameters: {study.best_params}\")\n",
    "print(f\"Best validation accuracy: {study.best_value:.4f}\")\n",
    "logging.info(\"Best hyperparameters: %s\", study.best_params)\n",
    "logging.info(\"Best validation accuracy: %f\", study.best_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8748ddae",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b2020f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial number: 0\n",
      "Parameters: {'alpha': 3.807947176588889}\n",
      "Value (e.g., validation accuracy): None\n",
      "------------------------------\n",
      "Trial number: 1\n",
      "Parameters: {'alpha': 3.807947176588889}\n",
      "Value (e.g., validation accuracy): 0.7764705882352941\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Retrieve all trials and print their parameters\n",
    "for trial in study.trials:\n",
    "    print(f\"Trial number: {trial.number}\")\n",
    "    print(f\"Parameters: {trial.params}\")\n",
    "    print(f\"Value (e.g., validation accuracy): {trial.value}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36f56844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial number: 1\n",
      "Best parameters: {'alpha': 3.807947176588889}\n",
      "Best validation loss: 0.7764705882352941\n"
     ]
    }
   ],
   "source": [
    "best_trial = study.best_trial\n",
    "print(\"Best trial number:\", best_trial.number)\n",
    "print(\"Best parameters:\", best_trial.params)\n",
    "print(\"Best validation loss:\", best_trial.value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrambmix",
   "language": "python",
   "name": "scrambmix"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
