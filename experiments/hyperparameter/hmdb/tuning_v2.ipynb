{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f6696d1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c5a298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from mmaction.datasets import build_dataset, build_dataloader\n",
    "from mmaction.models import build_model\n",
    "from mmcv import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f3cec5",
   "metadata": {},
   "source": [
    "## Loading batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9df620d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config.fromfile('./baseline_v2.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0258630",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2c341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.data.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccef55c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = build_dataset(cfg=cfg.data.train)\n",
    "train_loader = build_dataloader(\n",
    "        train_dataset,\n",
    "        videos_per_gpu=5,\n",
    "        workers_per_gpu=4,\n",
    "        persistent_workers=False,\n",
    "        num_gpus=1,\n",
    "        dist=False)\n",
    "\n",
    "val_dataset = build_dataset(cfg=cfg.data.val)\n",
    "val_loader = build_dataloader(\n",
    "        val_dataset,\n",
    "        videos_per_gpu=1,\n",
    "        workers_per_gpu=4,\n",
    "        persistent_workers=False,\n",
    "        num_gpus=1,\n",
    "        dist=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eacfdb",
   "metadata": {},
   "source": [
    "## Learning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b5656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from mmcv import Config\n",
    "import torch\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='optuna_training_adam.log', \n",
    "                    filemode='w', \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "                    level=logging.INFO)\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Database file path for saving study\n",
    "db_file = \"sqlite:///optuna_study_adam.db\"\n",
    "\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "\n",
    "# Set up study with the option to resume if it already exists\n",
    "study = optuna.create_study(\n",
    "    sampler=sampler,\n",
    "    direction=\"maximize\", \n",
    "    study_name=\"my_study\", \n",
    "    storage=db_file,\n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    dropout_ratio = trial.suggest_float(\"dropout_ratio\", 0.3, 0.7)\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-8, 1e-5)\n",
    "    max_norm = trial.suggest_int(\"max_norm\", 1, 50)\n",
    "    \n",
    "    # Backbone parameters\n",
    "    cfg.model.backbone.with_pool2 = trial.suggest_categorical(\"with_pool2\", [True, False])\n",
    "    cfg.model.backbone.bottleneck_mode = trial.suggest_categorical(\"bottleneck_mode\", [\"ir\", \"ip\"])\n",
    "    cfg.model.backbone.norm_eval = trial.suggest_categorical(\"norm_eval\", [True, False])\n",
    "    cfg.model.backbone.bn_frozen = trial.suggest_categorical(\"bn_frozen\", [True, False])\n",
    "    \n",
    "    # Fixed pretrained URL\n",
    "    cfg.model.backbone.pretrained = 'https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r50_ig65m_20210617-ce545a37.pth'\n",
    "\n",
    "    # Adjust config parameters\n",
    "    cfg.model.cls_head.dropout_ratio = dropout_ratio\n",
    "    \n",
    "    # Initialize model, criterion, optimizer, scheduler\n",
    "    model = build_model(cfg.model, train_cfg=None, test_cfg=cfg.get('test_cfg')).to(device)\n",
    "    \n",
    "    # Use Adam optimizer\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        weight_decay=0.00001\n",
    "    )\n",
    "    \n",
    "    # Early stopping parameters\n",
    "    total_epochs = 40\n",
    "    eval_interval = 1\n",
    "    patience = 5\n",
    "    best_val_accuracy = 0.0\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(total_epochs):\n",
    "\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        \n",
    "        for _, data in enumerate(train_loader):\n",
    "            inputs, labels = data['imgs'].to(device), data['label'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            results = model(inputs, labels, return_loss=True)\n",
    "            loss = results['loss_cls']\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            correct += (results['top1_acc'] * inputs.size(0))\n",
    "            total += inputs.size(0)\n",
    "\n",
    "        train_accuracy = correct / total\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "\n",
    "        logging.info(f\"Epoch [{epoch + 1}/{total_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "        # Validation loop (every `eval_interval` epochs)\n",
    "        if (epoch + 1) % eval_interval == 0:\n",
    "            model.eval()\n",
    "            val_running_loss, val_correct, val_total = 0.0, 0, 0\n",
    "            with torch.no_grad():\n",
    "                for val_data in val_loader:\n",
    "                    val_inputs, val_labels = val_data['imgs'].to(device), val_data['label'].to(device)\n",
    "                    \n",
    "                    val_results = model(val_inputs, val_labels, return_loss=True)\n",
    "                    val_loss = val_results['loss_cls']\n",
    "                    val_running_loss += val_loss.item()\n",
    "                    val_correct += (val_results['top1_acc'] * val_inputs.size(0))\n",
    "                    val_total += val_inputs.size(0)\n",
    "\n",
    "            val_accuracy = val_correct / val_total\n",
    "            val_loss = val_running_loss / len(val_loader)\n",
    "\n",
    "            # Report validation accuracy to Optuna\n",
    "            trial.report(val_accuracy, epoch)\n",
    "\n",
    "            # Check if validation accuracy improved\n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                epochs_without_improvement = 0  # Reset counter\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "\n",
    "            # Early stopping check\n",
    "            if epochs_without_improvement >= patience:\n",
    "                logging.info(f\"Early stopping at epoch {epoch + 1} due to no improvement in validation accuracy.\")\n",
    "                break\n",
    "\n",
    "            # Prune unpromising trials\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "    return best_val_accuracy\n",
    "\n",
    "\n",
    "# Run Optuna Study\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "logging.info(\"Best hyperparameters: %s\", study.best_params)\n",
    "logging.info(\"Best validation accuracy: %f\", study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8748ddae",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2020f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all trials and print their parameters\n",
    "for trial in study.trials:\n",
    "    print(f\"Trial number: {trial.number}\")\n",
    "    print(f\"Parameters: {trial.params}\")\n",
    "    print(f\"Value (e.g., validation accuracy): {trial.value}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f56844",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = study.best_trial\n",
    "print(\"Best trial number:\", best_trial.number)\n",
    "print(\"Best parameters:\", best_trial.params)\n",
    "print(\"Best validation accuracy:\", best_trial.value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrambmix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
