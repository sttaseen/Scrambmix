{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f6696d1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31c5a298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sadat/miniconda3/envs/scrambmix/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/sadat/miniconda3/envs/scrambmix/lib/python3.8/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from mmaction.datasets import build_dataset, build_dataloader\n",
    "from mmaction.models import build_model\n",
    "from mmcv import Config\n",
    "from mmaction.datasets import CutmixBlending\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0db510eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_score(y_true, y_pred):\n",
    "    \"\"\"Calculates precision score.\n",
    "\n",
    "    Args:\n",
    "        y_true: True labels.\n",
    "        y_pred: Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        Precision score.\n",
    "    \"\"\"\n",
    "\n",
    "    tp = sum(y_true[i] == y_pred[i] for i in range(len(y_true)) if y_pred[i] == 1)\n",
    "    fp = sum(y_true[i] != y_pred[i] for i in range(len(y_true)) if y_pred[i] == 1)\n",
    "    precision = tp / (tp + fp) if tp + fp != 0 else 0\n",
    "    return precision\n",
    "\n",
    "def recall_score(y_true, y_pred):\n",
    "    \"\"\"Calculates recall score.\n",
    "\n",
    "    Args:\n",
    "        y_true: True labels.\n",
    "        y_pred: Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        Recall score.\n",
    "    \"\"\"\n",
    "\n",
    "    tp = sum(y_true[i] == y_pred[i] for i in range(len(y_true)) if y_pred[i] == 1)\n",
    "    fn = sum(y_true[i] == 1 and y_pred[i] != 1 for i in range(len(y_true)))\n",
    "    recall = tp / (tp + fn) if tp + fn != 0 else 0\n",
    "    return recall\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    \"\"\"Calculates F1 score.\n",
    "\n",
    "    Args:\n",
    "        y_true: True labels.\n",
    "        y_pred: Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        F1 score.\n",
    "    \"\"\"\n",
    "\n",
    "    p = precision_score(y_true, y_pred)\n",
    "    r = recall_score(y_true, y_pred)\n",
    "    return 2 * p * r / (p + r) if p + r != 0 else 0\n",
    "\n",
    "def weighted_f1_score(y_true, y_pred):\n",
    "    \"\"\"Calculates the weighted F1 score, assuming equal class weights.\n",
    "\n",
    "    Args:\n",
    "        y_true: True labels.\n",
    "        y_pred: Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        Weighted F1 score.\n",
    "    \"\"\"\n",
    "\n",
    "    num_classes = len(set(y_true))\n",
    "    f1_scores = []\n",
    "    for i in range(num_classes):\n",
    "        class_mask = [1 if y == i else 0 for y in y_true]\n",
    "        class_f1 = f1_score(class_mask, [1 if y == i else 0 for y in y_pred])\n",
    "        f1_scores.append(class_f1)\n",
    "    return sum(f1_scores) / num_classes\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    \"\"\"Calculates the accuracy score.\n",
    "    \n",
    "    Args:\n",
    "    y_true: True labels.\n",
    "    y_pred: Predicted labels.\n",
    "    \n",
    "    Returns:\n",
    "    Accuracy score.\n",
    "    \"\"\"\n",
    "    \n",
    "    correct_predictions = sum(np.array(y_true) == np.array(y_pred))\n",
    "    total_predictions = len(y_true)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f3cec5",
   "metadata": {},
   "source": [
    "## Loading batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9df620d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config.fromfile('./mixup.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0258630",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a2c341f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'RawframeDataset',\n",
       " 'ann_file': 'data/hmdb51/annotation_train.txt',\n",
       " 'data_prefix': 'data/hmdb51/rawframes',\n",
       " 'pipeline': [{'type': 'SampleFrames',\n",
       "   'clip_len': 32,\n",
       "   'frame_interval': 2,\n",
       "   'num_clips': 1},\n",
       "  {'type': 'RawFrameDecode'},\n",
       "  {'type': 'Resize', 'scale': (-1, 256)},\n",
       "  {'type': 'RandomResizedCrop'},\n",
       "  {'type': 'Resize', 'scale': (224, 224), 'keep_ratio': False},\n",
       "  {'type': 'Flip', 'flip_ratio': 0.5},\n",
       "  {'type': 'Normalize',\n",
       "   'mean': [123.675, 116.28, 103.53],\n",
       "   'std': [58.395, 57.12, 57.375],\n",
       "   'to_bgr': False},\n",
       "  {'type': 'FormatShape', 'input_format': 'NCTHW'},\n",
       "  {'type': 'Collect', 'keys': ['imgs', 'label'], 'meta_keys': []},\n",
       "  {'type': 'ToTensor', 'keys': ['imgs', 'label']}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.data.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccef55c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = build_dataset(cfg=cfg.data.train)\n",
    "train_loader = build_dataloader(\n",
    "        train_dataset,\n",
    "        videos_per_gpu=8,\n",
    "        workers_per_gpu=4,\n",
    "        persistent_workers=False,\n",
    "        num_gpus=1,\n",
    "        dist=False)\n",
    "\n",
    "val_dataset = build_dataset(cfg=cfg.data.val)\n",
    "val_loader = build_dataloader(\n",
    "        val_dataset,\n",
    "        videos_per_gpu=1,\n",
    "        workers_per_gpu=4,\n",
    "        persistent_workers=False,\n",
    "        num_gpus=1,\n",
    "        dist=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eacfdb",
   "metadata": {},
   "source": [
    "## Learning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b5656d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-13 14:47:28,990] Using an existing study with name 'CutmixBlending_hmdb' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Optuna study...\n",
      "Starting a new trial...\n",
      "Trial 2: alpha = 3.807947176588889\n",
      "Building the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-13 14:47:29,341 - mmaction - INFO - load model from: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r50_ig65m_20210617-ce545a37.pth\n",
      "2024-12-13 14:47:29,341 - mmaction - INFO - load checkpoint from http path: https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r50_ig65m_20210617-ce545a37.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model built successfully!\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sadat/Desktop/scrambmix/mmaction2/mmaction/datasets/blending_utils.py:428: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  bbx1 = torch.clamp(cx - cut_w // 2, 0, w)\n",
      "/home/sadat/Desktop/scrambmix/mmaction2/mmaction/datasets/blending_utils.py:429: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  bby1 = torch.clamp(cy - cut_h // 2, 0, h)\n",
      "/home/sadat/Desktop/scrambmix/mmaction2/mmaction/datasets/blending_utils.py:430: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  bbx2 = torch.clamp(cx + cut_w // 2, 0, w)\n",
      "/home/sadat/Desktop/scrambmix/mmaction2/mmaction/datasets/blending_utils.py:431: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  bby2 = torch.clamp(cy + cut_h // 2, 0, h)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/60], Batch [0/447], Loss: 4.0017\n",
      "Epoch [1/60], Batch [10/447], Loss: 3.8550\n",
      "Epoch [1/60], Batch [20/447], Loss: 3.9037\n",
      "Epoch [1/60], Batch [30/447], Loss: 4.0036\n",
      "Epoch [1/60], Batch [40/447], Loss: 4.0364\n",
      "Epoch [1/60], Batch [50/447], Loss: 3.9965\n",
      "Epoch [1/60], Batch [60/447], Loss: 3.9191\n",
      "Epoch [1/60], Batch [70/447], Loss: 3.8570\n",
      "Epoch [1/60], Batch [80/447], Loss: 3.9293\n",
      "Epoch [1/60], Batch [90/447], Loss: 3.8626\n",
      "Epoch [1/60], Batch [100/447], Loss: 3.9847\n",
      "Epoch [1/60], Batch [110/447], Loss: 3.9310\n",
      "Epoch [1/60], Batch [120/447], Loss: 4.0001\n",
      "Epoch [1/60], Batch [130/447], Loss: 3.9716\n",
      "Epoch [1/60], Batch [140/447], Loss: 4.0392\n",
      "Epoch [1/60], Batch [150/447], Loss: 3.9258\n",
      "Epoch [1/60], Batch [160/447], Loss: 3.9586\n",
      "Epoch [1/60], Batch [170/447], Loss: 3.8823\n",
      "Epoch [1/60], Batch [180/447], Loss: 3.9372\n",
      "Epoch [1/60], Batch [190/447], Loss: 3.9580\n",
      "Epoch [1/60], Batch [200/447], Loss: 3.8277\n",
      "Epoch [1/60], Batch [210/447], Loss: 3.8995\n",
      "Epoch [1/60], Batch [220/447], Loss: 3.8848\n",
      "Epoch [1/60], Batch [230/447], Loss: 3.9089\n",
      "Epoch [1/60], Batch [240/447], Loss: 3.8905\n",
      "Epoch [1/60], Batch [250/447], Loss: 3.9762\n",
      "Epoch [1/60], Batch [260/447], Loss: 3.9293\n",
      "Epoch [1/60], Batch [270/447], Loss: 3.8866\n",
      "Epoch [1/60], Batch [280/447], Loss: 3.9120\n",
      "Epoch [1/60], Batch [290/447], Loss: 3.8654\n",
      "Epoch [1/60], Batch [300/447], Loss: 3.8936\n",
      "Epoch [1/60], Batch [310/447], Loss: 3.9404\n",
      "Epoch [1/60], Batch [320/447], Loss: 3.9758\n",
      "Epoch [1/60], Batch [330/447], Loss: 3.7813\n",
      "Epoch [1/60], Batch [340/447], Loss: 3.7833\n",
      "Epoch [1/60], Batch [350/447], Loss: 3.9065\n",
      "Epoch [1/60], Batch [360/447], Loss: 3.7566\n",
      "Epoch [1/60], Batch [370/447], Loss: 3.8781\n",
      "Epoch [1/60], Batch [380/447], Loss: 3.7806\n",
      "Epoch [1/60], Batch [390/447], Loss: 3.8351\n",
      "Epoch [1/60], Batch [400/447], Loss: 3.9585\n",
      "Epoch [1/60], Batch [410/447], Loss: 3.8462\n",
      "Epoch [1/60], Batch [420/447], Loss: 3.9470\n",
      "Epoch [1/60], Batch [430/447], Loss: 3.7769\n",
      "Epoch [1/60], Batch [440/447], Loss: 3.6894\n",
      "Epoch [1/60], Average Train Loss: 3.9241\n",
      "Epoch [1/60], Validation Accuracy: 0.2163, Validation Loss: 3.6810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sadat/Desktop/scrambmix/mmaction2/mmaction/datasets/blending_utils.py:428: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  bbx1 = torch.clamp(cx - cut_w // 2, 0, w)\n",
      "/home/sadat/Desktop/scrambmix/mmaction2/mmaction/datasets/blending_utils.py:429: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  bby1 = torch.clamp(cy - cut_h // 2, 0, h)\n",
      "/home/sadat/Desktop/scrambmix/mmaction2/mmaction/datasets/blending_utils.py:430: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  bbx2 = torch.clamp(cx + cut_w // 2, 0, w)\n",
      "/home/sadat/Desktop/scrambmix/mmaction2/mmaction/datasets/blending_utils.py:431: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  bby2 = torch.clamp(cy + cut_h // 2, 0, h)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/60], Batch [0/447], Loss: 3.8086\n",
      "Epoch [2/60], Batch [10/447], Loss: 3.8130\n",
      "Epoch [2/60], Batch [20/447], Loss: 3.9217\n",
      "Epoch [2/60], Batch [30/447], Loss: 3.8896\n",
      "Epoch [2/60], Batch [40/447], Loss: 3.6152\n",
      "Epoch [2/60], Batch [50/447], Loss: 3.8402\n",
      "Epoch [2/60], Batch [60/447], Loss: 3.8565\n",
      "Epoch [2/60], Batch [70/447], Loss: 3.8753\n",
      "Epoch [2/60], Batch [80/447], Loss: 3.8254\n",
      "Epoch [2/60], Batch [90/447], Loss: 3.8829\n",
      "Epoch [2/60], Batch [100/447], Loss: 3.9247\n",
      "Epoch [2/60], Batch [110/447], Loss: 3.9302\n",
      "Epoch [2/60], Batch [120/447], Loss: 3.7436\n",
      "Epoch [2/60], Batch [130/447], Loss: 3.7009\n",
      "Epoch [2/60], Batch [140/447], Loss: 3.7565\n",
      "Epoch [2/60], Batch [150/447], Loss: 3.6470\n",
      "Epoch [2/60], Batch [160/447], Loss: 3.7827\n",
      "Epoch [2/60], Batch [170/447], Loss: 3.6660\n",
      "Epoch [2/60], Batch [180/447], Loss: 3.7349\n",
      "Epoch [2/60], Batch [190/447], Loss: 3.7314\n",
      "Epoch [2/60], Batch [200/447], Loss: 3.7109\n",
      "Epoch [2/60], Batch [210/447], Loss: 3.5050\n",
      "Epoch [2/60], Batch [220/447], Loss: 3.7640\n",
      "Epoch [2/60], Batch [230/447], Loss: 3.7913\n",
      "Epoch [2/60], Batch [240/447], Loss: 3.6556\n",
      "Epoch [2/60], Batch [250/447], Loss: 3.9559\n",
      "Epoch [2/60], Batch [260/447], Loss: 3.8660\n",
      "Epoch [2/60], Batch [270/447], Loss: 3.7630\n",
      "Epoch [2/60], Batch [280/447], Loss: 3.6687\n",
      "Epoch [2/60], Batch [290/447], Loss: 3.6915\n",
      "Epoch [2/60], Batch [300/447], Loss: 3.9267\n",
      "Epoch [2/60], Batch [310/447], Loss: 3.7039\n",
      "Epoch [2/60], Batch [320/447], Loss: 3.9852\n",
      "Epoch [2/60], Batch [330/447], Loss: 3.7458\n",
      "Epoch [2/60], Batch [340/447], Loss: 3.8732\n",
      "Epoch [2/60], Batch [350/447], Loss: 3.9136\n",
      "Epoch [2/60], Batch [360/447], Loss: 3.7572\n",
      "Epoch [2/60], Batch [370/447], Loss: 3.8441\n",
      "Epoch [2/60], Batch [380/447], Loss: 3.6384\n",
      "Epoch [2/60], Batch [390/447], Loss: 3.6302\n",
      "Epoch [2/60], Batch [400/447], Loss: 3.8449\n",
      "Epoch [2/60], Batch [410/447], Loss: 3.5299\n",
      "Epoch [2/60], Batch [420/447], Loss: 3.6248\n",
      "Epoch [2/60], Batch [430/447], Loss: 3.4214\n",
      "Epoch [2/60], Batch [440/447], Loss: 3.7725\n",
      "Epoch [2/60], Average Train Loss: 3.7682\n",
      "Epoch [2/60], Validation Accuracy: 0.3105, Validation Loss: 3.1652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sadat/Desktop/scrambmix/mmaction2/mmaction/datasets/blending_utils.py:428: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  bbx1 = torch.clamp(cx - cut_w // 2, 0, w)\n",
      "/home/sadat/Desktop/scrambmix/mmaction2/mmaction/datasets/blending_utils.py:429: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  bby1 = torch.clamp(cy - cut_h // 2, 0, h)\n",
      "/home/sadat/Desktop/scrambmix/mmaction2/mmaction/datasets/blending_utils.py:430: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  bbx2 = torch.clamp(cx + cut_w // 2, 0, w)\n",
      "/home/sadat/Desktop/scrambmix/mmaction2/mmaction/datasets/blending_utils.py:431: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  bby2 = torch.clamp(cy + cut_h // 2, 0, h)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/60], Batch [0/447], Loss: 3.8049\n",
      "Epoch [3/60], Batch [10/447], Loss: 3.6762\n",
      "Epoch [3/60], Batch [20/447], Loss: 3.5102\n",
      "Epoch [3/60], Batch [30/447], Loss: 3.7569\n",
      "Epoch [3/60], Batch [40/447], Loss: 3.4445\n",
      "Epoch [3/60], Batch [50/447], Loss: 3.7259\n",
      "Epoch [3/60], Batch [60/447], Loss: 3.7490\n",
      "Epoch [3/60], Batch [70/447], Loss: 3.6203\n",
      "Epoch [3/60], Batch [80/447], Loss: 3.6655\n",
      "Epoch [3/60], Batch [90/447], Loss: 3.6924\n",
      "Epoch [3/60], Batch [100/447], Loss: 3.4790\n",
      "Epoch [3/60], Batch [110/447], Loss: 3.8090\n",
      "Epoch [3/60], Batch [120/447], Loss: 3.3918\n",
      "Epoch [3/60], Batch [130/447], Loss: 3.4917\n",
      "Epoch [3/60], Batch [140/447], Loss: 3.8106\n",
      "Epoch [3/60], Batch [150/447], Loss: 3.6530\n",
      "Epoch [3/60], Batch [160/447], Loss: 3.9826\n",
      "Epoch [3/60], Batch [170/447], Loss: 3.6579\n",
      "Epoch [3/60], Batch [180/447], Loss: 3.4836\n",
      "Epoch [3/60], Batch [190/447], Loss: 3.9067\n",
      "Epoch [3/60], Batch [200/447], Loss: 3.8630\n",
      "Epoch [3/60], Batch [210/447], Loss: 3.7085\n",
      "Epoch [3/60], Batch [220/447], Loss: 3.6508\n",
      "Epoch [3/60], Batch [230/447], Loss: 3.8534\n",
      "Epoch [3/60], Batch [240/447], Loss: 3.7718\n",
      "Epoch [3/60], Batch [250/447], Loss: 3.6741\n",
      "Epoch [3/60], Batch [260/447], Loss: 3.7780\n",
      "Epoch [3/60], Batch [270/447], Loss: 3.7361\n",
      "Epoch [3/60], Batch [280/447], Loss: 3.5233\n",
      "Epoch [3/60], Batch [290/447], Loss: 3.3693\n",
      "Epoch [3/60], Batch [300/447], Loss: 3.5835\n",
      "Epoch [3/60], Batch [310/447], Loss: 3.8167\n",
      "Epoch [3/60], Batch [320/447], Loss: 3.7177\n",
      "Epoch [3/60], Batch [330/447], Loss: 3.6351\n",
      "Epoch [3/60], Batch [340/447], Loss: 3.4886\n",
      "Epoch [3/60], Batch [350/447], Loss: 3.5971\n",
      "Epoch [3/60], Batch [360/447], Loss: 3.8377\n",
      "Epoch [3/60], Batch [370/447], Loss: 3.7079\n",
      "Epoch [3/60], Batch [380/447], Loss: 3.4175\n",
      "Epoch [3/60], Batch [390/447], Loss: 3.3290\n",
      "Epoch [3/60], Batch [400/447], Loss: 3.8611\n",
      "Epoch [3/60], Batch [410/447], Loss: 3.8624\n",
      "Epoch [3/60], Batch [420/447], Loss: 3.9241\n",
      "Epoch [3/60], Batch [430/447], Loss: 3.6855\n",
      "Epoch [3/60], Batch [440/447], Loss: 3.3837\n",
      "Epoch [3/60], Average Train Loss: 3.6229\n",
      "Epoch [3/60], Validation Accuracy: 0.4105, Validation Loss: 2.8725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sadat/Desktop/scrambmix/mmaction2/mmaction/datasets/blending_utils.py:428: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  bbx1 = torch.clamp(cx - cut_w // 2, 0, w)\n",
      "/home/sadat/Desktop/scrambmix/mmaction2/mmaction/datasets/blending_utils.py:429: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  bby1 = torch.clamp(cy - cut_h // 2, 0, h)\n",
      "/home/sadat/Desktop/scrambmix/mmaction2/mmaction/datasets/blending_utils.py:430: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  bbx2 = torch.clamp(cx + cut_w // 2, 0, w)\n",
      "/home/sadat/Desktop/scrambmix/mmaction2/mmaction/datasets/blending_utils.py:431: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  bby2 = torch.clamp(cy + cut_h // 2, 0, h)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/60], Batch [0/447], Loss: 3.6927\n",
      "Epoch [4/60], Batch [10/447], Loss: 3.6496\n",
      "Epoch [4/60], Batch [20/447], Loss: 3.5001\n",
      "Epoch [4/60], Batch [30/447], Loss: 3.6296\n",
      "Epoch [4/60], Batch [40/447], Loss: 3.5278\n",
      "Epoch [4/60], Batch [50/447], Loss: 3.6392\n",
      "Epoch [4/60], Batch [60/447], Loss: 3.5967\n",
      "Epoch [4/60], Batch [70/447], Loss: 3.1779\n",
      "Epoch [4/60], Batch [80/447], Loss: 3.6632\n",
      "Epoch [4/60], Batch [90/447], Loss: 3.5288\n",
      "Epoch [4/60], Batch [100/447], Loss: 3.8220\n",
      "Epoch [4/60], Batch [110/447], Loss: 3.7019\n",
      "Epoch [4/60], Batch [120/447], Loss: 3.2668\n",
      "Epoch [4/60], Batch [130/447], Loss: 3.5615\n",
      "Epoch [4/60], Batch [140/447], Loss: 3.5693\n",
      "Epoch [4/60], Batch [150/447], Loss: 3.6382\n",
      "Epoch [4/60], Batch [160/447], Loss: 3.2687\n",
      "Epoch [4/60], Batch [170/447], Loss: 3.5777\n",
      "Epoch [4/60], Batch [180/447], Loss: 3.7546\n",
      "Epoch [4/60], Batch [190/447], Loss: 3.7040\n",
      "Epoch [4/60], Batch [200/447], Loss: 3.2182\n",
      "Epoch [4/60], Batch [210/447], Loss: 3.3139\n",
      "Epoch [4/60], Batch [220/447], Loss: 3.3932\n",
      "Epoch [4/60], Batch [230/447], Loss: 3.4898\n",
      "Epoch [4/60], Batch [240/447], Loss: 3.0280\n",
      "Epoch [4/60], Batch [250/447], Loss: 3.3885\n",
      "Epoch [4/60], Batch [260/447], Loss: 3.4843\n",
      "Epoch [4/60], Batch [270/447], Loss: 3.3015\n",
      "Epoch [4/60], Batch [280/447], Loss: 3.5909\n",
      "Epoch [4/60], Batch [290/447], Loss: 3.4984\n",
      "Epoch [4/60], Batch [300/447], Loss: 3.5132\n",
      "Epoch [4/60], Batch [310/447], Loss: 3.7524\n",
      "Epoch [4/60], Batch [320/447], Loss: 3.5429\n",
      "Epoch [4/60], Batch [330/447], Loss: 3.6231\n",
      "Epoch [4/60], Batch [340/447], Loss: 3.5257\n",
      "Epoch [4/60], Batch [350/447], Loss: 3.3619\n",
      "Epoch [4/60], Batch [360/447], Loss: 3.3954\n",
      "Epoch [4/60], Batch [370/447], Loss: 3.5227\n",
      "Epoch [4/60], Batch [380/447], Loss: 3.1693\n",
      "Epoch [4/60], Batch [390/447], Loss: 3.8310\n",
      "Epoch [4/60], Batch [400/447], Loss: 3.6018\n",
      "Epoch [4/60], Batch [410/447], Loss: 3.4261\n",
      "Epoch [4/60], Batch [420/447], Loss: 3.6107\n",
      "Epoch [4/60], Batch [430/447], Loss: 3.6174\n",
      "Epoch [4/60], Batch [440/447], Loss: 3.2817\n",
      "Epoch [4/60], Average Train Loss: 3.4987\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import logging\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Best parameters from previous study\n",
    "best_params = {\n",
    "    'dropout_ratio': 0.6795542149013333,\n",
    "    'lr': 7.886714129990479e-06,\n",
    "    'max_norm': 41,\n",
    "    'with_pool2': True,\n",
    "    'bottleneck_mode': 'ir',\n",
    "    'norm_eval': False,\n",
    "    'bn_frozen': False\n",
    "}\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='optuna_training_CutmixBlending_hmdb.log', \n",
    "                    filemode='w', \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "                    level=logging.INFO)\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Database file path for saving study\n",
    "db_file = \"sqlite:///optuna_study_CutmixBlending_hmdb.db\"\n",
    "\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "\n",
    "# Set up study with the option to minimize validation loss\n",
    "study = optuna.create_study(\n",
    "    sampler=sampler,\n",
    "    direction=\"maximize\",\n",
    "    study_name=\"CutmixBlending_hmdb\", \n",
    "    storage=db_file,\n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "def objective(trial):\n",
    "    print(\"Starting a new trial...\")\n",
    "\n",
    "    # Hyperparameters to tune\n",
    "    alpha = trial.suggest_float(\"alpha\", 0.1, 10)  \n",
    "    print(f\"Trial {trial.number}: alpha = {alpha}\")\n",
    "\n",
    "    # Set hyperparameters\n",
    "    dropout_ratio = best_params['dropout_ratio']\n",
    "    lr = best_params['lr']\n",
    "    max_norm = best_params['max_norm']\n",
    "\n",
    "    # Backbone parameters\n",
    "    cfg.model.backbone.with_pool2 = best_params['with_pool2']\n",
    "    cfg.model.backbone.bottleneck_mode = best_params['bottleneck_mode']\n",
    "    cfg.model.backbone.norm_eval = best_params['norm_eval']\n",
    "    cfg.model.backbone.bn_frozen = best_params['bn_frozen']\n",
    "\n",
    "    # Fixed pretrained URL\n",
    "    cfg.model.backbone.pretrained = 'https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r50_ig65m_20210617-ce545a37.pth'\n",
    "\n",
    "    # Adjust config parameters\n",
    "    cfg.model.cls_head.dropout_ratio = dropout_ratio\n",
    "\n",
    "    # Initialize model, criterion, optimizer, scheduler\n",
    "    print(\"Building the model...\")\n",
    "    model = build_model(cfg.model, train_cfg=None, test_cfg=cfg.get('test_cfg')).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        weight_decay=0.00001\n",
    "    )\n",
    "\n",
    "    print(\"Model built successfully!\")\n",
    "    \n",
    "    # Early stopping parameters\n",
    "    total_epochs = 60\n",
    "    eval_interval = 1\n",
    "    best_val_accuracy = 0\n",
    "\n",
    "    # CutmixBlending Blending instance\n",
    "    cutmix_blending = CutmixBlending(num_classes=cfg.model.cls_head.num_classes, alpha=alpha)\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(total_epochs):\n",
    "\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            inputs, labels = data['imgs'].to(device), data['label'].to(device)\n",
    "\n",
    "            # Convert labels to one-hot encoding\n",
    "            labels_one_hot = F.one_hot(labels, num_classes=cfg.model.cls_head.num_classes).float()\n",
    "\n",
    "            # Apply CutmixBlending\n",
    "            mixed_inputs, mixed_labels = cutmix_blending.do_blending(inputs, labels_one_hot)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(mixed_inputs, mixed_labels, return_loss=True)\n",
    "            loss = outputs['loss_cls']\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{total_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        logging.info(f\"Epoch [{epoch + 1}/{total_epochs}], Train Loss: {epoch_loss / len(train_loader):.4f}\")\n",
    "        print(f\"Epoch [{epoch + 1}/{total_epochs}], Average Train Loss: {epoch_loss / len(train_loader):.4f}\")\n",
    "\n",
    "        # Validation loop (every `eval_interval` epochs)\n",
    "        if (epoch + 1) % eval_interval == 0:\n",
    "            model.eval()\n",
    "            total_val_loss = 0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            with torch.no_grad():\n",
    "                for val_data in val_loader:\n",
    "                    val_inputs, val_labels = val_data['imgs'].to(device), val_data['label'].to(device)\n",
    "\n",
    "                    val_results = model(val_inputs, return_loss=False)\n",
    "                    val_loss = model(val_inputs, val_labels, return_loss=True)['loss_cls']\n",
    "\n",
    "                    total_val_loss += val_loss.item()\n",
    "\n",
    "                    # Collect predictions and true labels\n",
    "                    predictions = np.argmax(val_results, axis=1)\n",
    "                    true_labels = val_labels.cpu().numpy()\n",
    "\n",
    "                    all_preds.extend(predictions)\n",
    "                    all_labels.extend(true_labels)\n",
    "\n",
    "            val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "            print(f\"Epoch [{epoch + 1}/{total_epochs}], Validation Accuracy: {val_accuracy:.4f}, Validation Loss: {total_val_loss / len(val_loader):.4f}\")\n",
    "            logging.info(f\"Epoch [{epoch + 1}/{total_epochs}], Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "            # Report validation loss to Optuna\n",
    "            trial.report(val_accuracy, epoch)\n",
    "\n",
    "            # Check if validation accuracy improved\n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "\n",
    "            # Prune unpromising trials\n",
    "            if trial.should_prune():\n",
    "                print(\"Trial pruned due to lack of improvement.\")\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return best_val_accuracy\n",
    "\n",
    "# Run Optuna Study\n",
    "print(\"Starting Optuna study...\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "print(f\"Best hyperparameters: {study.best_params}\")\n",
    "print(f\"Best validation accuracy: {study.best_value:.4f}\")\n",
    "logging.info(\"Best hyperparameters: %s\", study.best_params)\n",
    "logging.info(\"Best validation accuracy: %f\", study.best_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8748ddae",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2020f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all trials and print their parameters\n",
    "for trial in study.trials:\n",
    "    print(f\"Trial number: {trial.number}\")\n",
    "    print(f\"Parameters: {trial.params}\")\n",
    "    print(f\"Value (e.g., validation accuracy): {trial.value}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f56844",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = study.best_trial\n",
    "print(\"Best trial number:\", best_trial.number)\n",
    "print(\"Best parameters:\", best_trial.params)\n",
    "print(\"Best validation loss:\", best_trial.value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrambmix",
   "language": "python",
   "name": "scrambmix"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
